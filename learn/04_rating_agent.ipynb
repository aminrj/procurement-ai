{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2272bcd1",
   "metadata": {},
   "source": [
    "# 04: Building the Rating Agent\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Multi-dimensional scoring with LLMs\n",
    "- Business logic in AI agents\n",
    "- Prompt engineering for analytical tasks\n",
    "- Balancing objective metrics with subjective reasoning\n",
    "\n",
    "**What We're Building:**\n",
    "An agent that evaluates filtered tenders on multiple dimensions (strategic fit, win probability, effort required) to help prioritize opportunities. This is Agent #2 in our pipeline.\n",
    "\n",
    "**The Challenge:**\n",
    "\"Is it relevant?\" is easy. \"Is it worth bidding on?\" requires business judgment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2a202",
   "metadata": {},
   "source": [
    "## Why Rating Is Harder Than Filtering\n",
    "\n",
    "Filtering is binary: relevant or not.\n",
    "\n",
    "Rating requires:\n",
    "- Multiple dimensions (fit, probability, effort)\n",
    "- Comparative judgment (is 7.5 better than 6.8?)\n",
    "- Business context (company capabilities, competition)\n",
    "- Risk assessment (what could go wrong?)\n",
    "\n",
    "This is where LLMs shine: fuzzy judgment based on incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6262f6",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76822c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: anyio in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12f8c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports ready\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Type, TypeVar\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "BASE_URL = \"http://localhost:1234/v1\"\n",
    "MODEL = \"local-model\"\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "print(\"‚úì Imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5651ce",
   "metadata": {},
   "source": [
    "## Step 2: Define Rating Output Schema\n",
    "\n",
    "A good rating has multiple dimensions, not just a single score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f2a7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example rating:\n",
      "{\n",
      "  \"overall_score\": 8.5,\n",
      "  \"strategic_fit\": 9.0,\n",
      "  \"win_probability\": 7.5,\n",
      "  \"effort_required\": 8.0,\n",
      "  \"strengths\": [\n",
      "    \"Perfect match for our AI cybersecurity expertise\",\n",
      "    \"High-value contract with long-term potential\",\n",
      "    \"Existing relationship with client organization\"\n",
      "  ],\n",
      "  \"risks\": [\n",
      "    \"Tight timeline may strain resources\",\n",
      "    \"Likely to attract large competitors\",\n",
      "    \"Complex integration requirements\"\n",
      "  ],\n",
      "  \"recommendation\": \"GO - Strong strategic fit and reasonable win probability justify the effort.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class RatingResult(BaseModel):\n",
    "    \"\"\"Multi-dimensional tender rating\"\"\"\n",
    "    \n",
    "    # Scores (0-10)\n",
    "    overall_score: float = Field(description=\"Overall opportunity score 0-10\", ge=0, le=10)\n",
    "    strategic_fit: float = Field(description=\"How well this matches our expertise 0-10\", ge=0, le=10)\n",
    "    win_probability: float = Field(description=\"Likelihood of winning 0-10\", ge=0, le=10)\n",
    "    effort_required: float = Field(description=\"Complexity and resource needs 0-10\", ge=0, le=10)\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    strengths: List[str] = Field(description=\"Top 3 strengths/opportunities\")\n",
    "    risks: List[str] = Field(description=\"Top 3 risks/challenges\")\n",
    "    recommendation: str = Field(description=\"Go/No-Go recommendation with reasoning\")\n",
    "\n",
    "# Test the model\n",
    "example = RatingResult(\n",
    "    overall_score=8.5,\n",
    "    strategic_fit=9.0,\n",
    "    win_probability=7.5,\n",
    "    effort_required=8.0,\n",
    "    strengths=[\n",
    "        \"Perfect match for our AI cybersecurity expertise\",\n",
    "        \"High-value contract with long-term potential\",\n",
    "        \"Existing relationship with client organization\"\n",
    "    ],\n",
    "    risks=[\n",
    "        \"Tight timeline may strain resources\",\n",
    "        \"Likely to attract large competitors\",\n",
    "        \"Complex integration requirements\"\n",
    "    ],\n",
    "    recommendation=\"GO - Strong strategic fit and reasonable win probability justify the effort.\"\n",
    ")\n",
    "\n",
    "print(\"Example rating:\")\n",
    "print(example.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edab8b",
   "metadata": {},
   "source": [
    "## Step 3: Build LLM Helper Function\n",
    "\n",
    "**Important Learning:** In notebooks 02 and 03, we showed the LLM the JSON schema (the structure definition). That works, but LLMs sometimes get confused and return the schema itself instead of actual data!\n",
    "\n",
    "**The Problem:**\n",
    "```python\n",
    "# Schema approach (what we used before)\n",
    "schema = {\"properties\": {\"score\": {\"type\": \"number\"}}}\n",
    "# LLM sees: \"type\": \"number\" \n",
    "# LLM returns: {\"properties\": {\"score\": {\"type\": \"number\"}}} ‚ùå\n",
    "```\n",
    "\n",
    "**The Solution:**\n",
    "Show the LLM a concrete example instead! LLMs understand examples better than abstract schemas.\n",
    "\n",
    "```python\n",
    "# Example approach (what we use now)\n",
    "example = {\"score\": 8.5}\n",
    "# LLM sees: 8.5\n",
    "# LLM returns: {\"score\": 7.2} ‚úÖ\n",
    "```\n",
    "\n",
    "This is a **production pattern** used in real systems. Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6b30a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM helper ready (using example-based prompts)\n"
     ]
    }
   ],
   "source": [
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def build_structured_prompt(prompt: str, model_class: Type[BaseModel]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt with concrete example instead of abstract schema.\n",
    "    \n",
    "    Why? LLMs understand examples better than JSON schemas.\n",
    "    Showing {\"score\": 8.5} is clearer than showing {\"type\": \"number\", \"min\": 0}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build a concrete example based on the model\n",
    "    if model_class.__name__ == \"RatingResult\":\n",
    "        # For RatingResult, show realistic rating values\n",
    "        example = {\n",
    "            \"overall_score\": 8.5,\n",
    "            \"strategic_fit\": 9.0,\n",
    "            \"win_probability\": 7.5,\n",
    "            \"effort_required\": 8.0,\n",
    "            \"strengths\": [\n",
    "                \"Strong match for our expertise\",\n",
    "                \"Reasonable contract value\",\n",
    "                \"Good client relationship\"\n",
    "            ],\n",
    "            \"risks\": [\n",
    "                \"Tight timeline may be challenging\",\n",
    "                \"Competition from larger firms\",\n",
    "                \"Technical complexity\"\n",
    "            ],\n",
    "            \"recommendation\": \"GO - Strategic fit justifies the effort.\"\n",
    "        }\n",
    "    else:\n",
    "        # Fallback: use schema if we don't have a specific example\n",
    "        example = model_class.model_json_schema()\n",
    "    \n",
    "    return f\"\"\"{prompt}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "You must respond with ONLY a valid JSON object containing YOUR ANALYSIS.\n",
    "Do NOT return a schema, return ACTUAL DATA.\n",
    "\n",
    "EXPECTED FORMAT (fill in with your actual analysis):\n",
    "{json.dumps(example, indent=2)}\n",
    "\n",
    "IMPORTANT:\n",
    "- Use actual numbers (like 8.5), not descriptions\n",
    "- Use actual text in arrays (3 items for strengths and risks)\n",
    "- All fields are required\n",
    "- No markdown, no code blocks, no explanations\n",
    "- Start with {{ and end with }}\n",
    "\n",
    "Your JSON response:\"\"\"\n",
    "\n",
    "async def call_llm(\n",
    "    prompt: str,\n",
    "    response_model: Type[T],\n",
    "    system_prompt: str,\n",
    "    temperature: float = 0.1,\n",
    "    max_retries: int = 3\n",
    ") -> T:\n",
    "    \"\"\"Call LLM with structured output and retries\"\"\"\n",
    "    \n",
    "    full_prompt = build_structured_prompt(prompt, response_model)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "                response = await client.post(\n",
    "                    f\"{BASE_URL}/chat/completions\",\n",
    "                    json={\n",
    "                        \"model\": MODEL,\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": full_prompt}\n",
    "                        ],\n",
    "                        \"temperature\": temperature,\n",
    "                    },\n",
    "                )\n",
    "                \n",
    "                result = response.json()\n",
    "                content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                \n",
    "                # Clean response - remove markdown code blocks\n",
    "                content = content.strip()\n",
    "                if content.startswith(\"```json\"):\n",
    "                    content = content[7:]\n",
    "                if content.startswith(\"```\"):\n",
    "                    content = content[3:]\n",
    "                if content.endswith(\"```\"):\n",
    "                    content = content[:-3]\n",
    "                content = content.strip()\n",
    "                \n",
    "                # Extract JSON by finding balanced braces (handles extra text)\n",
    "                start_idx = content.find('{')\n",
    "                if start_idx != -1:\n",
    "                    brace_count = 0\n",
    "                    end_idx = -1\n",
    "                    for i in range(start_idx, len(content)):\n",
    "                        if content[i] == '{':\n",
    "                            brace_count += 1\n",
    "                        elif content[i] == '}':\n",
    "                            brace_count -= 1\n",
    "                            if brace_count == 0:\n",
    "                                end_idx = i\n",
    "                                break\n",
    "                    if end_idx != -1:\n",
    "                        content = content[start_idx:end_idx + 1]\n",
    "                \n",
    "                # Parse and validate\n",
    "                data = json.loads(content)\n",
    "                return response_model.model_validate(data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\" !! Attempt {attempt + 1} failed: {str(e)[:80]}...\")\n",
    "                print(f\" !! Retrying...\")\n",
    "                await asyncio.sleep(1)\n",
    "            else:\n",
    "                raise Exception(f\"Failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "print(\"‚úì LLM helper ready (using example-based prompts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332f1bf",
   "metadata": {},
   "source": [
    "## Step 4: Build the Rating Agent\n",
    "\n",
    "This is where prompt engineering matters. We need to guide the LLM to think like a business analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78943bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Rating agent ready\n"
     ]
    }
   ],
   "source": [
    "class Tender(BaseModel):\n",
    "    \"\"\"Input tender data\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    organization: str\n",
    "    deadline: str\n",
    "    estimated_value: str | None = None\n",
    "\n",
    "async def rate_tender(\n",
    "    tender: Tender, \n",
    "    categories: List[str],\n",
    "    temperature: float = 0.1\n",
    ") -> RatingResult:\n",
    "    \"\"\"\n",
    "    Rating Agent: Evaluate business opportunity\n",
    "    \n",
    "    Key design decisions:\n",
    "    - Low temperature (0.1) for consistent scoring\n",
    "    - Multiple dimensions to avoid single-number bias\n",
    "    - Explicit company context in prompt\n",
    "    - Required strengths AND risks (balanced view)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Rate this tender opportunity for a small tech consultancy:\n",
    "\n",
    "TENDER DETAILS:\n",
    "Title: {tender.title}\n",
    "Client: {tender.organization}\n",
    "Value: {tender.estimated_value or \"Not specified\"}\n",
    "Deadline: {tender.deadline}\n",
    "Categories: {', '.join(categories)}\n",
    "\n",
    "DESCRIPTION:\n",
    "{tender.description}\n",
    "\n",
    "OUR COMPANY PROFILE:\n",
    "- Small tech consultancy (10-15 people)\n",
    "- Core expertise: AI/ML, Cybersecurity, Software Development\n",
    "- Strong technical skills, limited by team size\n",
    "- Track record with mid-sized government contracts\n",
    "- Prefer projects lasting 3-12 months\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "\n",
    "1. STRATEGIC FIT (0-10):\n",
    "   - How well does this match our expertise in {', '.join(categories)}?\n",
    "   - Does it leverage our unique strengths?\n",
    "   - Will it build valuable capabilities or relationships?\n",
    "\n",
    "2. WIN PROBABILITY (0-10):\n",
    "   - Are we genuinely competitive for this?\n",
    "   - What's the likely competition (size, specialization)?\n",
    "   - Do we have relevant experience and credibility?\n",
    "\n",
    "3. EFFORT REQUIRED (0-10):\n",
    "   - Technical complexity and scope\n",
    "   - Resource requirements vs our team size\n",
    "   - Timeline pressure and delivery risk\n",
    "\n",
    "4. OVERALL SCORE (0-10):\n",
    "   - Weighted assessment considering all factors\n",
    "   - Value of opportunity vs investment required\n",
    "\n",
    "Provide REALISTIC scores (not optimistic). Most opportunities should score 5-7.\n",
    "Identify specific strengths and concrete risks.\n",
    "Give clear Go/No-Go recommendation. \n",
    "\"\"\"\n",
    "    \n",
    "    system = \"\"\"You are a business development expert evaluating tender opportunities.\n",
    "You have 15 years of experience in government contracting and tech consulting.\n",
    "Be analytical and realistic, not optimistic. Consider both opportunity and risk.\"\"\"\n",
    "    \n",
    "    return await call_llm(\n",
    "        prompt=prompt,\n",
    "        response_model=RatingResult,\n",
    "        system_prompt=system,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "print(\"‚úì Rating agent ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c5c87",
   "metadata": {},
   "source": [
    "## Step 5: Test with High-Value Opportunity\n",
    "\n",
    "Let's rate a tender that should score well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d363709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 1: Strong Opportunity\n",
      "======================================================================\n",
      "Overall Score: 6.0/10\n",
      "Strategic Fit: 8.0/10\n",
      "Win Probability: 5.0/10\n",
      "Effort Required: 7.0/10\n",
      "\n",
      "Strengths:\n",
      "  + Strong AI/ML expertise aligns with fraud detection\n",
      "  + Existing cybersecurity skills aid secure integration\n",
      "  + Opportunity to deepen relationship with state commission\n",
      "\n",
      "Risks:\n",
      "  - Large competition from 50+ firm vendors\n",
      "  - High technical complexity of real‚Äëtime integration\n",
      "  - Limited team size may strain delivery timeline\n",
      "\n",
      "Recommendation: NO-GO - High effort and competitive risk outweigh strategic fit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST 1: Strong Opportunity\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender1 = Tender(\n",
    "    id=\"R001\",\n",
    "    title=\"AI-Powered Fraud Detection System\",\n",
    "    description=\"\"\"Develop machine learning system to detect fraudulent transactions \n",
    "    in real-time. Must integrate with existing payment processing infrastructure. \n",
    "    Project includes model development, deployment, and 6 months of monitoring and \n",
    "    refinement. Team will work closely with our data science division.\"\"\",\n",
    "    organization=\"State Financial Services Commission\",\n",
    "    deadline=\"2025-02-01\",\n",
    "    estimated_value=\"$850K\"\n",
    ")\n",
    "\n",
    "rating1 = await rate_tender(\n",
    "    tender1, \n",
    "    categories=[\"ai\", \"software\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating1.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating1.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating1.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating1.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating1.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating1.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating1.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bffd3",
   "metadata": {},
   "source": [
    "## Step 6: Test with Poor Fit\n",
    "\n",
    "A tender that's technically relevant but a bad business opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d17ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 2: Poor Fit (Scale Mismatch)\n",
      "======================================================================\n",
      "Overall Score: 4.0/10\n",
      "Strategic Fit: 6.0/10\n",
      "Win Probability: 3.0/10\n",
      "Effort Required: 9.0/10\n",
      "\n",
      "Strengths:\n",
      "  + Strong cybersecurity expertise\n",
      "  + Experience with AI/ML can add value to modernization\n",
      "  + Existing government contract experience\n",
      "\n",
      "Risks:\n",
      "  - Requires 100+ engineers, far beyond our team size\n",
      "  - Need national security clearances we lack\n",
      "  - High competition from large prime contractors with established subcontractor networks\n",
      "\n",
      "Recommendation: NO - The effort and resource gap outweigh the strategic fit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST 2: Poor Fit (Scale Mismatch)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender2 = Tender(\n",
    "    id=\"R002\",\n",
    "    title=\"National Cybersecurity Infrastructure Modernization\",\n",
    "    description=\"\"\"Massive 5-year program to modernize cybersecurity infrastructure \n",
    "    across all federal agencies. Requires dedicated team of 100+ engineers, \n",
    "    proven experience with large-scale deployments, and existing national security \n",
    "    clearances. Prime contractor will coordinate 10+ subcontractors.\"\"\",\n",
    "    organization=\"Department of Homeland Security\",\n",
    "    deadline=\"2024-11-01\",\n",
    "    estimated_value=\"$250M\"\n",
    ")\n",
    "\n",
    "rating2 = await rate_tender(\n",
    "    tender2,\n",
    "    categories=[\"cybersecurity\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating2.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating2.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating2.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating2.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating2.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating2.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating2.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb46357",
   "metadata": {},
   "source": [
    "## Step 7: Test Edge Case - High Risk, High Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aa8eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 3: High Risk, High Reward\n",
      "======================================================================\n",
      "Overall Score: 5.0/10\n",
      "Strategic Fit: 8.0/10\n",
      "Win Probability: 4.0/10\n",
      "Effort Required: 9.0/10\n",
      "\n",
      "Strengths:\n",
      "  + Strong alignment with AI and cybersecurity expertise\n",
      "  + Potential to develop cutting‚Äëedge capabilities\n",
      "  + High contract value for a small firm\n",
      "\n",
      "Risks:\n",
      "  - Very high technical risk with no commercial precedent\n",
      "  - Extremely tight 18‚Äëmonth timeline for a 10‚Äë15 person team\n",
      "  - Likely competition from large, specialized defense contractors\n",
      "\n",
      "Recommendation: NO - High effort and low win probability outweigh strategic benefits.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST 3: High Risk, High Reward\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender3 = Tender(\n",
    "    id=\"R003\",\n",
    "    title=\"Experimental AI Research Platform\",\n",
    "    description=\"\"\"Build novel AI research platform using cutting-edge techniques \n",
    "    (federated learning, differential privacy, quantum-resistant cryptography). \n",
    "    No existing commercial solutions. High technical risk but potential for \n",
    "    groundbreaking capabilities. 18-month timeline with staged milestones.\"\"\",\n",
    "    organization=\"Defense Advanced Research Agency\",\n",
    "    deadline=\"2024-12-15\",\n",
    "    estimated_value=\"$1.2M\"\n",
    ")\n",
    "\n",
    "rating3 = await rate_tender(\n",
    "    tender3,\n",
    "    categories=[\"ai\", \"cybersecurity\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating3.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating3.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating3.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating3.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating3.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating3.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating3.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c144f8",
   "metadata": {},
   "source": [
    "## Step 8: Comparative Analysis\n",
    "\n",
    "Let's rate multiple tenders and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7568e3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARATIVE RATING\n",
      "======================================================================\n",
      "\n",
      "RANKED OPPORTUNITIES:\n",
      "\n",
      "1. Complex ML System\n",
      "   Overall: 6.5 | Fit: 8.0 | Win: 6.0 | Effort: 7.5\n",
      "   Value: $980K\n",
      "   ‚Üí GO - Strategic fit and client value outweigh the resource risks....\n",
      "\n",
      "2. Security Assessment\n",
      "   Overall: 6.5 | Fit: 8.0 | Win: 6.0 | Effort: 7.5\n",
      "   Value: $180K\n",
      "   ‚Üí GO - Strategic fit outweighs moderate competition risk....\n",
      "\n",
      "3. Simple Web App\n",
      "   Overall: 5.0 | Fit: 6.0 | Win: 4.0 | Effort: 8.0\n",
      "   Value: $450K\n",
      "   ‚Üí NO-GO - High effort and low win probability outweigh strategic fit....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create diverse batch\n",
    "batch_tenders = [\n",
    "    (\"Simple Web App\", Tender(\n",
    "        id=\"B1\",\n",
    "        title=\"Citizen Portal Development\",\n",
    "        description=\"Build responsive web portal for permit applications. Standard tech stack, 6-month timeline.\",\n",
    "        organization=\"City Services\",\n",
    "        deadline=\"2025-01-31\",\n",
    "        estimated_value=\"$450K\"\n",
    "    ), [\"software\"]),\n",
    "    \n",
    "    (\"Complex ML System\", Tender(\n",
    "        id=\"B2\",\n",
    "        title=\"Predictive Maintenance ML Platform\",\n",
    "        description=\"ML system for predicting infrastructure failures. Real-time analytics, IoT integration.\",\n",
    "        organization=\"Transportation Authority\",\n",
    "        deadline=\"2025-03-01\",\n",
    "        estimated_value=\"$980K\"\n",
    "    ), [\"ai\", \"software\"]),\n",
    "    \n",
    "    (\"Security Assessment\", Tender(\n",
    "        id=\"B3\",\n",
    "        title=\"Annual Penetration Testing\",\n",
    "        description=\"Quarterly pentesting of 20 web applications. Reports and remediation guidance.\",\n",
    "        organization=\"State IT Security\",\n",
    "        deadline=\"2024-12-01\",\n",
    "        estimated_value=\"$180K\"\n",
    "    ), [\"cybersecurity\"]),\n",
    "]\n",
    "\n",
    "print(\"COMPARATIVE RATING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ratings = []\n",
    "for name, tender, categories in batch_tenders:\n",
    "    rating = await rate_tender(tender, categories)\n",
    "    ratings.append((name, tender, rating))\n",
    "\n",
    "# Sort by overall score\n",
    "ratings.sort(key=lambda x: x[2].overall_score, reverse=True)\n",
    "\n",
    "print(\"\\nRANKED OPPORTUNITIES:\\n\")\n",
    "for i, (name, tender, rating) in enumerate(ratings, 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Overall: {rating.overall_score:.1f} | \"\n",
    "          f\"Fit: {rating.strategic_fit:.1f} | \"\n",
    "          f\"Win: {rating.win_probability:.1f} | \"\n",
    "          f\"Effort: {rating.effort_required:.1f}\")\n",
    "    print(f\"   Value: {tender.estimated_value}\")\n",
    "    print(f\"   ‚Üí {rating.recommendation[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d58f",
   "metadata": {},
   "source": [
    "## Step 9: Production-Ready Class\n",
    "\n",
    "Wrap everything in a reusable class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e048e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 6.5/10\n",
      "Proceed to bid document: False\n",
      "Recommendation: NO-GO - High effort and moderate win probability outweigh strategic fit.\n"
     ]
    }
   ],
   "source": [
    "class RatingAgent:\n",
    "    \"\"\"\n",
    "    Production-ready Rating Agent\n",
    "    \n",
    "    Evaluates tender opportunities on multiple dimensions:\n",
    "    - Strategic fit with company capabilities\n",
    "    - Win probability considering competition\n",
    "    - Effort required vs resources available\n",
    "    \n",
    "    Returns structured rating with explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_url: str = BASE_URL,\n",
    "        temperature: float = 0.1,\n",
    "        min_score: float = 7.0\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.temperature = temperature\n",
    "        self.min_score = min_score\n",
    "    \n",
    "    async def rate(\n",
    "        self, \n",
    "        tender: Tender, \n",
    "        categories: List[str]\n",
    "    ) -> RatingResult:\n",
    "        \"\"\"Rate a tender opportunity\"\"\"\n",
    "        return await rate_tender(tender, categories, self.temperature)\n",
    "    \n",
    "    def should_proceed(self, rating: RatingResult) -> bool:\n",
    "        \"\"\"Business logic: should we generate bid documents?\"\"\"\n",
    "        return rating.overall_score >= self.min_score\n",
    "\n",
    "# Test the class\n",
    "agent = RatingAgent(min_score=7.0)\n",
    "\n",
    "test = Tender(\n",
    "    id=\"CLASS-TEST\",\n",
    "    title=\"Cloud Security Audit\",\n",
    "    description=\"Comprehensive security audit of AWS infrastructure\",\n",
    "    organization=\"Tech Startup\",\n",
    "    deadline=\"2024-12-01\",\n",
    "    estimated_value=\"$120K\"\n",
    ")\n",
    "\n",
    "result = await agent.rate(test, [\"cybersecurity\"])\n",
    "proceed = agent.should_proceed(result)\n",
    "\n",
    "print(f\"Rating: {result.overall_score:.1f}/10\")\n",
    "print(f\"Proceed to bid document: {proceed}\")\n",
    "print(f\"Recommendation: {result.recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae6ed2",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You built a sophisticated rating agent!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "1. **Multi-dimensional scoring** - Avoid single-number bias\n",
    "2. **Balanced analysis** - Force consideration of both strengths and risks\n",
    "3. **Company context matters** - Prompt includes capabilities and constraints\n",
    "4. **Comparative ranking** - Multiple scores enable prioritization\n",
    "5. **Business logic integration** - Thresholds and rules on top of AI\n",
    "\n",
    "## Design Decisions\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|----------|\n",
    "| Temperature 0.1 | Consistent scoring across tenders |\n",
    "| Multiple dimensions | More nuanced than single score |\n",
    "| Required strengths AND risks | Prevents overly optimistic ratings |\n",
    "| 0-10 scale | Intuitive and fine-grained |\n",
    "| Company profile in prompt | Context for realistic assessment |\n",
    "\n",
    "## Prompt Engineering Lessons\n",
    "\n",
    "1. **Explicit calibration** - \"Most should score 5-7, not 8-10\"\n",
    "2. **Multiple perspectives** - Force analysis from different angles\n",
    "3. **Concrete criteria** - Not just \"rate this\", but \"rate on X, Y, Z\"\n",
    "4. **Role definition** - \"15 years experience\" sets expectation\n",
    "5. **Balanced instructions** - \"Realistic, not optimistic\"\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now we can:\n",
    "1. ‚úì Filter tenders for relevance\n",
    "2. ‚úì Rate opportunities on multiple dimensions\n",
    "3. ? Generate professional bid documents\n",
    "\n",
    "Let's build the document generator!\n",
    "\n",
    "‚û°Ô∏è Continue to `05_doc_generator.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
