{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2272bcd1",
   "metadata": {},
   "source": [
    "# 04: Building the Rating Agent\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Multi-dimensional scoring with LLMs\n",
    "- Business logic in AI agents\n",
    "- Prompt engineering for analytical tasks\n",
    "- Balancing objective metrics with subjective reasoning\n",
    "\n",
    "**What We're Building:**\n",
    "An agent that evaluates filtered tenders on multiple dimensions (strategic fit, win probability, effort required) to help prioritize opportunities. This is Agent #2 in our pipeline.\n",
    "\n",
    "**The Challenge:**\n",
    "\"Is it relevant?\" is easy. \"Is it worth bidding on?\" requires business judgment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2a202",
   "metadata": {},
   "source": [
    "## Why Rating Is Harder Than Filtering\n",
    "\n",
    "Filtering is binary: relevant or not.\n",
    "\n",
    "Rating requires:\n",
    "- Multiple dimensions (fit, probability, effort)\n",
    "- Comparative judgment (is 7.5 better than 6.8?)\n",
    "- Business context (company capabilities, competition)\n",
    "- Risk assessment (what could go wrong?)\n",
    "\n",
    "This is where LLMs shine: fuzzy judgment based on incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6262f6",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76822c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: anyio in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12f8c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports ready\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Type, TypeVar\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "BASE_URL = \"http://localhost:1234/v1\"\n",
    "MODEL = \"local-model\"\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "print(\"‚úì Imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5651ce",
   "metadata": {},
   "source": [
    "## Step 2: Define Rating Output Schema\n",
    "\n",
    "A good rating has multiple dimensions, not just a single score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f2a7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example rating:\n",
      "{\n",
      "  \"overall_score\": 8.5,\n",
      "  \"strategic_fit\": 9.0,\n",
      "  \"win_probability\": 7.5,\n",
      "  \"effort_required\": 8.0,\n",
      "  \"strengths\": [\n",
      "    \"Perfect match for our AI cybersecurity expertise\",\n",
      "    \"High-value contract with long-term potential\",\n",
      "    \"Existing relationship with client organization\"\n",
      "  ],\n",
      "  \"risks\": [\n",
      "    \"Tight timeline may strain resources\",\n",
      "    \"Likely to attract large competitors\",\n",
      "    \"Complex integration requirements\"\n",
      "  ],\n",
      "  \"recommendation\": \"GO - Strong strategic fit and reasonable win probability justify the effort.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class RatingResult(BaseModel):\n",
    "    \"\"\"Multi-dimensional tender rating\"\"\"\n",
    "    \n",
    "    # Scores (0-10)\n",
    "    overall_score: float = Field(description=\"Overall opportunity score 0-10\", ge=0, le=10)\n",
    "    strategic_fit: float = Field(description=\"How well this matches our expertise 0-10\", ge=0, le=10)\n",
    "    win_probability: float = Field(description=\"Likelihood of winning 0-10\", ge=0, le=10)\n",
    "    effort_required: float = Field(description=\"Complexity and resource needs 0-10\", ge=0, le=10)\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    strengths: List[str] = Field(description=\"Top 3 strengths/opportunities\")\n",
    "    risks: List[str] = Field(description=\"Top 3 risks/challenges\")\n",
    "    recommendation: str = Field(description=\"Go/No-Go recommendation with reasoning\")\n",
    "\n",
    "# Test the model\n",
    "example = RatingResult(\n",
    "    overall_score=8.5,\n",
    "    strategic_fit=9.0,\n",
    "    win_probability=7.5,\n",
    "    effort_required=8.0,\n",
    "    strengths=[\n",
    "        \"Perfect match for our AI cybersecurity expertise\",\n",
    "        \"High-value contract with long-term potential\",\n",
    "        \"Existing relationship with client organization\"\n",
    "    ],\n",
    "    risks=[\n",
    "        \"Tight timeline may strain resources\",\n",
    "        \"Likely to attract large competitors\",\n",
    "        \"Complex integration requirements\"\n",
    "    ],\n",
    "    recommendation=\"GO - Strong strategic fit and reasonable win probability justify the effort.\"\n",
    ")\n",
    "\n",
    "print(\"Example rating:\")\n",
    "print(example.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edab8b",
   "metadata": {},
   "source": [
    "## Step 3: Build LLM Helper Function\n",
    "\n",
    "**Important Learning:** In notebooks 02 and 03, we showed the LLM the JSON schema (the structure definition). That works, but LLMs sometimes get confused and return the schema itself instead of actual data!\n",
    "\n",
    "**The Problem:**\n",
    "```python\n",
    "# Schema approach (what we used before)\n",
    "schema = {\"properties\": {\"score\": {\"type\": \"number\"}}}\n",
    "# LLM sees: \"type\": \"number\" \n",
    "# LLM returns: {\"properties\": {\"score\": {\"type\": \"number\"}}} ‚ùå\n",
    "```\n",
    "\n",
    "**The Solution:**\n",
    "Show the LLM a concrete example instead! LLMs understand examples better than abstract schemas.\n",
    "\n",
    "```python\n",
    "# Example approach (what we use now)\n",
    "example = {\"score\": 8.5}\n",
    "# LLM sees: 8.5\n",
    "# LLM returns: {\"score\": 7.2} ‚úÖ\n",
    "```\n",
    "\n",
    "This is a **production pattern** used in real systems. Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b30a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM helper ready\n"
     ]
    }
   ],
   "source": [
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def build_structured_prompt(prompt: str, model_class: Type[BaseModel]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt with concrete example instead of abstract schema.\n",
    "    \n",
    "    Why? LLMs understand examples better than JSON schemas.\n",
    "    Showing {\"score\": 8.5} is clearer than showing {\"type\": \"number\", \"min\": 0}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build a concrete example based on the model\n",
    "    if model_class.__name__ == \"RatingResult\":\n",
    "        # For RatingResult, show realistic rating values\n",
    "        example = {\n",
    "            \"overall_score\": 8.5,\n",
    "            \"strategic_fit\": 9.0,\n",
    "            \"win_probability\": 7.5,\n",
    "            \"effort_required\": 8.0,\n",
    "            \"strengths\": [\n",
    "                \"Strong match for our expertise\",\n",
    "                \"Reasonable contract value\",\n",
    "                \"Good client relationship\"\n",
    "            ],\n",
    "            \"risks\": [\n",
    "                \"Tight timeline may be challenging\",\n",
    "                \"Competition from larger firms\",\n",
    "                \"Technical complexity\"\n",
    "            ],\n",
    "            \"recommendation\": \"GO - Strategic fit justifies the effort.\"\n",
    "        }\n",
    "    else:\n",
    "        # Fallback: use schema if we don't have a specific example\n",
    "        example = model_class.model_json_schema()\n",
    "    \n",
    "    return f\"\"\"{prompt}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "You must respond with ONLY a valid JSON object containing YOUR ANALYSIS.\n",
    "Do NOT return a schema, return ACTUAL DATA.\n",
    "\n",
    "EXPECTED FORMAT (fill in with your actual analysis):\n",
    "{json.dumps(example, indent=2)}\n",
    "\n",
    "IMPORTANT:\n",
    "- Use actual numbers (like 8.5), not descriptions\n",
    "- Use actual text in arrays (3 items for strengths and risks)\n",
    "- All fields are required\n",
    "- No markdown, no code blocks, no explanations\n",
    "- Start with {{ and end with }}\n",
    "\n",
    "Your JSON response:\"\"\"\n",
    "\n",
    "async def call_llm(\n",
    "    prompt: str,\n",
    "    response_model: Type[T],\n",
    "    system_prompt: str,\n",
    "    temperature: float = 0.1,\n",
    "    max_retries: int = 3\n",
    ") -> T:\n",
    "    \"\"\"Call LLM with structured output and retries\"\"\"\n",
    "    \n",
    "    full_prompt = build_structured_prompt(prompt, response_model)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "                response = await client.post(\n",
    "                    f\"{BASE_URL}/chat/completions\",\n",
    "                    json={\n",
    "                        \"model\": MODEL,\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": full_prompt}\n",
    "                        ],\n",
    "                        \"temperature\": temperature,\n",
    "                    },\n",
    "                )\n",
    "                \n",
    "                result = response.json()\n",
    "                content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                \n",
    "                # Clean response - remove markdown code blocks\n",
    "                content = content.strip()\n",
    "                if content.startswith(\"```json\"):\n",
    "                    content = content[7:]\n",
    "                if content.startswith(\"```\"):\n",
    "                    content = content[3:]\n",
    "                if content.endswith(\"```\"):\n",
    "                    content = content[:-3]\n",
    "                content = content.strip()\n",
    "                \n",
    "                # Extract JSON by finding balanced braces (handles extra text)\n",
    "                start_idx = content.find('{')\n",
    "                if start_idx != -1:\n",
    "                    brace_count = 0\n",
    "                    end_idx = -1\n",
    "                    for i in range(start_idx, len(content)):\n",
    "                        if content[i] == '{':\n",
    "                            brace_count += 1\n",
    "                        elif content[i] == '}':\n",
    "                            brace_count -= 1\n",
    "                            if brace_count == 0:\n",
    "                                end_idx = i\n",
    "                                break\n",
    "                    if end_idx != -1:\n",
    "                        content = content[start_idx:end_idx + 1]\n",
    "                \n",
    "                # Parse and validate\n",
    "                data = json.loads(content)\n",
    "                return response_model.model_validate(data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\" !! Attempt {attempt + 1} failed: {str(e)[:80]}...\")\n",
    "                print(f\" !! Retrying...\")\n",
    "                await asyncio.sleep(1)\n",
    "            else:\n",
    "                raise Exception(f\"Failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "print(\"‚úì LLM helper ready (using example-based prompts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332f1bf",
   "metadata": {},
   "source": [
    "## Step 4: Build the Rating Agent\n",
    "\n",
    "This is where prompt engineering matters. We need to guide the LLM to think like a business analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78943bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Rating agent ready\n"
     ]
    }
   ],
   "source": [
    "class Tender(BaseModel):\n",
    "    \"\"\"Input tender data\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    organization: str\n",
    "    deadline: str\n",
    "    estimated_value: str | None = None\n",
    "\n",
    "async def rate_tender(\n",
    "    tender: Tender, \n",
    "    categories: List[str],\n",
    "    temperature: float = 0.1\n",
    ") -> RatingResult:\n",
    "    \"\"\"\n",
    "    Rating Agent: Evaluate business opportunity\n",
    "    \n",
    "    Key design decisions:\n",
    "    - Low temperature (0.1) for consistent scoring\n",
    "    - Multiple dimensions to avoid single-number bias\n",
    "    - Explicit company context in prompt\n",
    "    - Required strengths AND risks (balanced view)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Rate this tender opportunity for a small tech consultancy:\n",
    "\n",
    "TENDER DETAILS:\n",
    "Title: {tender.title}\n",
    "Client: {tender.organization}\n",
    "Value: {tender.estimated_value or \"Not specified\"}\n",
    "Deadline: {tender.deadline}\n",
    "Categories: {', '.join(categories)}\n",
    "\n",
    "DESCRIPTION:\n",
    "{tender.description}\n",
    "\n",
    "OUR COMPANY PROFILE:\n",
    "- Small tech consultancy (10-15 people)\n",
    "- Core expertise: AI/ML, Cybersecurity, Software Development\n",
    "- Strong technical skills, limited by team size\n",
    "- Track record with mid-sized government contracts\n",
    "- Prefer projects lasting 3-12 months\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "\n",
    "1. STRATEGIC FIT (0-10):\n",
    "   - How well does this match our expertise in {', '.join(categories)}?\n",
    "   - Does it leverage our unique strengths?\n",
    "   - Will it build valuable capabilities or relationships?\n",
    "\n",
    "2. WIN PROBABILITY (0-10):\n",
    "   - Are we genuinely competitive for this?\n",
    "   - What's the likely competition (size, specialization)?\n",
    "   - Do we have relevant experience and credibility?\n",
    "\n",
    "3. EFFORT REQUIRED (0-10):\n",
    "   - Technical complexity and scope\n",
    "   - Resource requirements vs our team size\n",
    "   - Timeline pressure and delivery risk\n",
    "\n",
    "4. OVERALL SCORE (0-10):\n",
    "   - Weighted assessment considering all factors\n",
    "   - Value of opportunity vs investment required\n",
    "\n",
    "Provide REALISTIC scores (not optimistic). Most opportunities should score 5-7.\n",
    "Identify specific strengths and concrete risks.\n",
    "Give clear Go/No-Go recommendation.\n",
    "\"\"\"\n",
    "    \n",
    "    system = \"\"\"You are a business development expert evaluating tender opportunities.\n",
    "You have 15 years of experience in government contracting and tech consulting.\n",
    "Be analytical and realistic, not optimistic. Consider both opportunity and risk.\"\"\"\n",
    "    \n",
    "    return await call_llm(\n",
    "        prompt=prompt,\n",
    "        response_model=RatingResult,\n",
    "        system_prompt=system,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "print(\"‚úì Rating agent ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c5c87",
   "metadata": {},
   "source": [
    "## Step 5: Test with High-Value Opportunity\n",
    "\n",
    "Let's rate a tender that should score well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d363709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 1: Strong Opportunity\n",
      "======================================================================\n",
      " !! Attempt 1 failed, retrying...\n",
      " !! Attempt 2 failed, retrying...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed after 3 attempts: Extra data: line 1 column 1102 (char 1101)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mcall_llm\u001b[39m\u001b[34m(prompt, response_model, system_prompt, temperature, max_retries)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Parse and validate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response_model.model_validate(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:340\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExtra data\u001b[39m\u001b[33m\"\u001b[39m, s, end)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 1 column 1102 (char 1101)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m      4\u001b[39m tender1 = Tender(\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mid\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mR001\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     title=\u001b[33m\"\u001b[39m\u001b[33mAI-Powered Fraud Detection System\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     estimated_value=\u001b[33m\"\u001b[39m\u001b[33m$850K\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m rating1 = \u001b[38;5;28;01mawait\u001b[39;00m rate_tender(\n\u001b[32m     17\u001b[39m     tender1, \n\u001b[32m     18\u001b[39m     categories=[\u001b[33m\"\u001b[39m\u001b[33mai\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msoftware\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrating1.overall_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategic Fit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrating1.strategic_fit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/10\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mrate_tender\u001b[39m\u001b[34m(tender, categories, temperature)\u001b[39m\n\u001b[32m     25\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mRate this tender opportunity for a small tech consultancy:\u001b[39m\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[33mTENDER DETAILS:\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[33mGive clear Go/No-Go recommendation.\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     70\u001b[39m     system = \u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a business development expert evaluating tender opportunities.\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33mYou have 15 years of experience in government contracting and tech consulting.\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33mBe analytical and realistic, not optimistic. Consider both opportunity and risk.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m call_llm(\n\u001b[32m     75\u001b[39m         prompt=prompt,\n\u001b[32m     76\u001b[39m         response_model=RatingResult,\n\u001b[32m     77\u001b[39m         system_prompt=system,\n\u001b[32m     78\u001b[39m         temperature=temperature\n\u001b[32m     79\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mcall_llm\u001b[39m\u001b[34m(prompt, response_model, system_prompt, temperature, max_retries)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attempts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Failed after 3 attempts: Extra data: line 1 column 1102 (char 1101)"
     ]
    }
   ],
   "source": [
    "print(\"TEST 1: Strong Opportunity\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender1 = Tender(\n",
    "    id=\"R001\",\n",
    "    title=\"AI-Powered Fraud Detection System\",\n",
    "    description=\"\"\"Develop machine learning system to detect fraudulent transactions \n",
    "    in real-time. Must integrate with existing payment processing infrastructure. \n",
    "    Project includes model development, deployment, and 6 months of monitoring and \n",
    "    refinement. Team will work closely with our data science division.\"\"\",\n",
    "    organization=\"State Financial Services Commission\",\n",
    "    deadline=\"2025-02-01\",\n",
    "    estimated_value=\"$850K\"\n",
    ")\n",
    "\n",
    "rating1 = await rate_tender(\n",
    "    tender1, \n",
    "    categories=[\"ai\", \"software\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating1.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating1.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating1.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating1.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating1.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating1.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating1.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bffd3",
   "metadata": {},
   "source": [
    "## Step 6: Test with Poor Fit\n",
    "\n",
    "A tender that's technically relevant but a bad business opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 2: Poor Fit (Scale Mismatch)\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "7 validation errors for RatingResult\noverall_score\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nstrategic_fit\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nwin_probability\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\neffort_required\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nstrengths\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nrisks\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nrecommendation\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m      4\u001b[39m tender2 = Tender(\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mid\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mR002\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     title=\u001b[33m\"\u001b[39m\u001b[33mNational Cybersecurity Infrastructure Modernization\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     estimated_value=\u001b[33m\"\u001b[39m\u001b[33m$250M\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m rating2 = \u001b[38;5;28;01mawait\u001b[39;00m rate_tender(\n\u001b[32m     17\u001b[39m     tender2,\n\u001b[32m     18\u001b[39m     categories=[\u001b[33m\"\u001b[39m\u001b[33mcybersecurity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrating2.overall_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStrategic Fit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrating2.strategic_fit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/10\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mrate_tender\u001b[39m\u001b[34m(tender, categories, temperature)\u001b[39m\n\u001b[32m     25\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mRate this tender opportunity for a small tech consultancy:\u001b[39m\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[33mTENDER DETAILS:\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[33mGive clear Go/No-Go recommendation.\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     70\u001b[39m     system = \u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a business development expert evaluating tender opportunities.\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33mYou have 15 years of experience in government contracting and tech consulting.\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33mBe analytical and realistic, not optimistic. Consider both opportunity and risk.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m call_llm(\n\u001b[32m     75\u001b[39m         prompt=prompt,\n\u001b[32m     76\u001b[39m         response_model=RatingResult,\n\u001b[32m     77\u001b[39m         system_prompt=system,\n\u001b[32m     78\u001b[39m         temperature=temperature\n\u001b[32m     79\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mcall_llm\u001b[39m\u001b[34m(prompt, response_model, system_prompt, temperature)\u001b[39m\n\u001b[32m     61\u001b[39m         content = content[start_idx:end_idx + \u001b[32m1\u001b[39m]\n\u001b[32m     63\u001b[39m data = json.loads(content)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages/pydantic/main.py:716\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, extra, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    712\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    713\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    714\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 7 validation errors for RatingResult\noverall_score\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nstrategic_fit\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nwin_probability\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\neffort_required\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nstrengths\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nrisks\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nrecommendation\n  Field required [type=missing, input_value={'description': 'Multi-di...sult', 'type': 'object'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing"
     ]
    }
   ],
   "source": [
    "print(\"TEST 2: Poor Fit (Scale Mismatch)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender2 = Tender(\n",
    "    id=\"R002\",\n",
    "    title=\"National Cybersecurity Infrastructure Modernization\",\n",
    "    description=\"\"\"Massive 5-year program to modernize cybersecurity infrastructure \n",
    "    across all federal agencies. Requires dedicated team of 100+ engineers, \n",
    "    proven experience with large-scale deployments, and existing national security \n",
    "    clearances. Prime contractor will coordinate 10+ subcontractors.\"\"\",\n",
    "    organization=\"Department of Homeland Security\",\n",
    "    deadline=\"2024-11-01\",\n",
    "    estimated_value=\"$250M\"\n",
    ")\n",
    "\n",
    "rating2 = await rate_tender(\n",
    "    tender2,\n",
    "    categories=[\"cybersecurity\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating2.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating2.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating2.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating2.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating2.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating2.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating2.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb46357",
   "metadata": {},
   "source": [
    "## Step 7: Test Edge Case - High Risk, High Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST 3: High Risk, High Reward\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender3 = Tender(\n",
    "    id=\"R003\",\n",
    "    title=\"Experimental AI Research Platform\",\n",
    "    description=\"\"\"Build novel AI research platform using cutting-edge techniques \n",
    "    (federated learning, differential privacy, quantum-resistant cryptography). \n",
    "    No existing commercial solutions. High technical risk but potential for \n",
    "    groundbreaking capabilities. 18-month timeline with staged milestones.\"\"\",\n",
    "    organization=\"Defense Advanced Research Agency\",\n",
    "    deadline=\"2024-12-15\",\n",
    "    estimated_value=\"$1.2M\"\n",
    ")\n",
    "\n",
    "rating3 = await rate_tender(\n",
    "    tender3,\n",
    "    categories=[\"ai\", \"cybersecurity\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating3.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating3.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating3.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating3.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating3.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating3.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating3.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c144f8",
   "metadata": {},
   "source": [
    "## Step 8: Comparative Analysis\n",
    "\n",
    "Let's rate multiple tenders and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse batch\n",
    "batch_tenders = [\n",
    "    (\"Simple Web App\", Tender(\n",
    "        id=\"B1\",\n",
    "        title=\"Citizen Portal Development\",\n",
    "        description=\"Build responsive web portal for permit applications. Standard tech stack, 6-month timeline.\",\n",
    "        organization=\"City Services\",\n",
    "        deadline=\"2025-01-31\",\n",
    "        estimated_value=\"$450K\"\n",
    "    ), [\"software\"]),\n",
    "    \n",
    "    (\"Complex ML System\", Tender(\n",
    "        id=\"B2\",\n",
    "        title=\"Predictive Maintenance ML Platform\",\n",
    "        description=\"ML system for predicting infrastructure failures. Real-time analytics, IoT integration.\",\n",
    "        organization=\"Transportation Authority\",\n",
    "        deadline=\"2025-03-01\",\n",
    "        estimated_value=\"$980K\"\n",
    "    ), [\"ai\", \"software\"]),\n",
    "    \n",
    "    (\"Security Assessment\", Tender(\n",
    "        id=\"B3\",\n",
    "        title=\"Annual Penetration Testing\",\n",
    "        description=\"Quarterly pentesting of 20 web applications. Reports and remediation guidance.\",\n",
    "        organization=\"State IT Security\",\n",
    "        deadline=\"2024-12-01\",\n",
    "        estimated_value=\"$180K\"\n",
    "    ), [\"cybersecurity\"]),\n",
    "]\n",
    "\n",
    "print(\"COMPARATIVE RATING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ratings = []\n",
    "for name, tender, categories in batch_tenders:\n",
    "    rating = await rate_tender(tender, categories)\n",
    "    ratings.append((name, tender, rating))\n",
    "\n",
    "# Sort by overall score\n",
    "ratings.sort(key=lambda x: x[2].overall_score, reverse=True)\n",
    "\n",
    "print(\"\\nRANKED OPPORTUNITIES:\\n\")\n",
    "for i, (name, tender, rating) in enumerate(ratings, 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Overall: {rating.overall_score:.1f} | \"\n",
    "          f\"Fit: {rating.strategic_fit:.1f} | \"\n",
    "          f\"Win: {rating.win_probability:.1f} | \"\n",
    "          f\"Effort: {rating.effort_required:.1f}\")\n",
    "    print(f\"   Value: {tender.estimated_value}\")\n",
    "    print(f\"   ‚Üí {rating.recommendation[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d58f",
   "metadata": {},
   "source": [
    "## Step 9: Production-Ready Class\n",
    "\n",
    "Wrap everything in a reusable class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e048e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingAgent:\n",
    "    \"\"\"\n",
    "    Production-ready Rating Agent\n",
    "    \n",
    "    Evaluates tender opportunities on multiple dimensions:\n",
    "    - Strategic fit with company capabilities\n",
    "    - Win probability considering competition\n",
    "    - Effort required vs resources available\n",
    "    \n",
    "    Returns structured rating with explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_url: str = BASE_URL,\n",
    "        temperature: float = 0.1,\n",
    "        min_score: float = 7.0\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.temperature = temperature\n",
    "        self.min_score = min_score\n",
    "    \n",
    "    async def rate(\n",
    "        self, \n",
    "        tender: Tender, \n",
    "        categories: List[str]\n",
    "    ) -> RatingResult:\n",
    "        \"\"\"Rate a tender opportunity\"\"\"\n",
    "        return await rate_tender(tender, categories, self.temperature)\n",
    "    \n",
    "    def should_proceed(self, rating: RatingResult) -> bool:\n",
    "        \"\"\"Business logic: should we generate bid documents?\"\"\"\n",
    "        return rating.overall_score >= self.min_score\n",
    "\n",
    "# Test the class\n",
    "agent = RatingAgent(min_score=7.0)\n",
    "\n",
    "test = Tender(\n",
    "    id=\"CLASS-TEST\",\n",
    "    title=\"Cloud Security Audit\",\n",
    "    description=\"Comprehensive security audit of AWS infrastructure\",\n",
    "    organization=\"Tech Startup\",\n",
    "    deadline=\"2024-12-01\",\n",
    "    estimated_value=\"$120K\"\n",
    ")\n",
    "\n",
    "result = await agent.rate(test, [\"cybersecurity\"])\n",
    "proceed = agent.should_proceed(result)\n",
    "\n",
    "print(f\"Rating: {result.overall_score:.1f}/10\")\n",
    "print(f\"Proceed to bid document: {proceed}\")\n",
    "print(f\"Recommendation: {result.recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae6ed2",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You built a sophisticated rating agent!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "1. **Multi-dimensional scoring** - Avoid single-number bias\n",
    "2. **Balanced analysis** - Force consideration of both strengths and risks\n",
    "3. **Company context matters** - Prompt includes capabilities and constraints\n",
    "4. **Comparative ranking** - Multiple scores enable prioritization\n",
    "5. **Business logic integration** - Thresholds and rules on top of AI\n",
    "\n",
    "## Design Decisions\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|----------|\n",
    "| Temperature 0.1 | Consistent scoring across tenders |\n",
    "| Multiple dimensions | More nuanced than single score |\n",
    "| Required strengths AND risks | Prevents overly optimistic ratings |\n",
    "| 0-10 scale | Intuitive and fine-grained |\n",
    "| Company profile in prompt | Context for realistic assessment |\n",
    "\n",
    "## Prompt Engineering Lessons\n",
    "\n",
    "1. **Explicit calibration** - \"Most should score 5-7, not 8-10\"\n",
    "2. **Multiple perspectives** - Force analysis from different angles\n",
    "3. **Concrete criteria** - Not just \"rate this\", but \"rate on X, Y, Z\"\n",
    "4. **Role definition** - \"15 years experience\" sets expectation\n",
    "5. **Balanced instructions** - \"Realistic, not optimistic\"\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now we can:\n",
    "1. ‚úì Filter tenders for relevance\n",
    "2. ‚úì Rate opportunities on multiple dimensions\n",
    "3. ? Generate professional bid documents\n",
    "\n",
    "Let's build the document generator!\n",
    "\n",
    "‚û°Ô∏è Continue to `05_doc_generator.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
