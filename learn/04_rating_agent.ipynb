{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2272bcd1",
   "metadata": {},
   "source": [
    "# 04: Building the Rating Agent\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Multi-dimensional scoring with LLMs\n",
    "- Business logic in AI agents\n",
    "- Prompt engineering for analytical tasks\n",
    "- Balancing objective metrics with subjective reasoning\n",
    "\n",
    "**What We're Building:**\n",
    "An agent that evaluates filtered tenders on multiple dimensions (strategic fit, win probability, effort required) to help prioritize opportunities. This is Agent #2 in our pipeline.\n",
    "\n",
    "**The Challenge:**\n",
    "\"Is it relevant?\" is easy. \"Is it worth bidding on?\" requires business judgment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2a202",
   "metadata": {},
   "source": [
    "## Why Rating Is Harder Than Filtering\n",
    "\n",
    "Filtering is binary: relevant or not.\n",
    "\n",
    "Rating requires:\n",
    "- Multiple dimensions (fit, probability, effort)\n",
    "- Comparative judgment (is 7.5 better than 6.8?)\n",
    "- Business context (company capabilities, competition)\n",
    "- Risk assessment (what could go wrong?)\n",
    "\n",
    "This is where LLMs shine: fuzzy judgment based on incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6262f6",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76822c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpx pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Type, TypeVar\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "BASE_URL = \"http://localhost:1234/v1\"\n",
    "MODEL = \"local-model\"\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "print(\"‚úì Imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5651ce",
   "metadata": {},
   "source": [
    "## Step 2: Define Rating Output Schema\n",
    "\n",
    "A good rating has multiple dimensions, not just a single score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingResult(BaseModel):\n",
    "    \"\"\"Multi-dimensional tender rating\"\"\"\n",
    "    \n",
    "    # Scores (0-10)\n",
    "    overall_score: float = Field(description=\"Overall opportunity score 0-10\", ge=0, le=10)\n",
    "    strategic_fit: float = Field(description=\"How well this matches our expertise 0-10\", ge=0, le=10)\n",
    "    win_probability: float = Field(description=\"Likelihood of winning 0-10\", ge=0, le=10)\n",
    "    effort_required: float = Field(description=\"Complexity and resource needs 0-10\", ge=0, le=10)\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    strengths: List[str] = Field(description=\"Top 3 strengths/opportunities\")\n",
    "    risks: List[str] = Field(description=\"Top 3 risks/challenges\")\n",
    "    recommendation: str = Field(description=\"Go/No-Go recommendation with reasoning\")\n",
    "\n",
    "# Test the model\n",
    "example = RatingResult(\n",
    "    overall_score=8.5,\n",
    "    strategic_fit=9.0,\n",
    "    win_probability=7.5,\n",
    "    effort_required=8.0,\n",
    "    strengths=[\n",
    "        \"Perfect match for our AI cybersecurity expertise\",\n",
    "        \"High-value contract with long-term potential\",\n",
    "        \"Existing relationship with client organization\"\n",
    "    ],\n",
    "    risks=[\n",
    "        \"Tight timeline may strain resources\",\n",
    "        \"Likely to attract large competitors\",\n",
    "        \"Complex integration requirements\"\n",
    "    ],\n",
    "    recommendation=\"GO - Strong strategic fit and reasonable win probability justify the effort.\"\n",
    ")\n",
    "\n",
    "print(\"Example rating:\")\n",
    "print(example.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edab8b",
   "metadata": {},
   "source": [
    "## Step 3: Build LLM Helper Function\n",
    "\n",
    "Reusing our structured output pattern from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_structured_prompt(prompt: str, model_class: Type[BaseModel]) -> str:\n",
    "    \"\"\"Add schema to prompt\"\"\"\n",
    "    schema = model_class.model_json_schema()\n",
    "    return f\"\"\"{prompt}\n",
    "\n",
    "CRITICAL: Respond with ONLY valid JSON matching this schema:\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Return ONLY the raw JSON object, no markdown or explanatory text.\n",
    "\"\"\"\n",
    "\n",
    "async def call_llm(\n",
    "    prompt: str,\n",
    "    response_model: Type[T],\n",
    "    system_prompt: str,\n",
    "    temperature: float = 0.1\n",
    ") -> T:\n",
    "    \"\"\"Call LLM with structured output\"\"\"\n",
    "    \n",
    "    full_prompt = build_structured_prompt(prompt, response_model)\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{BASE_URL}/chat/completions\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": full_prompt}\n",
    "                ],\n",
    "                \"temperature\": temperature,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        result = response.json()\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Clean response\n",
    "        content = content.strip()\n",
    "        for marker in [\"```json\", \"```\"]:\n",
    "            if content.startswith(marker):\n",
    "                content = content[len(marker):]\n",
    "            if content.endswith(marker):\n",
    "                content = content[:-len(marker)]\n",
    "        content = content.strip()\n",
    "        \n",
    "        data = json.loads(content)\n",
    "        return response_model.model_validate(data)\n",
    "\n",
    "print(\"‚úì LLM helper ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332f1bf",
   "metadata": {},
   "source": [
    "## Step 4: Build the Rating Agent\n",
    "\n",
    "This is where prompt engineering matters. We need to guide the LLM to think like a business analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78943bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tender(BaseModel):\n",
    "    \"\"\"Input tender data\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    organization: str\n",
    "    deadline: str\n",
    "    estimated_value: str | None = None\n",
    "\n",
    "async def rate_tender(\n",
    "    tender: Tender, \n",
    "    categories: List[str],\n",
    "    temperature: float = 0.1\n",
    ") -> RatingResult:\n",
    "    \"\"\"\n",
    "    Rating Agent: Evaluate business opportunity\n",
    "    \n",
    "    Key design decisions:\n",
    "    - Low temperature (0.1) for consistent scoring\n",
    "    - Multiple dimensions to avoid single-number bias\n",
    "    - Explicit company context in prompt\n",
    "    - Required strengths AND risks (balanced view)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Rate this tender opportunity for a small tech consultancy:\n",
    "\n",
    "TENDER DETAILS:\n",
    "Title: {tender.title}\n",
    "Client: {tender.organization}\n",
    "Value: {tender.estimated_value or \"Not specified\"}\n",
    "Deadline: {tender.deadline}\n",
    "Categories: {', '.join(categories)}\n",
    "\n",
    "DESCRIPTION:\n",
    "{tender.description}\n",
    "\n",
    "OUR COMPANY PROFILE:\n",
    "- Small tech consultancy (10-15 people)\n",
    "- Core expertise: AI/ML, Cybersecurity, Software Development\n",
    "- Strong technical skills, limited by team size\n",
    "- Track record with mid-sized government contracts\n",
    "- Prefer projects lasting 3-12 months\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "\n",
    "1. STRATEGIC FIT (0-10):\n",
    "   - How well does this match our expertise in {', '.join(categories)}?\n",
    "   - Does it leverage our unique strengths?\n",
    "   - Will it build valuable capabilities or relationships?\n",
    "\n",
    "2. WIN PROBABILITY (0-10):\n",
    "   - Are we genuinely competitive for this?\n",
    "   - What's the likely competition (size, specialization)?\n",
    "   - Do we have relevant experience and credibility?\n",
    "\n",
    "3. EFFORT REQUIRED (0-10):\n",
    "   - Technical complexity and scope\n",
    "   - Resource requirements vs our team size\n",
    "   - Timeline pressure and delivery risk\n",
    "\n",
    "4. OVERALL SCORE (0-10):\n",
    "   - Weighted assessment considering all factors\n",
    "   - Value of opportunity vs investment required\n",
    "\n",
    "Provide REALISTIC scores (not optimistic). Most opportunities should score 5-7.\n",
    "Identify specific strengths and concrete risks.\n",
    "Give clear Go/No-Go recommendation.\n",
    "\"\"\"\n",
    "    \n",
    "    system = \"\"\"You are a business development expert evaluating tender opportunities.\n",
    "You have 15 years of experience in government contracting and tech consulting.\n",
    "Be analytical and realistic, not optimistic. Consider both opportunity and risk.\"\"\"\n",
    "    \n",
    "    return await call_llm(\n",
    "        prompt=prompt,\n",
    "        response_model=RatingResult,\n",
    "        system_prompt=system,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "print(\"‚úì Rating agent ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c5c87",
   "metadata": {},
   "source": [
    "## Step 5: Test with High-Value Opportunity\n",
    "\n",
    "Let's rate a tender that should score well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST 1: Strong Opportunity\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender1 = Tender(\n",
    "    id=\"R001\",\n",
    "    title=\"AI-Powered Fraud Detection System\",\n",
    "    description=\"\"\"Develop machine learning system to detect fraudulent transactions \n",
    "    in real-time. Must integrate with existing payment processing infrastructure. \n",
    "    Project includes model development, deployment, and 6 months of monitoring and \n",
    "    refinement. Team will work closely with our data science division.\"\"\",\n",
    "    organization=\"State Financial Services Commission\",\n",
    "    deadline=\"2025-02-01\",\n",
    "    estimated_value=\"$850K\"\n",
    ")\n",
    "\n",
    "rating1 = await rate_tender(\n",
    "    tender1, \n",
    "    categories=[\"ai\", \"software\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating1.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating1.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating1.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating1.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating1.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating1.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating1.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bffd3",
   "metadata": {},
   "source": [
    "## Step 6: Test with Poor Fit\n",
    "\n",
    "A tender that's technically relevant but a bad business opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST 2: Poor Fit (Scale Mismatch)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender2 = Tender(\n",
    "    id=\"R002\",\n",
    "    title=\"National Cybersecurity Infrastructure Modernization\",\n",
    "    description=\"\"\"Massive 5-year program to modernize cybersecurity infrastructure \n",
    "    across all federal agencies. Requires dedicated team of 100+ engineers, \n",
    "    proven experience with large-scale deployments, and existing national security \n",
    "    clearances. Prime contractor will coordinate 10+ subcontractors.\"\"\",\n",
    "    organization=\"Department of Homeland Security\",\n",
    "    deadline=\"2024-11-01\",\n",
    "    estimated_value=\"$250M\"\n",
    ")\n",
    "\n",
    "rating2 = await rate_tender(\n",
    "    tender2,\n",
    "    categories=[\"cybersecurity\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating2.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating2.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating2.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating2.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating2.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating2.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating2.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb46357",
   "metadata": {},
   "source": [
    "## Step 7: Test Edge Case - High Risk, High Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST 3: High Risk, High Reward\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tender3 = Tender(\n",
    "    id=\"R003\",\n",
    "    title=\"Experimental AI Research Platform\",\n",
    "    description=\"\"\"Build novel AI research platform using cutting-edge techniques \n",
    "    (federated learning, differential privacy, quantum-resistant cryptography). \n",
    "    No existing commercial solutions. High technical risk but potential for \n",
    "    groundbreaking capabilities. 18-month timeline with staged milestones.\"\"\",\n",
    "    organization=\"Defense Advanced Research Agency\",\n",
    "    deadline=\"2024-12-15\",\n",
    "    estimated_value=\"$1.2M\"\n",
    ")\n",
    "\n",
    "rating3 = await rate_tender(\n",
    "    tender3,\n",
    "    categories=[\"ai\", \"cybersecurity\"]\n",
    ")\n",
    "\n",
    "print(f\"Overall Score: {rating3.overall_score}/10\")\n",
    "print(f\"Strategic Fit: {rating3.strategic_fit}/10\")\n",
    "print(f\"Win Probability: {rating3.win_probability}/10\")\n",
    "print(f\"Effort Required: {rating3.effort_required}/10\")\n",
    "print(f\"\\nStrengths:\")\n",
    "for s in rating3.strengths:\n",
    "    print(f\"  + {s}\")\n",
    "print(f\"\\nRisks:\")\n",
    "for r in rating3.risks:\n",
    "    print(f\"  - {r}\")\n",
    "print(f\"\\nRecommendation: {rating3.recommendation}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c144f8",
   "metadata": {},
   "source": [
    "## Step 8: Comparative Analysis\n",
    "\n",
    "Let's rate multiple tenders and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse batch\n",
    "batch_tenders = [\n",
    "    (\"Simple Web App\", Tender(\n",
    "        id=\"B1\",\n",
    "        title=\"Citizen Portal Development\",\n",
    "        description=\"Build responsive web portal for permit applications. Standard tech stack, 6-month timeline.\",\n",
    "        organization=\"City Services\",\n",
    "        deadline=\"2025-01-31\",\n",
    "        estimated_value=\"$450K\"\n",
    "    ), [\"software\"]),\n",
    "    \n",
    "    (\"Complex ML System\", Tender(\n",
    "        id=\"B2\",\n",
    "        title=\"Predictive Maintenance ML Platform\",\n",
    "        description=\"ML system for predicting infrastructure failures. Real-time analytics, IoT integration.\",\n",
    "        organization=\"Transportation Authority\",\n",
    "        deadline=\"2025-03-01\",\n",
    "        estimated_value=\"$980K\"\n",
    "    ), [\"ai\", \"software\"]),\n",
    "    \n",
    "    (\"Security Assessment\", Tender(\n",
    "        id=\"B3\",\n",
    "        title=\"Annual Penetration Testing\",\n",
    "        description=\"Quarterly pentesting of 20 web applications. Reports and remediation guidance.\",\n",
    "        organization=\"State IT Security\",\n",
    "        deadline=\"2024-12-01\",\n",
    "        estimated_value=\"$180K\"\n",
    "    ), [\"cybersecurity\"]),\n",
    "]\n",
    "\n",
    "print(\"COMPARATIVE RATING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ratings = []\n",
    "for name, tender, categories in batch_tenders:\n",
    "    rating = await rate_tender(tender, categories)\n",
    "    ratings.append((name, tender, rating))\n",
    "\n",
    "# Sort by overall score\n",
    "ratings.sort(key=lambda x: x[2].overall_score, reverse=True)\n",
    "\n",
    "print(\"\\nRANKED OPPORTUNITIES:\\n\")\n",
    "for i, (name, tender, rating) in enumerate(ratings, 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Overall: {rating.overall_score:.1f} | \"\n",
    "          f\"Fit: {rating.strategic_fit:.1f} | \"\n",
    "          f\"Win: {rating.win_probability:.1f} | \"\n",
    "          f\"Effort: {rating.effort_required:.1f}\")\n",
    "    print(f\"   Value: {tender.estimated_value}\")\n",
    "    print(f\"   ‚Üí {rating.recommendation[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628d58f",
   "metadata": {},
   "source": [
    "## Step 9: Production-Ready Class\n",
    "\n",
    "Wrap everything in a reusable class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e048e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingAgent:\n",
    "    \"\"\"\n",
    "    Production-ready Rating Agent\n",
    "    \n",
    "    Evaluates tender opportunities on multiple dimensions:\n",
    "    - Strategic fit with company capabilities\n",
    "    - Win probability considering competition\n",
    "    - Effort required vs resources available\n",
    "    \n",
    "    Returns structured rating with explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_url: str = BASE_URL,\n",
    "        temperature: float = 0.1,\n",
    "        min_score: float = 7.0\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.temperature = temperature\n",
    "        self.min_score = min_score\n",
    "    \n",
    "    async def rate(\n",
    "        self, \n",
    "        tender: Tender, \n",
    "        categories: List[str]\n",
    "    ) -> RatingResult:\n",
    "        \"\"\"Rate a tender opportunity\"\"\"\n",
    "        return await rate_tender(tender, categories, self.temperature)\n",
    "    \n",
    "    def should_proceed(self, rating: RatingResult) -> bool:\n",
    "        \"\"\"Business logic: should we generate bid documents?\"\"\"\n",
    "        return rating.overall_score >= self.min_score\n",
    "\n",
    "# Test the class\n",
    "agent = RatingAgent(min_score=7.0)\n",
    "\n",
    "test = Tender(\n",
    "    id=\"CLASS-TEST\",\n",
    "    title=\"Cloud Security Audit\",\n",
    "    description=\"Comprehensive security audit of AWS infrastructure\",\n",
    "    organization=\"Tech Startup\",\n",
    "    deadline=\"2024-12-01\",\n",
    "    estimated_value=\"$120K\"\n",
    ")\n",
    "\n",
    "result = await agent.rate(test, [\"cybersecurity\"])\n",
    "proceed = agent.should_proceed(result)\n",
    "\n",
    "print(f\"Rating: {result.overall_score:.1f}/10\")\n",
    "print(f\"Proceed to bid document: {proceed}\")\n",
    "print(f\"Recommendation: {result.recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae6ed2",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You built a sophisticated rating agent!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "1. **Multi-dimensional scoring** - Avoid single-number bias\n",
    "2. **Balanced analysis** - Force consideration of both strengths and risks\n",
    "3. **Company context matters** - Prompt includes capabilities and constraints\n",
    "4. **Comparative ranking** - Multiple scores enable prioritization\n",
    "5. **Business logic integration** - Thresholds and rules on top of AI\n",
    "\n",
    "## Design Decisions\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|----------|\n",
    "| Temperature 0.1 | Consistent scoring across tenders |\n",
    "| Multiple dimensions | More nuanced than single score |\n",
    "| Required strengths AND risks | Prevents overly optimistic ratings |\n",
    "| 0-10 scale | Intuitive and fine-grained |\n",
    "| Company profile in prompt | Context for realistic assessment |\n",
    "\n",
    "## Prompt Engineering Lessons\n",
    "\n",
    "1. **Explicit calibration** - \"Most should score 5-7, not 8-10\"\n",
    "2. **Multiple perspectives** - Force analysis from different angles\n",
    "3. **Concrete criteria** - Not just \"rate this\", but \"rate on X, Y, Z\"\n",
    "4. **Role definition** - \"15 years experience\" sets expectation\n",
    "5. **Balanced instructions** - \"Realistic, not optimistic\"\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now we can:\n",
    "1. ‚úì Filter tenders for relevance\n",
    "2. ‚úì Rate opportunities on multiple dimensions\n",
    "3. ? Generate professional bid documents\n",
    "\n",
    "Let's build the document generator!\n",
    "\n",
    "‚û°Ô∏è Continue to `05_doc_generator.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
