{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 2: Your First AI Agent System\n",
        "\n",
        "**Goal:** Build a complete working AI system in 2 hours\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to call local LLMs (Large Language Models via LM Studio)\n",
        "- How to get structured outputs using Pydantic\n",
        "- How to build AI agents that make decisions\n",
        "- How to orchestrate multiple agents to work together\n",
        "\n",
        "**Learning approach:** Start with working code, then understand by experimenting\n",
        "\n",
        "**Prerequisites:**\n",
        "This notebook assumes you have basic Python knowledge. No prior experience with AI or machine learning is required.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites Check\n",
        "\n",
        "Make sure you have the following ready before starting:\n",
        "\n",
        "- **LM Studio:** Running on localhost:1234  \n",
        "  - LM Studio is a desktop application that runs LLMs on your computer\n",
        "  - Download from: lmstudio.ai\n",
        "  \n",
        "- **Model loaded:** Llama 3.1 8B Instruct model  \n",
        "  - This is a powerful open-source language model\n",
        "  - Load it in LM Studio before running this notebook\n",
        "  \n",
        "- **Python packages:** httpx and pydantic installed  \n",
        "  - httpx: For making HTTP requests to the LLM\n",
        "  - pydantic: For data validation and structured outputs\n",
        "  - Install with: `pip install httpx pydantic`\n",
        "\n",
        "**What we're building:**\n",
        "A procurement AI system that automatically processes tender opportunities:\n",
        "1. Filters tenders to find relevant opportunities\n",
        "2. Scores each opportunity on multiple criteria\n",
        "3. Generates professional bid documents for high-scoring opportunities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LM Studio is running!\n",
            "Model: openai/gpt-oss-20b\n"
          ]
        }
      ],
      "source": [
        "# Test LM Studio connection\n",
        "import httpx\n",
        "\n",
        "try:\n",
        "    response = httpx.get(\"http://localhost:1234/v1/models\")\n",
        "    print(\"LM Studio is running!\")\n",
        "    print(f\"Model: {response.json()['data'][0]['id']}\")\n",
        "except Exception as e:\n",
        "    print(\"LM Studio not running. Start it first!\")\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: First LLM Call\n",
        "\n",
        "Let's start simple: send text, get text back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM: Hi, I'm here to assist.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "\n",
        "async def simple_llm_call(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Make a simple call to the LLM (Large Language Model).\n",
        "    \n",
        "    This function sends a text prompt to the LLM and gets a text response back.\n",
        "    Think of it like sending a question to ChatGPT and receiving an answer.\n",
        "    \"\"\"\n",
        "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
        "        response = await client.post(\n",
        "            \"http://localhost:1234/v1/chat/completions\",\n",
        "            json={\n",
        "                \"model\": \"llama-3.1-8b-instruct\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"temperature\": 0.7,\n",
        "            }\n",
        "        )\n",
        "        result = response.json()\n",
        "        \n",
        "        # Check if the response has the expected structure\n",
        "        if \"choices\" not in result or len(result[\"choices\"]) == 0:\n",
        "            raise ValueError(f\"Unexpected API response: {result}\")\n",
        "        \n",
        "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# Test it - in Jupyter notebooks we can use 'await' directly\n",
        "response = await simple_llm_call(\"Say hello in exactly 5 words\")\n",
        "print(f\"LLM: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise:** Change the prompt and run again. Try:\n",
        "- \"Count from 1 to 10\"\n",
        "- \"Explain AI in one sentence\"\n",
        "- \"Write a haiku about code\"\n",
        "\n",
        "**What to observe:** Notice how the LLM responds differently to each prompt. This demonstrates the flexibility of LLMs to handle various types of text generation tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Structured Outputs\n",
        "\n",
        "**The Critical Pattern:** Make LLMs return structured data instead of free-form text\n",
        "\n",
        "**Why this matters:**\n",
        "When you ask an LLM a question, it naturally returns unstructured text. But in applications, we need:\n",
        "- Predictable data formats that code can process\n",
        "- Type safety and validation\n",
        "- Consistent field names and structures\n",
        "- The ability to use the data programmatically\n",
        "\n",
        "**The solution:** Define a schema (data structure template) and ask the LLM to fill it in.\n",
        "\n",
        "This is one of the most important patterns in LLM engineering. Without it, you'd need to write complex parsing logic to extract information from free-form text, which is error-prone and fragile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema that will be sent to the LLM:\n",
            "{\n",
            "  \"properties\": {\n",
            "    \"is_relevant\": {\n",
            "      \"description\": \"Is this relevant?\",\n",
            "      \"title\": \"Is Relevant\",\n",
            "      \"type\": \"boolean\"\n",
            "    },\n",
            "    \"confidence\": {\n",
            "      \"description\": \"Confidence 0-1\",\n",
            "      \"maximum\": 1,\n",
            "      \"minimum\": 0,\n",
            "      \"title\": \"Confidence\",\n",
            "      \"type\": \"number\"\n",
            "    },\n",
            "    \"reasoning\": {\n",
            "      \"description\": \"Why?\",\n",
            "      \"title\": \"Reasoning\",\n",
            "      \"type\": \"string\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"is_relevant\",\n",
            "    \"confidence\",\n",
            "    \"reasoning\"\n",
            "  ],\n",
            "  \"title\": \"Analysis\",\n",
            "  \"type\": \"object\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "\n",
        "# Define output structure using Pydantic\n",
        "# This creates a template that the LLM will fill in\n",
        "class Analysis(BaseModel):\n",
        "    # Field() provides metadata: description helps the LLM understand what to put here\n",
        "    is_relevant: bool = Field(description=\"Is this relevant?\")\n",
        "    \n",
        "    # ge=0, le=1 means \"greater than or equal to 0, less than or equal to 1\"\n",
        "    # This enforces that confidence must be between 0 and 1\n",
        "    confidence: float = Field(description=\"Confidence 0-1\", ge=0, le=1)\n",
        "    \n",
        "    reasoning: str = Field(description=\"Why?\")\n",
        "\n",
        "# Show the schema in JSON format\n",
        "# This is what we'll send to the LLM to tell it what structure we expect\n",
        "print(\"Schema that will be sent to the LLM:\")\n",
        "print(json.dumps(Analysis.model_json_schema(), indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Structured Output:\n",
            "Relevant: True\n",
            "Confidence: 0.9\n",
            "Reasoning: The subject is directly about developing an AI-based cybersecurity platform, which aligns with current interests in applying artificial intelligence to security solutions. It is a topical and relevant area for discussion, research, or project planning.\n"
          ]
        }
      ],
      "source": [
        "async def get_structured(prompt: str, schema: BaseModel) -> BaseModel:\n",
        "    \"\"\"\n",
        "    Get LLM output that matches a specific schema.\n",
        "    \n",
        "    This function takes any Pydantic model as a schema and asks the LLM\n",
        "    to return data matching that structure.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The question or task for the LLM\n",
        "        schema: A Pydantic BaseModel class defining the expected structure\n",
        "    \n",
        "    Returns:\n",
        "        An instance of the schema class with validated data from the LLM\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert the schema to JSON format\n",
        "    schema_json = json.dumps(schema.model_json_schema(), indent=2)\n",
        "    \n",
        "    # Build a complete prompt that includes the original prompt plus schema instructions\n",
        "    full_prompt = f\"\"\"{prompt}\n",
        "\n",
        "Respond with valid JSON matching this schema:\n",
        "{schema_json}\n",
        "\n",
        "Rules: ONLY JSON, no markdown code blocks, no explanations\"\"\"\n",
        "    \n",
        "    # Call LLM with the enhanced prompt\n",
        "    response = await simple_llm_call(full_prompt)\n",
        "    \n",
        "    # Clean the response: sometimes LLMs add markdown code blocks like ```json\n",
        "    # We need to remove these to get pure JSON\n",
        "    cleaned = response.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    \n",
        "    # Parse and validate the JSON against our schema\n",
        "    # If validation fails, Pydantic will raise an error\n",
        "    return schema.model_validate_json(cleaned)\n",
        "\n",
        "# Test the function with our Analysis schema\n",
        "result = await get_structured(\n",
        "    \"Analyze: 'AI Cybersecurity Platform Development'\",\n",
        "    Analysis\n",
        ")\n",
        "\n",
        "print(\"\\nStructured Output:\")\n",
        "print(f\"Relevant: {result.is_relevant}\")\n",
        "print(f\"Confidence: {result.confidence}\")\n",
        "print(f\"Reasoning: {result.reasoning}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this is powerful:**\n",
        "- We defined a specific data structure using Pydantic (a Python library for data validation)\n",
        "- The LLM filled it out according to our schema\n",
        "- The data was automatically validated to ensure it matches our requirements\n",
        "- We can now use this in code with proper type checking: `result.is_relevant`\n",
        "\n",
        "**Key concept:** Instead of getting free-form text that we'd need to parse, we get structured data that's immediately usable in our application. This is one of the most important patterns in modern LLM engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Complete System\n",
        "\n",
        "Now let's build the full procurement AI system.\n",
        "\n",
        "**System Architecture:**\n",
        "```\n",
        "Tender Document → Filter Agent → Rating Agent → Document Generator → Final Output\n",
        "```\n",
        "\n",
        "**How it works:**\n",
        "1. **Filter Agent:** Receives a tender, determines if it's relevant to our business\n",
        "2. **Rating Agent:** Scores relevant tenders on strategic fit and win probability\n",
        "3. **Document Generator:** Creates professional bid documents for high-scoring opportunities\n",
        "\n",
        "**Key design principle:**\n",
        "Each agent is specialized and focused on one task. This makes the system:\n",
        "- Easier to test (test each agent independently)\n",
        "- Easier to debug (identify which agent has issues)\n",
        "- Easier to improve (optimize one agent without affecting others)\n",
        "- More maintainable (clear separation of concerns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All imports\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from enum import Enum\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import httpx\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data models defined successfully\n"
          ]
        }
      ],
      "source": [
        "# === DATA MODELS ===\n",
        "# These define the structure of data throughout our system\n",
        "\n",
        "# Enum: A set of predefined constant values\n",
        "# This ensures categories are consistent (no typos like \"cibersecurity\")\n",
        "class Category(str, Enum):\n",
        "    CYBER = \"cybersecurity\"\n",
        "    AI = \"ai\"\n",
        "    SOFTWARE = \"software\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "# Input: Represents a tender document we want to process\n",
        "class Tender(BaseModel):\n",
        "    id: str\n",
        "    title: str\n",
        "    description: str\n",
        "    organization: str\n",
        "    estimated_value: Optional[str] = None  # Optional because not all tenders include value\n",
        "\n",
        "# Output from Filter Agent: Decision on whether tender is relevant\n",
        "class FilterResult(BaseModel):\n",
        "    is_relevant: bool\n",
        "    confidence: float = Field(ge=0, le=1)  # Confidence score between 0 and 1\n",
        "    categories: List[Category]  # What categories apply to this tender\n",
        "    reasoning: str  # Explanation of the decision\n",
        "\n",
        "# Output from Rating Agent: Detailed scoring of a tender\n",
        "class RatingResult(BaseModel):\n",
        "    overall_score: float = Field(ge=0, le=10)  # Overall opportunity score out of 10\n",
        "    strategic_fit: float = Field(ge=0, le=10)  # How well it aligns with our strategy\n",
        "    win_probability: float = Field(ge=0, le=10)  # Likelihood we could win this bid\n",
        "    strengths: List[str]  # List of our advantages for this opportunity\n",
        "    risks: List[str]  # List of potential challenges or concerns\n",
        "    recommendation: str  # Final recommendation text\n",
        "\n",
        "# Output from Document Generator: A structured bid document\n",
        "class BidDoc(BaseModel):\n",
        "    executive_summary: str  # High-level overview for decision makers\n",
        "    technical_approach: str  # How we would execute the project\n",
        "    value_proposition: str  # Why the client should choose us\n",
        "\n",
        "print(\"Data models defined successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM service initialized and ready\n"
          ]
        }
      ],
      "source": [
        "# === LLM SERVICE ===\n",
        "# This class encapsulates all communication with the LLM\n",
        "\n",
        "class LLM:\n",
        "    \"\"\"\n",
        "    A service for interacting with the LLM API.\n",
        "    \n",
        "    This wraps the API calls in a convenient interface and handles\n",
        "    structured output generation consistently across all agents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.url = \"http://localhost:1234/v1\"\n",
        "        self.model = \"llama-3.1-8b-instruct\"\n",
        "    \n",
        "    def _clean_json(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove markdown artifacts and extract valid JSON from LLM response.\n",
        "        \n",
        "        This method handles various formats that LLMs might return:\n",
        "        - Markdown code blocks (```json ... ```)\n",
        "        - Extra text before or after the JSON\n",
        "        - Model-specific tokens\n",
        "        \n",
        "        It uses brace counting to find the actual JSON object boundaries.\n",
        "        \"\"\"\n",
        "        cleaned = text.strip()\n",
        "        \n",
        "        # Remove markdown code blocks\n",
        "        if cleaned.startswith(\"```json\"):\n",
        "            cleaned = cleaned[7:]\n",
        "        elif cleaned.startswith(\"```\"):\n",
        "            cleaned = cleaned[3:]\n",
        "        if cleaned.endswith(\"```\"):\n",
        "            cleaned = cleaned[:-3]\n",
        "        \n",
        "        cleaned = cleaned.strip()\n",
        "        \n",
        "        # Find the JSON object by looking for balanced braces\n",
        "        # This is more robust than regex because it handles nested objects\n",
        "        start_idx = cleaned.find('{')\n",
        "        if start_idx == -1:\n",
        "            return cleaned\n",
        "        \n",
        "        # Count braces to find the matching closing brace\n",
        "        brace_count = 0\n",
        "        end_idx = -1\n",
        "        \n",
        "        for i, char in enumerate(cleaned[start_idx:], start_idx):\n",
        "            if char == '{':\n",
        "                brace_count += 1\n",
        "            elif char == '}':\n",
        "                brace_count -= 1\n",
        "                if brace_count == 0:\n",
        "                    end_idx = i\n",
        "                    break\n",
        "        \n",
        "        if end_idx != -1:\n",
        "            return cleaned[start_idx:end_idx + 1]\n",
        "        \n",
        "        return cleaned\n",
        "    \n",
        "    async def generate(self, prompt: str, schema: BaseModel, \n",
        "                       system: str = \"\", temp: float = 0.1) -> BaseModel:\n",
        "        \"\"\"\n",
        "        Generate structured output from the LLM.\n",
        "        \n",
        "        Args:\n",
        "            prompt: The main instruction or question\n",
        "            schema: Pydantic model defining the expected output structure\n",
        "            system: System message to set the LLM's role/behavior (optional)\n",
        "            temp: Temperature parameter (0.0 = focused, 1.0 = creative)\n",
        "        \n",
        "        Returns:\n",
        "            Validated instance of the schema with LLM-generated data\n",
        "        \"\"\"\n",
        "        messages = []\n",
        "        \n",
        "        # System message sets the context and role for the LLM\n",
        "        if system:\n",
        "            messages.append({\"role\": \"system\", \"content\": system})\n",
        "        \n",
        "        # Add schema to the prompt so LLM knows what format to use\n",
        "        schema_json = json.dumps(schema.model_json_schema(), indent=2)\n",
        "        full_prompt = f\"\"\"{prompt}\n",
        "\n",
        "Respond ONLY with valid JSON matching:\n",
        "{schema_json}\n",
        "\n",
        "IMPORTANT: Return ONLY the JSON object, nothing else. No explanations, no markdown, no extra text.\n",
        "Start with {{ and end with }}\"\"\"\n",
        "        \n",
        "        messages.append({\"role\": \"user\", \"content\": full_prompt})\n",
        "        \n",
        "        # Make the API call with extended timeout for longer generations\n",
        "        async with httpx.AsyncClient(timeout=120) as client:\n",
        "            r = await client.post(\n",
        "                f\"{self.url}/chat/completions\",\n",
        "                json={\n",
        "                    \"model\": self.model,\n",
        "                    \"messages\": messages,\n",
        "                    \"temperature\": temp,\n",
        "                    \"max_tokens\": 2000,  # Maximum length of response\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            # Check for API errors\n",
        "            if r.status_code != 200:\n",
        "                raise ValueError(f\"API error: {r.status_code} - {r.text}\")\n",
        "            \n",
        "            result = r.json()\n",
        "            if \"choices\" not in result or len(result[\"choices\"]) == 0:\n",
        "                raise ValueError(f\"Unexpected API response: {result}\")\n",
        "            \n",
        "            content = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        \n",
        "        # Clean the JSON using the same method as in src/procurement_ai/services/llm.py\n",
        "        clean_json = self._clean_json(content)\n",
        "        \n",
        "        # Validate that we have something that looks like JSON\n",
        "        if not clean_json.startswith('{') or not clean_json.endswith('}'):\n",
        "            print(f\"\\nWarning: Response doesn't look like JSON\")\n",
        "            print(f\"Raw content (first 300 chars): {content[:300]}\")\n",
        "            print(f\"Cleaned (first 300 chars): {clean_json[:300]}\")\n",
        "            raise ValueError(f\"Response doesn't look like valid JSON\")\n",
        "        \n",
        "        # Parse and validate against schema\n",
        "        try:\n",
        "            # First parse as JSON to catch JSON errors\n",
        "            parsed = json.loads(clean_json)\n",
        "            # Then validate against the Pydantic model\n",
        "            return schema.model_validate(parsed)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"\\nJSON parsing error: {e}\")\n",
        "            print(f\"Raw content (first 300 chars): {content[:300]}\")\n",
        "            print(f\"Cleaned JSON (first 300 chars): {clean_json[:300]}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"\\nValidation error: {e}\")\n",
        "            print(f\"Cleaned JSON (first 300 chars): {clean_json[:300]}\")\n",
        "            raise\n",
        "\n",
        "# Create a single instance to reuse across all agents\n",
        "llm = LLM()\n",
        "print(\"LLM service initialized and ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent 1: Filter Agent\n",
        "\n",
        "**Purpose:** Determines if a tender is relevant to our business\n",
        "\n",
        "**How it works:**\n",
        "1. Receives a Tender object\n",
        "2. Analyzes the title and description\n",
        "3. Returns a FilterResult with relevance decision, confidence score, and categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filter agent defined successfully\n"
          ]
        }
      ],
      "source": [
        "class FilterAgent:\n",
        "    \"\"\"\n",
        "    Filters tenders to identify relevant opportunities.\n",
        "    \n",
        "    This agent acts as a first-pass filter to avoid wasting time\n",
        "    analyzing tenders that don't match our business focus.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: LLM):\n",
        "        self.llm = llm\n",
        "    \n",
        "    async def filter(self, tender: Tender) -> FilterResult:\n",
        "        \"\"\"\n",
        "        Analyze a tender and determine if it's relevant.\n",
        "        \n",
        "        Args:\n",
        "            tender: The tender document to analyze\n",
        "        \n",
        "        Returns:\n",
        "            FilterResult with relevance decision and reasoning\n",
        "        \"\"\"\n",
        "        # Craft a clear prompt explaining the task\n",
        "        prompt = f\"\"\"Analyze this tender:\n",
        "\n",
        "TITLE: {tender.title}\n",
        "DESCRIPTION: {tender.description}\n",
        "\n",
        "RELEVANCE CRITERIA:\n",
        "- RELEVANT if: cybersecurity, AI/ML, or software development\n",
        "- NOT relevant if: hardware, construction, non-tech services\n",
        "\n",
        "Determine if this is relevant and assign appropriate categories.\"\"\"\n",
        "        \n",
        "        # Use low temperature (0.1) for consistent, deterministic filtering\n",
        "        return await self.llm.generate(\n",
        "            prompt, FilterResult,\n",
        "            system=\"You are a procurement analyst. Be precise and consistent in your judgments.\",\n",
        "            temp=0.1\n",
        "        )\n",
        "\n",
        "print(\"Filter agent defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent 2: Rating Agent\n",
        "\n",
        "**Purpose:** Scores opportunities on strategic fit and win probability\n",
        "\n",
        "**How it works:**\n",
        "1. Receives a Tender and its categories from the Filter Agent\n",
        "2. Evaluates on multiple dimensions (strategic fit, win probability)\n",
        "3. Returns RatingResult with scores, strengths, risks, and recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rating agent defined successfully\n"
          ]
        }
      ],
      "source": [
        "class RatingAgent:\n",
        "    \"\"\"\n",
        "    Rates opportunities on multiple criteria.\n",
        "    \n",
        "    This agent performs deeper analysis on relevant tenders to help\n",
        "    prioritize which opportunities to pursue.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: LLM):\n",
        "        self.llm = llm\n",
        "    \n",
        "    async def rate(self, tender: Tender, cats: List[str]) -> RatingResult:\n",
        "        \"\"\"\n",
        "        Rate a tender opportunity.\n",
        "        \n",
        "        Args:\n",
        "            tender: The tender to rate\n",
        "            cats: Categories assigned by the Filter Agent\n",
        "        \n",
        "        Returns:\n",
        "            RatingResult with detailed scoring and analysis\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Rate this opportunity:\n",
        "\n",
        "TENDER: {tender.title}\n",
        "ORGANIZATION: {tender.organization}\n",
        "ESTIMATED VALUE: {tender.estimated_value}\n",
        "CATEGORIES: {', '.join(cats)}\n",
        "\n",
        "Provide scores (0-10 scale):\n",
        "- Strategic fit: How well does this align with our capabilities?\n",
        "- Win probability: What are our chances of winning this bid?\n",
        "- Overall score: Combined assessment\n",
        "\n",
        "Also identify:\n",
        "- Top 3 strengths (why we'd be good fit)\n",
        "- Top 3 risks (potential challenges)\n",
        "- Clear recommendation (pursue, consider, or skip)\"\"\"\n",
        "        \n",
        "        # Use low temperature for consistent scoring\n",
        "        return await self.llm.generate(\n",
        "            prompt, RatingResult,\n",
        "            system=\"You are a business analyst. Be realistic and data-driven in your assessments.\",\n",
        "            temp=0.1\n",
        "        )\n",
        "\n",
        "print(\"Rating agent defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent 3: Document Generator\n",
        "\n",
        "**Purpose:** Creates professional bid documents\n",
        "\n",
        "**How it works:**\n",
        "1. Receives tender information, categories, and strengths from previous agents\n",
        "2. Generates three key sections: executive summary, technical approach, and value proposition\n",
        "3. Uses higher temperature for more creative, varied writing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document generator defined successfully\n"
          ]
        }
      ],
      "source": [
        "class DocGenerator:\n",
        "    \"\"\"\n",
        "    Generates professional bid documents.\n",
        "    \n",
        "    This agent takes the analysis from previous agents and creates\n",
        "    polished, professional content for bid submissions.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: LLM):\n",
        "        self.llm = llm\n",
        "    \n",
        "    async def generate(self, tender: Tender, cats: List[str], \n",
        "                       strengths: List[str]) -> BidDoc:\n",
        "        \"\"\"\n",
        "        Generate a bid document.\n",
        "        \n",
        "        Args:\n",
        "            tender: The tender we're bidding on\n",
        "            cats: Relevant categories\n",
        "            strengths: Our key strengths (from Rating Agent)\n",
        "        \n",
        "        Returns:\n",
        "            BidDoc with executive summary, technical approach, and value proposition\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Create a professional bid document for:\n",
        "\n",
        "TENDER: {tender.title}\n",
        "CLIENT: {tender.organization}\n",
        "OUR EXPERTISE: {', '.join(cats)}\n",
        "KEY STRENGTHS: {', '.join(strengths)}\n",
        "\n",
        "Generate three sections:\n",
        "\n",
        "1. Executive Summary (2-3 paragraphs)\n",
        "   - High-level overview of our proposed solution\n",
        "   - Why we're the right choice\n",
        "   - Expected outcomes\n",
        "\n",
        "2. Technical Approach (2-3 paragraphs)\n",
        "   - How we would execute the project\n",
        "   - Methodology and tools\n",
        "   - Timeline and milestones\n",
        "\n",
        "3. Value Proposition (2 paragraphs)\n",
        "   - Unique benefits we bring\n",
        "   - ROI and long-term value\n",
        "\n",
        "Tone: Professional, confident, client-focused\n",
        "Style: Specific and concrete, not generic marketing language\"\"\"\n",
        "        \n",
        "        # Use higher temperature (0.7) for more creative, varied writing\n",
        "        return await self.llm.generate(\n",
        "            prompt, BidDoc,\n",
        "            system=\"You are an expert proposal writer with 10+ years of experience winning government contracts.\",\n",
        "            temp=0.7  # Higher temperature for creativity\n",
        "        )\n",
        "\n",
        "print(\"Document generator defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Orchestrator: Coordinating the Agents\n",
        "\n",
        "**Purpose:** Manages the workflow and coordinates all three agents\n",
        "\n",
        "**How it works:**\n",
        "1. Receives a tender as input\n",
        "2. Runs the Filter Agent first\n",
        "3. If relevant, runs the Rating Agent\n",
        "4. If high-scoring, runs the Document Generator\n",
        "5. Returns the combined results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orchestrator initialized and ready to process tenders\n"
          ]
        }
      ],
      "source": [
        "class Orchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the multi-agent workflow.\n",
        "    \n",
        "    This class coordinates the three agents, managing data flow between them\n",
        "    and implementing business logic (e.g., skip low-scoring tenders).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.llm = LLM()\n",
        "        self.filter = FilterAgent(self.llm)\n",
        "        self.rater = RatingAgent(self.llm)\n",
        "        self.doc_gen = DocGenerator(self.llm)\n",
        "    \n",
        "    async def process(self, tender: Tender):\n",
        "        \"\"\"\n",
        "        Process a tender through the complete workflow.\n",
        "        \n",
        "        Args:\n",
        "            tender: The tender to process\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with filter results, rating, and document (if generated)\n",
        "            Returns None if tender is filtered out as not relevant\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing: {tender.title[:50]}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        start = datetime.now()\n",
        "        \n",
        "        # Step 1: Filter - Determine relevance\n",
        "        print(\"\\n[1/3] Filtering...\")\n",
        "        filter_result = await self.filter.filter(tender)\n",
        "        print(f\"  Relevant: {filter_result.is_relevant} (Confidence: {filter_result.confidence:.0%})\")\n",
        "        print(f\"  Categories: {[c.value for c in filter_result.categories]}\")\n",
        "        \n",
        "        # Business logic: Skip if not relevant or low confidence\n",
        "        if not filter_result.is_relevant or filter_result.confidence < 0.6:\n",
        "            print(\"\\n  Decision: Skipping (not relevant or low confidence)\")\n",
        "            return None\n",
        "        \n",
        "        # Step 2: Rate - Score the opportunity\n",
        "        print(\"\\n[2/3] Rating...\")\n",
        "        cats = [c.value for c in filter_result.categories]\n",
        "        rating = await self.rater.rate(tender, cats)\n",
        "        print(f\"  Overall Score: {rating.overall_score:.1f}/10\")\n",
        "        print(f\"  Strategic Fit: {rating.strategic_fit:.1f}/10\")\n",
        "        print(f\"  Win Probability: {rating.win_probability:.1f}/10\")\n",
        "        \n",
        "        # Business logic: Only generate documents for high-scoring opportunities\n",
        "        if rating.overall_score < 7.0:\n",
        "            print(\"\\n  Decision: Skipping document generation (score below 7.0 threshold)\")\n",
        "            return {\"filter\": filter_result, \"rating\": rating}\n",
        "        \n",
        "        # Step 3: Generate - Create bid document\n",
        "        print(\"\\n[3/3] Generating document...\")\n",
        "        doc = await self.doc_gen.generate(tender, cats, rating.strengths)\n",
        "        print(\"  Status: Document ready\")\n",
        "        \n",
        "        elapsed = (datetime.now() - start).total_seconds()\n",
        "        print(f\"\\n  Total processing time: {elapsed:.1f} seconds\")\n",
        "        \n",
        "        return {\n",
        "            \"filter\": filter_result,\n",
        "            \"rating\": rating,\n",
        "            \"document\": doc,\n",
        "            \"time\": elapsed\n",
        "        }\n",
        "\n",
        "# Create orchestrator instance\n",
        "orchestrator = Orchestrator()\n",
        "print(\"Orchestrator initialized and ready to process tenders\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Test Data\n",
        "\n",
        "Let's create some sample tenders to test our system. We have three different types:\n",
        "1. A highly relevant AI/Cybersecurity project\n",
        "2. A completely irrelevant furniture procurement\n",
        "3. A moderately relevant healthcare software project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test data prepared: 3 tenders ready for processing\n"
          ]
        }
      ],
      "source": [
        "# Sample tenders covering different scenarios\n",
        "tenders = [\n",
        "    # Tender 1: High relevance - matches our expertise perfectly\n",
        "    Tender(\n",
        "        id=\"T001\",\n",
        "        title=\"AI-Powered Cybersecurity Platform\",\n",
        "        description=\"\"\"Develop AI-based threat detection system for government \n",
        "        infrastructure. Must use machine learning for anomaly detection and \n",
        "        integrate with existing SIEM tools. Real-time monitoring required.\"\"\",\n",
        "        organization=\"National Cyber Agency\",\n",
        "        estimated_value=\"3.2M EUR\"\n",
        "    ),\n",
        "    \n",
        "    # Tender 2: Not relevant - completely outside our domain\n",
        "    Tender(\n",
        "        id=\"T002\",\n",
        "        title=\"Office Furniture Supply\",\n",
        "        description=\"\"\"Supply ergonomic office furniture for 500 workstations \n",
        "        including desks, chairs, and storage. Must meet ISO 9001 standards\n",
        "        and provide 5-year warranty.\"\"\",\n",
        "        organization=\"Ministry of Public Works\",\n",
        "        estimated_value=\"450K EUR\"\n",
        "    ),\n",
        "    \n",
        "    # Tender 3: Moderately relevant - software but different domain\n",
        "    Tender(\n",
        "        id=\"T003\",\n",
        "        title=\"Custom Healthcare CRM System\",\n",
        "        description=\"\"\"Build cloud-based CRM for healthcare network. Must \n",
        "        handle patient data, appointments, billing. GDPR compliant with \n",
        "        mobile app for doctors and patients.\"\"\",\n",
        "        organization=\"Regional Health Authority\",\n",
        "        estimated_value=\"1.8M EUR\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(f\"Test data prepared: {len(tenders)} tenders ready for processing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Run the System\n",
        "\n",
        "Now let's process all three test tenders through our system.\n",
        "Each tender will go through the filter, rating, and potentially document generation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Processing: AI-Powered Cybersecurity Platform...\n",
            "============================================================\n",
            "\n",
            "[1/3] Filtering...\n"
          ]
        },
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for FilterResult\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='<|channel|>final <|const...rvices are requested.\"}', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m results = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tender \u001b[38;5;129;01min\u001b[39;00m tenders:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m orchestrator.process(tender)\n\u001b[32m      7\u001b[39m     results.append({\u001b[33m\"\u001b[39m\u001b[33mtender\u001b[39m\u001b[33m\"\u001b[39m: tender, \u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m: result})\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll tenders processed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mOrchestrator.process\u001b[39m\u001b[34m(self, tender)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Step 1: Filter - Determine relevance\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[1/3] Filtering...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m filter_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filter.filter(tender)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Relevant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilter_result.is_relevant\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilter_result.confidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Categories: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[c.value\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mfilter_result.categories]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mFilterAgent.filter\u001b[39m\u001b[34m(self, tender)\u001b[39m\n\u001b[32m     22\u001b[39m         prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mAnalyze this tender:\u001b[39m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[33mTITLE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtender.title\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[33mDetermine if this is relevant and assign appropriate categories.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[38;5;66;03m# Use low temperature (0.1) for consistent, deterministic filtering\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.generate(\n\u001b[32m     35\u001b[39m             prompt, FilterResult,\n\u001b[32m     36\u001b[39m             system=\u001b[33m\"\u001b[39m\u001b[33mYou are a procurement analyst. Be precise and consistent in your judgments.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m             temp=\u001b[32m0.1\u001b[39m\n\u001b[32m     38\u001b[39m         )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompt, schema, system, temp)\u001b[39m\n\u001b[32m     69\u001b[39m     clean = clean.replace(marker, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Parse and validate against schema\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages/pydantic/main.py:766\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, extra, context, by_alias, by_name)\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    761\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    762\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    763\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    764\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mValidationError\u001b[39m: 1 validation error for FilterResult\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='<|channel|>final <|const...rvices are requested.\"}', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
          ]
        }
      ],
      "source": [
        "# Process all tenders sequentially\n",
        "# Store results for later analysis\n",
        "results = []\n",
        "\n",
        "for tender in tenders:\n",
        "    result = await orchestrator.process(tender)\n",
        "    results.append({\"tender\": tender, \"result\": result})\n",
        "\n",
        "print(\"\\n\\nAll tenders processed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Results Analysis\n",
        "\n",
        "Let's analyze the aggregate results across all processed tenders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Count tenders at each stage\n",
        "relevant = sum(1 for r in results if r[\"result\"] and \"filter\" in r[\"result\"])\n",
        "high_scored = sum(1 for r in results if r[\"result\"] and \"rating\" in r[\"result\"] \n",
        "                  and r[\"result\"][\"rating\"].overall_score >= 7.0)\n",
        "docs_made = sum(1 for r in results if r[\"result\"] and \"document\" in r[\"result\"])\n",
        "\n",
        "print(f\"Total Tenders Processed: {len(tenders)}\")\n",
        "print(f\"Relevant (passed filter): {relevant}\")\n",
        "print(f\"High Scored (>= 7.0): {high_scored}\")\n",
        "print(f\"Documents Generated: {docs_made}\")\n",
        "\n",
        "# Calculate processing time\n",
        "total_time = sum(r[\"result\"][\"time\"] for r in results if r[\"result\"] and \"time\" in r[\"result\"])\n",
        "print(f\"\\nTotal Processing Time: {total_time:.1f} seconds\")\n",
        "if len(tenders) > 0:\n",
        "    print(f\"Average per Tender: {total_time/len(tenders):.1f} seconds\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Results\n",
        "\n",
        "Now let's look at each tender in detail to see how it was processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, item in enumerate(results, 1):\n",
        "    tender = item[\"tender\"]\n",
        "    result = item[\"result\"]\n",
        "    \n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"TENDER {i}: {tender.title}\")\n",
        "    print(f\"{'─'*60}\")\n",
        "    print(f\"Organization: {tender.organization}\")\n",
        "    print(f\"Value: {tender.estimated_value}\")\n",
        "    \n",
        "    # Case 1: Filtered out completely\n",
        "    if not result:\n",
        "        print(\"\\nStatus: FILTERED OUT (not relevant)\")\n",
        "        continue\n",
        "    \n",
        "    # Case 2: Has filter results\n",
        "    if \"filter\" in result:\n",
        "        f = result[\"filter\"]\n",
        "        print(f\"\\nFilter Results:\")\n",
        "        print(f\"  Relevant: {f.is_relevant}\")\n",
        "        print(f\"  Confidence: {f.confidence:.0%}\")\n",
        "        print(f\"  Categories: {[c.value for c in f.categories]}\")\n",
        "        print(f\"  Reasoning: {f.reasoning}\")\n",
        "    \n",
        "    # Case 3: Has rating results\n",
        "    if \"rating\" in result:\n",
        "        r = result[\"rating\"]\n",
        "        print(f\"\\nRating Results:\")\n",
        "        print(f\"  Overall Score: {r.overall_score:.1f}/10\")\n",
        "        print(f\"  Strategic Fit: {r.strategic_fit:.1f}/10\")\n",
        "        print(f\"  Win Probability: {r.win_probability:.1f}/10\")\n",
        "        print(f\"\\n  Strengths:\")\n",
        "        for s in r.strengths:\n",
        "            print(f\"    - {s}\")\n",
        "        print(f\"\\n  Risks:\")\n",
        "        for risk in r.risks:\n",
        "            print(f\"    - {risk}\")\n",
        "        print(f\"\\n  Recommendation: {r.recommendation[:150]}...\")\n",
        "    \n",
        "    # Case 4: Document was generated\n",
        "    if \"document\" in result:\n",
        "        print(f\"\\nStatus: DOCUMENT GENERATED\")\n",
        "        print(f\"\\nExecutive Summary (excerpt):\")\n",
        "        print(f\"  {result['document'].executive_summary[:250]}...\")\n",
        "        print(f\"\\nProcessing Time: {result['time']:.1f} seconds\")\n",
        "    elif \"rating\" in result:\n",
        "        print(f\"\\nStatus: RATED BUT NO DOCUMENT (score below threshold)\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What You Just Built\n",
        "\n",
        "Congratulations! You just built a complete AI agent system that:\n",
        "\n",
        "1. **Filters** tenders by relevance (classification task)\n",
        "2. **Rates** opportunities on multiple dimensions (scoring task)\n",
        "3. **Generates** professional documents (content creation task)\n",
        "4. **Orchestrates** multiple agents in a workflow (coordination task)\n",
        "\n",
        "**Key Patterns You Learned:**\n",
        "\n",
        "**1. LLM API calls:** Making HTTP requests to interact with a Language Model\n",
        "- Similar to how you'd call any web API\n",
        "- Send a request with your prompt, receive a response with generated text\n",
        "\n",
        "**2. Structured outputs:** Using Pydantic schemas to get predictable, validated data\n",
        "- Instead of parsing free-form text, we get data structures we can use directly\n",
        "- The LLM fills in fields according to our defined schema\n",
        "- Automatic validation ensures data integrity\n",
        "\n",
        "**3. Prompt engineering:** Writing clear instructions to guide LLM behavior\n",
        "- Specific prompts lead to better results\n",
        "- Including examples, constraints, and output format requirements helps\n",
        "- Iterative refinement improves accuracy\n",
        "\n",
        "**4. Temperature control:** Balancing precision versus creativity\n",
        "- Low temperature (0.1-0.3): Focused, deterministic, good for classification\n",
        "- High temperature (0.7-1.0): Creative, varied, good for content generation\n",
        "- This parameter controls the randomness in LLM outputs\n",
        "\n",
        "**5. Multi-agent design:** Breaking complex tasks into specialized components\n",
        "- Each agent has a specific role and expertise\n",
        "- Sequential workflow: Output of one agent feeds into the next\n",
        "- Easier to test, debug, and improve individual components\n",
        "\n",
        "---\n",
        "\n",
        "## Experiments to Try\n",
        "\n",
        "**1. Temperature Impact:**\n",
        "- Change temperature in FilterAgent to 0.5\n",
        "- Run again, compare confidence scores\n",
        "- Question to explore: Does it change decisions? Why might that happen?\n",
        "\n",
        "**2. Prompt Engineering:**\n",
        "- Modify FilterAgent prompt to be more specific\n",
        "- Add examples of relevant tenders in the prompt\n",
        "- Question: Does accuracy improve? What happens if you make it too specific?\n",
        "\n",
        "**3. Add Your Own Tender:**\n",
        "- Create a new Tender object with real or fictional data\n",
        "- Process it through the system\n",
        "- Analyze: Did it get categorized correctly? What was the score?\n",
        "\n",
        "**4. Scoring Threshold:**\n",
        "- Change the 7.0 threshold in the orchestrator\n",
        "- Try 5.0 (more lenient) or 8.0 (more strict)\n",
        "- Observe: How does it affect which documents get generated?\n",
        "\n",
        "**5. Error Handling:**\n",
        "- What happens if LM Studio is not running?\n",
        "- Try to handle errors gracefully with try/except blocks\n",
        "- Add timeout handling for slow responses\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts Explained\n",
        "\n",
        "**What is an Agent?**\n",
        "An agent is a component that uses an LLM to perform a specific task. It has:\n",
        "- A clear responsibility (filtering, rating, or generating)\n",
        "- A specialized prompt tailored to its task\n",
        "- Input and output data structures\n",
        "- Configuration parameters (like temperature)\n",
        "\n",
        "**What is Orchestration?**\n",
        "Orchestration is the coordination of multiple agents to complete a complex workflow. The orchestrator:\n",
        "- Determines the order of operations\n",
        "- Passes data between agents\n",
        "- Handles conditional logic (e.g., skip document generation if score is low)\n",
        "- Manages errors and provides feedback\n",
        "\n",
        "**Why Use Local LLMs?**\n",
        "Running LLMs locally (like with LM Studio) offers:\n",
        "- Privacy: Your data never leaves your machine\n",
        "- Cost: No per-token API fees\n",
        "- Speed: No network latency to external services\n",
        "- Control: You can choose and customize the model\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "**Notebook 02:** Experiments with prompt engineering  \n",
        "**Notebook 03:** Real data collection and validation  \n",
        "**Notebook 04:** Production code structure and best practices  \n",
        "\n",
        "**Advanced topics to explore:**\n",
        "- Parallel processing of multiple tenders\n",
        "- Caching LLM responses to avoid redundant calls\n",
        "- Adding a feedback loop to improve agent performance\n",
        "- Implementing retrieval-augmented generation (RAG) for domain knowledge\n",
        "- Monitoring and logging for production deployments\n",
        "\n",
        "---\n",
        "\n",
        "## Save Your Work\n",
        "\n",
        "```bash\n",
        "# In terminal:\n",
        "git add learn/02_mvp_complete.ipynb\n",
        "git commit -m \"Complete: First working AI agent system\n",
        "\n",
        "Built 3-agent orchestration:\n",
        "- Filter: Classifies tenders by relevance with confidence scoring\n",
        "- Rating: Multi-dimensional scoring of opportunities\n",
        "- Generator: Professional document creation with variable creativity\n",
        "\n",
        "Processing time: Approximately 8s per tender on M1 Mac\n",
        "Cost: $0 (local LLM via LM Studio)\n",
        "\n",
        "Ready for experimentation phase.\"\n",
        "\n",
        "git tag -a v0.1-mvp -m \"First working MVP complete\"\n",
        "```\n",
        "\n",
        "**What this commit represents:**\n",
        "- A working baseline you can return to\n",
        "- Clear documentation of what was accomplished\n",
        "- A reference point for measuring improvements"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
