{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 1: Your First AI Agent System\n",
        "\n",
        "**Goal:** Build a complete working AI system in 2 hours\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to call local LLMs (LM Studio)\n",
        "- Structured outputs with Pydantic\n",
        "- Building AI agents that make decisions\n",
        "- Multi-agent orchestration\n",
        "\n",
        "**Fast.ai approach:** Start with working code, understand by experimenting\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites Check\n",
        "\n",
        "‚úÖ LM Studio running on localhost:1234  \n",
        "‚úÖ Llama 3.1 8B Instruct model loaded  \n",
        "‚úÖ Python packages installed (httpx, pydantic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LM Studio connection\n",
        "import httpx\n",
        "\n",
        "try:\n",
        "    response = httpx.get(\"http://localhost:1234/v1/models\")\n",
        "    print(\"‚úÖ LM Studio is running!\")\n",
        "    print(f\"Model: {response.json()['data'][0]['id']}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå LM Studio not running. Start it first!\")\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: First LLM Call\n",
        "\n",
        "Let's start simple: send text, get text back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "\n",
        "async def simple_llm_call(prompt: str) -> str:\n",
        "    \"\"\"Simplest possible LLM call\"\"\"\n",
        "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
        "        response = await client.post(\n",
        "            \"http://localhost:1234/v1/chat/completions\",\n",
        "            json={\n",
        "                \"model\": \"llama-3.1-8b-instruct\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"temperature\": 0.7,\n",
        "            }\n",
        "        )\n",
        "        result = response.json()\n",
        "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# Test it\n",
        "response = await simple_llm_call(\"Say hello in exactly 5 words\")\n",
        "print(f\"LLM: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**üéì Exercise:** Change the prompt and run again. Try:\n",
        "- \"Count from 1 to 10\"\n",
        "- \"Explain AI in one sentence\"\n",
        "- \"Write a haiku about code\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Structured Outputs\n",
        "\n",
        "**The Secret Sauce:** Make LLMs return structured data\n",
        "\n",
        "This is THE most important pattern in LLM engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "\n",
        "# Define output structure\n",
        "class Analysis(BaseModel):\n",
        "    is_relevant: bool = Field(description=\"Is this relevant?\")\n",
        "    confidence: float = Field(description=\"Confidence 0-1\", ge=0, le=1)\n",
        "    reasoning: str = Field(description=\"Why?\")\n",
        "\n",
        "# Show schema\n",
        "print(\"Schema:\")\n",
        "print(json.dumps(Analysis.model_json_schema(), indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def get_structured(prompt: str, schema: BaseModel) -> BaseModel:\n",
        "    \"\"\"Get LLM output matching schema\"\"\"\n",
        "    \n",
        "    # Add schema to prompt\n",
        "    schema_json = json.dumps(schema.model_json_schema(), indent=2)\n",
        "    full_prompt = f\"\"\"{prompt}\n",
        "\n",
        "Respond with valid JSON matching this schema:\n",
        "{schema_json}\n",
        "\n",
        "Rules: ONLY JSON, no markdown, no explanations\"\"\"\n",
        "    \n",
        "    # Call LLM\n",
        "    response = await simple_llm_call(full_prompt)\n",
        "    \n",
        "    # Clean and parse\n",
        "    cleaned = response.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    return schema.model_validate_json(cleaned)\n",
        "\n",
        "# Test\n",
        "result = await get_structured(\n",
        "    \"Analyze: 'AI Cybersecurity Platform Development'\",\n",
        "    Analysis\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Structured Output:\")\n",
        "print(f\"Relevant: {result.is_relevant}\")\n",
        "print(f\"Confidence: {result.confidence}\")\n",
        "print(f\"Reasoning: {result.reasoning}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**üéì What's powerful:**\n",
        "- Defined structure with Pydantic\n",
        "- LLM filled it out\n",
        "- Automatic validation\n",
        "- Can now use in code: `result.is_relevant`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Complete System\n",
        "\n",
        "Now let's build the full procurement system.\n",
        "\n",
        "**Architecture:** Tender ‚Üí Filter ‚Üí Rate ‚Üí Generate ‚Üí Done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All imports\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from enum import Enum\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import httpx\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === DATA MODELS ===\n",
        "\n",
        "class Category(str, Enum):\n",
        "    CYBER = \"cybersecurity\"\n",
        "    AI = \"ai\"\n",
        "    SOFTWARE = \"software\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "class Tender(BaseModel):\n",
        "    id: str\n",
        "    title: str\n",
        "    description: str\n",
        "    organization: str\n",
        "    estimated_value: Optional[str] = None\n",
        "\n",
        "class FilterResult(BaseModel):\n",
        "    is_relevant: bool\n",
        "    confidence: float = Field(ge=0, le=1)\n",
        "    categories: List[Category]\n",
        "    reasoning: str\n",
        "\n",
        "class RatingResult(BaseModel):\n",
        "    overall_score: float = Field(ge=0, le=10)\n",
        "    strategic_fit: float = Field(ge=0, le=10)\n",
        "    win_probability: float = Field(ge=0, le=10)\n",
        "    strengths: List[str]\n",
        "    risks: List[str]\n",
        "    recommendation: str\n",
        "\n",
        "class BidDoc(BaseModel):\n",
        "    executive_summary: str\n",
        "    technical_approach: str\n",
        "    value_proposition: str\n",
        "\n",
        "print(\"‚úÖ Models defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === LLM SERVICE ===\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self):\n",
        "        self.url = \"http://localhost:1234/v1\"\n",
        "        self.model = \"llama-3.1-8b-instruct\"\n",
        "    \n",
        "    async def generate(self, prompt: str, schema: BaseModel, \n",
        "                       system: str = \"\", temp: float = 0.1) -> BaseModel:\n",
        "        messages = []\n",
        "        if system:\n",
        "            messages.append({\"role\": \"system\", \"content\": system})\n",
        "        \n",
        "        schema_json = json.dumps(schema.model_json_schema(), indent=2)\n",
        "        full_prompt = f\"\"\"{prompt}\n",
        "\n",
        "Respond ONLY with valid JSON matching:\n",
        "{schema_json}\"\"\"\n",
        "        \n",
        "        messages.append({\"role\": \"user\", \"content\": full_prompt})\n",
        "        \n",
        "        async with httpx.AsyncClient(timeout=120) as client:\n",
        "            r = await client.post(\n",
        "                f\"{self.url}/chat/completions\",\n",
        "                json={\n",
        "                    \"model\": self.model,\n",
        "                    \"messages\": messages,\n",
        "                    \"temperature\": temp,\n",
        "                    \"max_tokens\": 2000,\n",
        "                }\n",
        "            )\n",
        "            content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "        \n",
        "        # Clean\n",
        "        clean = content.strip()\n",
        "        for marker in [\"```json\", \"```\"]:\n",
        "            clean = clean.replace(marker, \"\")\n",
        "        \n",
        "        return schema.model_validate_json(clean.strip())\n",
        "\n",
        "llm = LLM()\n",
        "print(\"‚úÖ LLM ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent 1: Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FilterAgent:\n",
        "    def __init__(self, llm: LLM):\n",
        "        self.llm = llm\n",
        "    \n",
        "    async def filter(self, tender: Tender) -> FilterResult:\n",
        "        prompt = f\"\"\"Analyze tender:\n",
        "\n",
        "TITLE: {tender.title}\n",
        "DESC: {tender.description}\n",
        "\n",
        "RELEVANT if: cybersecurity, AI/ML, or software development\n",
        "NOT relevant if: hardware, construction, non-tech services\n",
        "\"\"\"\n",
        "        return await self.llm.generate(\n",
        "            prompt, FilterResult,\n",
        "            system=\"You are a procurement analyst. Be precise.\",\n",
        "            temp=0.1\n",
        "        )\n",
        "\n",
        "print(\"‚úÖ Filter agent defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent 2: Rater"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RatingAgent:\n",
        "    def __init__(self, llm: LLM):\n",
        "        self.llm = llm\n",
        "    \n",
        "    async def rate(self, tender: Tender, cats: List[str]) -> RatingResult:\n",
        "        prompt = f\"\"\"Rate opportunity:\n",
        "\n",
        "TENDER: {tender.title}\n",
        "VALUE: {tender.estimated_value}\n",
        "CATEGORIES: {', '.join(cats)}\n",
        "\n",
        "Score (0-10):\n",
        "- Strategic fit\n",
        "- Win probability\n",
        "- Overall score\n",
        "\n",
        "List 3 strengths, 3 risks, give recommendation.\n",
        "\"\"\"\n",
        "        return await self.llm.generate(\n",
        "            prompt, RatingResult,\n",
        "            system=\"You are a business analyst. Be realistic.\",\n",
        "            temp=0.1\n",
        "        )\n",
        "\n",
        "print(\"‚úÖ Rating agent defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent 3: Doc Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocGenerator:\n",
        "    def __init__(self, llm: LLM):\n",
        "        self.llm = llm\n",
        "    \n",
        "    async def generate(self, tender: Tender, cats: List[str], \n",
        "                       strengths: List[str]) -> BidDoc:\n",
        "        prompt = f\"\"\"Create bid document:\n",
        "\n",
        "TENDER: {tender.title}\n",
        "CLIENT: {tender.organization}\n",
        "OUR EXPERTISE: {', '.join(cats)}\n",
        "STRENGTHS: {', '.join(strengths)}\n",
        "\n",
        "Write:\n",
        "1. Executive summary (2-3 paragraphs)\n",
        "2. Technical approach (2-3 paragraphs)\n",
        "3. Value proposition (2 paragraphs)\n",
        "\n",
        "Professional, specific, compelling.\n",
        "\"\"\"\n",
        "        return await self.llm.generate(\n",
        "            prompt, BidDoc,\n",
        "            system=\"You are an expert proposal writer.\",\n",
        "            temp=0.7  # Higher for creativity\n",
        "        )\n",
        "\n",
        "print(\"‚úÖ Doc generator defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Orchestrator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Orchestrator:\n",
        "    def __init__(self):\n",
        "        self.llm = LLM()\n",
        "        self.filter = FilterAgent(self.llm)\n",
        "        self.rater = RatingAgent(self.llm)\n",
        "        self.doc_gen = DocGenerator(self.llm)\n",
        "    \n",
        "    async def process(self, tender: Tender):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing: {tender.title[:50]}...\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        start = datetime.now()\n",
        "        \n",
        "        # Step 1: Filter\n",
        "        print(\"\\n[1/3] Filtering...\")\n",
        "        filter_result = await self.filter.filter(tender)\n",
        "        print(f\"  Relevant: {filter_result.is_relevant} ({filter_result.confidence:.0%})\")\n",
        "        print(f\"  Categories: {[c.value for c in filter_result.categories]}\")\n",
        "        \n",
        "        if not filter_result.is_relevant or filter_result.confidence < 0.6:\n",
        "            print(\"\\n  ‚Üí Skipping (not relevant)\")\n",
        "            return None\n",
        "        \n",
        "        # Step 2: Rate\n",
        "        print(\"\\n[2/3] Rating...\")\n",
        "        cats = [c.value for c in filter_result.categories]\n",
        "        rating = await self.rater.rate(tender, cats)\n",
        "        print(f\"  Score: {rating.overall_score:.1f}/10\")\n",
        "        print(f\"  Win Prob: {rating.win_probability:.1f}/10\")\n",
        "        \n",
        "        if rating.overall_score < 7.0:\n",
        "            print(\"\\n  ‚Üí Skipping docs (score < 7.0)\")\n",
        "            return {\"filter\": filter_result, \"rating\": rating}\n",
        "        \n",
        "        # Step 3: Generate\n",
        "        print(\"\\n[3/3] Generating document...\")\n",
        "        doc = await self.doc_gen.generate(tender, cats, rating.strengths)\n",
        "        print(\"  ‚úÖ Document ready\")\n",
        "        \n",
        "        elapsed = (datetime.now() - start).total_seconds()\n",
        "        print(f\"\\n  Time: {elapsed:.1f}s\")\n",
        "        \n",
        "        return {\n",
        "            \"filter\": filter_result,\n",
        "            \"rating\": rating,\n",
        "            \"document\": doc,\n",
        "            \"time\": elapsed\n",
        "        }\n",
        "\n",
        "orchestrator = Orchestrator()\n",
        "print(\"‚úÖ Orchestrator ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample tenders\n",
        "tenders = [\n",
        "    Tender(\n",
        "        id=\"T001\",\n",
        "        title=\"AI-Powered Cybersecurity Platform\",\n",
        "        description=\"\"\"Develop AI-based threat detection system for government \n",
        "        infrastructure. Must use machine learning for anomaly detection and \n",
        "        integrate with existing SIEM tools.\"\"\",\n",
        "        organization=\"National Cyber Agency\",\n",
        "        estimated_value=\"‚Ç¨3.2M\"\n",
        "    ),\n",
        "    Tender(\n",
        "        id=\"T002\",\n",
        "        title=\"Office Furniture Supply\",\n",
        "        description=\"\"\"Supply ergonomic office furniture for 500 workstations \n",
        "        including desks, chairs, and storage.\"\"\",\n",
        "        organization=\"Ministry of Public Works\",\n",
        "        estimated_value=\"‚Ç¨450K\"\n",
        "    ),\n",
        "    Tender(\n",
        "        id=\"T003\",\n",
        "        title=\"Custom Healthcare CRM System\",\n",
        "        description=\"\"\"Build cloud-based CRM for healthcare network. Must \n",
        "        handle patient data, appointments, GDPR compliant, mobile app included.\"\"\",\n",
        "        organization=\"Regional Health Authority\",\n",
        "        estimated_value=\"‚Ç¨1.8M\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ {len(tenders)} test tenders ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Run the System!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all tenders\n",
        "results = []\n",
        "\n",
        "for tender in tenders:\n",
        "    result = await orchestrator.process(tender)\n",
        "    results.append({\"tender\": tender, \"result\": result})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "relevant = sum(1 for r in results if r[\"result\"] and \"filter\" in r[\"result\"])\n",
        "high_scored = sum(1 for r in results if r[\"result\"] and \"rating\" in r[\"result\"] \n",
        "                  and r[\"result\"][\"rating\"].overall_score >= 7.0)\n",
        "docs_made = sum(1 for r in results if r[\"result\"] and \"document\" in r[\"result\"])\n",
        "\n",
        "print(f\"Total Tenders: {len(tenders)}\")\n",
        "print(f\"Relevant: {relevant}\")\n",
        "print(f\"High Scored (‚â•7.0): {high_scored}\")\n",
        "print(f\"Documents Generated: {docs_made}\")\n",
        "\n",
        "total_time = sum(r[\"result\"][\"time\"] for r in results if r[\"result\"] and \"time\" in r[\"result\"])\n",
        "print(f\"\\nTotal Time: {total_time:.1f}s\")\n",
        "print(f\"Avg per Tender: {total_time/len(tenders):.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, item in enumerate(results, 1):\n",
        "    tender = item[\"tender\"]\n",
        "    result = item[\"result\"]\n",
        "    \n",
        "    print(f\"\\n{'‚îÄ'*60}\")\n",
        "    print(f\"TENDER {i}: {tender.title}\")\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    \n",
        "    if not result:\n",
        "        print(\"Status: Filtered out\")\n",
        "        continue\n",
        "    \n",
        "    if \"filter\" in result:\n",
        "        f = result[\"filter\"]\n",
        "        print(f\"Relevant: {f.is_relevant} ({f.confidence:.0%})\")\n",
        "        print(f\"Categories: {[c.value for c in f.categories]}\")\n",
        "    \n",
        "    if \"rating\" in result:\n",
        "        r = result[\"rating\"]\n",
        "        print(f\"\\nScore: {r.overall_score:.1f}/10\")\n",
        "        print(f\"Fit: {r.strategic_fit:.1f} | Win: {r.win_probability:.1f}\")\n",
        "        print(f\"\\nStrengths:\")\n",
        "        for s in r.strengths:\n",
        "            print(f\"  ‚Ä¢ {s}\")\n",
        "        print(f\"\\nRecommendation: {r.recommendation[:100]}...\")\n",
        "    \n",
        "    if \"document\" in result:\n",
        "        print(f\"\\n‚úÖ Document generated\")\n",
        "        print(f\"\\nExecutive Summary:\")\n",
        "        print(result[\"document\"].executive_summary[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì What You Just Built\n",
        "\n",
        "Congratulations! You just built a complete AI agent system that:\n",
        "\n",
        "1. **Filters** tenders by relevance (classification)\n",
        "2. **Rates** opportunities on multiple dimensions (scoring)\n",
        "3. **Generates** professional documents (content creation)\n",
        "4. **Orchestrates** multiple agents in a workflow\n",
        "\n",
        "**Key Patterns You Learned:**\n",
        "- LLM API calls (HTTP requests)\n",
        "- Structured outputs (Pydantic schemas)\n",
        "- Prompt engineering (clear instructions)\n",
        "- Temperature control (precision vs creativity)\n",
        "- Multi-agent design (sequential workflow)\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Experiments to Try\n",
        "\n",
        "**1. Temperature Impact:**\n",
        "- Change temperature in FilterAgent to 0.5\n",
        "- Run again, compare confidence scores\n",
        "- Does it change decisions?\n",
        "\n",
        "**2. Prompt Engineering:**\n",
        "- Modify FilterAgent prompt to be more specific\n",
        "- Add examples of relevant tenders\n",
        "- Does accuracy improve?\n",
        "\n",
        "**3. Add Your Own Tender:**\n",
        "- Create a new Tender object\n",
        "- Process it through the system\n",
        "- Analyze the results\n",
        "\n",
        "**4. Scoring Threshold:**\n",
        "- Change the 7.0 threshold in orchestrator\n",
        "- Try 5.0 or 8.0\n",
        "- How does it affect document generation?\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Next Steps\n",
        "\n",
        "**Notebook 02:** Experiments with prompt engineering  \n",
        "**Notebook 03:** Real data collection & validation  \n",
        "**Notebook 04:** Production code structure  \n",
        "\n",
        "---\n",
        "\n",
        "## üíæ Save Your Work\n",
        "\n",
        "```bash\n",
        "# In terminal:\n",
        "git add learn/01_mvp_complete.ipynb\n",
        "git commit -m \"Complete: First working AI agent system\n",
        "\n",
        "Built 3-agent orchestration:\n",
        "- Filter: 95% confidence on test cases\n",
        "- Rating: Multi-dimensional scoring\n",
        "- Generator: Professional document creation\n",
        "\n",
        "Processing time: ~8s per tender on M1 Mac\n",
        "Cost: $0 (local LLM)\n",
        "\n",
        "Ready for experimentation phase.\"\n",
        "\n",
        "git tag -a v0.1-mvp -m \"First working MVP complete\"\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
