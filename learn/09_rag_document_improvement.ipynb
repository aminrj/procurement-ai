{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a014b07d",
   "metadata": {},
   "source": [
    "# RAG for Document Quality Improvement\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines information retrieval with LLM generation to produce better outputs.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Our DocumentGenerator agent creates bid documents, but they're generic. The LLM doesn't know:\n",
    "- What good procurement documents look like for our domain\n",
    "- Common patterns in successful bids\n",
    "- Domain-specific terminology and structure\n",
    "\n",
    "**Result**: Documents are grammatically correct but lack the quality and specificity of human-written bids.\n",
    "\n",
    "## The Solution: RAG\n",
    "\n",
    "Instead of generating from scratch, we:\n",
    "1. **Build a knowledge base** of high-quality example documents\n",
    "2. **Retrieve relevant examples** similar to the current tender\n",
    "3. **Augment the prompt** with these examples before generation\n",
    "\n",
    "This teaches the LLM by example, dramatically improving output quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "1. How RAG works (theory ‚Üí practice)\n",
    "2. Building and querying vector databases\n",
    "3. Embeddings and similarity search\n",
    "4. Prompt augmentation techniques\n",
    "5. Measuring RAG impact on output quality\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eec96c",
   "metadata": {},
   "source": [
    "## Part 1: Understanding RAG\n",
    "\n",
    "### Traditional Generation (No RAG)\n",
    "\n",
    "```\n",
    "User: \"Generate a bid for cybersecurity tender\"\n",
    "       ‚Üì\n",
    "LLM: Uses only its training data\n",
    "       ‚Üì\n",
    "Output: Generic security document\n",
    "```\n",
    "\n",
    "**Problem**: LLM has general knowledge but not your specific domain expertise.\n",
    "\n",
    "### RAG-Enhanced Generation\n",
    "\n",
    "```\n",
    "User: \"Generate a bid for cybersecurity tender\"\n",
    "       ‚Üì\n",
    "1. Search knowledge base for similar tenders\n",
    "       ‚Üì\n",
    "2. Retrieve: 3 high-quality cybersecurity bid examples\n",
    "       ‚Üì\n",
    "3. Augment prompt with retrieved examples\n",
    "       ‚Üì\n",
    "LLM: \"Here are examples of excellent bids... Now generate:\"\n",
    "       ‚Üì\n",
    "Output: Domain-specific, high-quality document\n",
    "```\n",
    "\n",
    "**Key insight**: RAG gives the LLM access to your institutional knowledge.\n",
    "\n",
    "### How Similarity Search Works\n",
    "\n",
    "Question: How do we find \"similar\" documents?\n",
    "\n",
    "Answer: **Embeddings** - convert text into numerical vectors that capture meaning.\n",
    "\n",
    "```\n",
    "Text: \"AI cybersecurity threat detection\"\n",
    "  ‚Üì (embedding model)\n",
    "Vector: [0.23, -0.45, 0.89, ..., 0.12]  (1536 dimensions)\n",
    "\n",
    "Text: \"ML-based security monitoring\"\n",
    "  ‚Üì (embedding model)\n",
    "Vector: [0.25, -0.43, 0.91, ..., 0.15]  (similar!)\n",
    "\n",
    "Text: \"Office furniture procurement\"\n",
    "  ‚Üì (embedding model)\n",
    "Vector: [-0.67, 0.12, -0.34, ..., 0.78]  (very different)\n",
    "```\n",
    "\n",
    "Similar concepts have similar vectors. We measure similarity using **cosine similarity**:\n",
    "- 1.0 = identical\n",
    "- 0.9+ = very similar\n",
    "- 0.7-0.9 = somewhat similar\n",
    "- <0.5 = unrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffc593",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up the Environment\n",
    "\n",
    "We'll use:\n",
    "- **ChromaDB**: Simple, fast vector database (no server needed)\n",
    "- **OpenAI Embeddings**: Convert text to vectors (via our LM Studio endpoint)\n",
    "- **Existing agents**: Filter and DocumentGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de019389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG environment initialized!\n",
      "LLM Endpoint: http://localhost:1234/v1\n",
      "Model: openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "from procurement_ai.config import Config\n",
    "\n",
    "# Initialize\n",
    "config = Config()\n",
    "\n",
    "print(\"‚úÖ RAG environment initialized!\")\n",
    "print(f\"LLM Endpoint: {config.LLM_BASE_URL}\")\n",
    "print(f\"Model: {config.LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844dbe4",
   "metadata": {},
   "source": [
    "## Part 3: Building a Knowledge Base\n",
    "\n",
    "First, let's create sample high-quality documents. In production, these would be real successful bids.\n",
    "\n",
    "### What Makes a Good Knowledge Base Entry?\n",
    "\n",
    "Each document should have:\n",
    "1. **Content**: The actual high-quality text\n",
    "2. **Metadata**: Category, success rate, date (for filtering)\n",
    "3. **Context**: When/why this example is relevant\n",
    "\n",
    "Let's create 5-6 examples covering different procurement categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52c4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Knowledge base created with 5 high-quality examples\n",
      "\n",
      "Categories:\n",
      "  - cybersecurity: AI-Powered Threat Detection Implementation (Success rate: 95%)\n",
      "  - ai: Predictive Maintenance AI Platform (Success rate: 88%)\n",
      "  - software: Custom CRM System Development (Success rate: 92%)\n",
      "  - data_analytics: Business Intelligence Dashboard Suite (Success rate: 87%)\n",
      "  - cloud: Cloud Migration and Infrastructure Modernization (Success rate: 91%)\n"
     ]
    }
   ],
   "source": [
    "# Sample knowledge base: high-quality bid excerpts\n",
    "KNOWLEDGE_BASE = [\n",
    "    {\n",
    "        \"id\": \"kb_001\",\n",
    "        \"category\": \"cybersecurity\",\n",
    "        \"title\": \"AI-Powered Threat Detection Implementation\",\n",
    "        \"content\": \"\"\"## Executive Summary\n",
    "\n",
    "Our proposed solution delivers enterprise-grade threat detection leveraging machine learning algorithms specifically tuned for government security requirements. The system processes 10M+ events daily, automatically categorizing threats by severity and triggering appropriate response protocols.\n",
    "\n",
    "## Technical Approach\n",
    "\n",
    "We employ a three-tier detection architecture:\n",
    "\n",
    "1. **Real-time Pattern Recognition**: Neural network models trained on 5+ years of government sector threat data, achieving 99.2% accuracy with <0.1% false positive rate.\n",
    "\n",
    "2. **Behavioral Analytics**: Anomaly detection using unsupervised learning to identify zero-day threats and insider risks before they escalate.\n",
    "\n",
    "3. **Automated Response**: Integration with existing SIEM infrastructure, enabling immediate containment actions while alerting security teams.\n",
    "\n",
    "## Implementation Timeline\n",
    "\n",
    "Phase 1 (Weeks 1-4): Infrastructure setup and data integration\n",
    "Phase 2 (Weeks 5-8): Model training and validation\n",
    "Phase 3 (Weeks 9-12): Deployment and team training\n",
    "\n",
    "## Compliance & Certification\n",
    "\n",
    "Our team holds ISO 27001, SOC 2 Type II, and government clearance. All data processing occurs on-premises, ensuring complete data sovereignty.\"\"\",\n",
    "        \"success_rate\": 0.95,\n",
    "        \"year\": 2025\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"kb_002\",\n",
    "        \"category\": \"ai\",\n",
    "        \"title\": \"Predictive Maintenance AI Platform\",\n",
    "        \"content\": \"\"\"## Solution Overview\n",
    "\n",
    "Our predictive maintenance platform reduces equipment downtime by 40% through advanced AI-driven failure prediction. The system monitors 200+ sensor parameters in real-time, predicting failures 72 hours in advance with 94% accuracy.\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "Built on proven open-source frameworks (TensorFlow, FastAPI), ensuring full transparency and no vendor lock-in. The platform consists of:\n",
    "\n",
    "**Data Pipeline**: Ingests sensor data at 1000 Hz, processes using Apache Kafka for real-time streaming\n",
    "\n",
    "**ML Models**: Ensemble of LSTM networks and Random Forests, retrained weekly with new operational data\n",
    "\n",
    "**Alert System**: Multi-channel notifications (email, SMS, dashboard) with configurable thresholds\n",
    "\n",
    "## Business Value\n",
    "\n",
    "Based on pilot deployments:\n",
    "- 40% reduction in unplanned downtime\n",
    "- 25% decrease in maintenance costs\n",
    "- ROI achieved within 8 months\n",
    "\n",
    "## Team Expertise\n",
    "\n",
    "Our engineers have deployed AI systems in 15+ industrial facilities. Lead architect: PhD in ML from MIT, 10 years industry experience.\"\"\",\n",
    "        \"success_rate\": 0.88,\n",
    "        \"year\": 2025\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"kb_003\",\n",
    "        \"category\": \"software\",\n",
    "        \"title\": \"Custom CRM System Development\",\n",
    "        \"content\": \"\"\"## Project Scope\n",
    "\n",
    "We will develop a cloud-native CRM system tailored to your organization's unique workflow, replacing legacy systems while preserving 15 years of customer data integrity.\n",
    "\n",
    "## Development Methodology\n",
    "\n",
    "**Agile with Weekly Sprints**: Client reviews every Friday, ensuring alignment and early issue detection\n",
    "\n",
    "**Technology Stack**:\n",
    "- Frontend: React with TypeScript for type safety\n",
    "- Backend: Python FastAPI, PostgreSQL database\n",
    "- Cloud: AWS with auto-scaling (handles 10x traffic spikes)\n",
    "- Security: OAuth2, encryption at rest and in transit\n",
    "\n",
    "## Migration Strategy\n",
    "\n",
    "Zero data loss guaranteed:\n",
    "1. Automated backup before each migration step\n",
    "2. Parallel running (old and new systems) for 30 days\n",
    "3. Validation: 100% data reconciliation before legacy shutdown\n",
    "\n",
    "## Support & Maintenance\n",
    "\n",
    "Year 1: Included in project cost (24/7 support)\n",
    "Year 2+: Optional SLA packages (99.9% uptime guarantee)\n",
    "\n",
    "## References\n",
    "\n",
    "Similar CRM projects completed for [Client A] and [Client B], both reporting >30% productivity gains post-implementation.\"\"\",\n",
    "        \"success_rate\": 0.92,\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"kb_004\",\n",
    "        \"category\": \"data_analytics\",\n",
    "        \"title\": \"Business Intelligence Dashboard Suite\",\n",
    "        \"content\": \"\"\"## Vision\n",
    "\n",
    "Transform your raw operational data into actionable insights with real-time dashboards that executives actually use. Our BI platform consolidates 15 data sources into unified, drill-down visualizations.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "**Executive Dashboard**: 6 KPIs updated every 5 minutes\n",
    "- Revenue trends with forecasting\n",
    "- Operational efficiency metrics\n",
    "- Customer satisfaction tracking\n",
    "\n",
    "**Departmental Views**: Customized for sales, operations, finance\n",
    "\n",
    "**Mobile-First Design**: Full functionality on tablets and phones\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "Built on Tableau/Power BI (client preference), with custom connectors for your ERP, CRM, and legacy systems.\n",
    "\n",
    "**Data Pipeline**: ETL processes running hourly, validating data quality at each step\n",
    "\n",
    "**Performance**: 2-second load times even with 5-year historical data\n",
    "\n",
    "## Training & Adoption\n",
    "\n",
    "3-day workshop for power users + recorded tutorials for all staff. Our track record: 85% adoption rate within first month.\"\"\",\n",
    "        \"success_rate\": 0.87,\n",
    "        \"year\": 2025\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"kb_005\",\n",
    "        \"category\": \"cloud\",\n",
    "        \"title\": \"Cloud Migration and Infrastructure Modernization\",\n",
    "        \"content\": \"\"\"## Migration Strategy\n",
    "\n",
    "We execute low-risk, phased cloud migrations that maintain business continuity. No downtime for critical services.\n",
    "\n",
    "## Assessment Phase (Weeks 1-2)\n",
    "\n",
    "- Inventory all applications and dependencies\n",
    "- Categorize by migration complexity (lift-and-shift vs. re-architecture)\n",
    "- Identify cost optimization opportunities\n",
    "\n",
    "## Migration Approach\n",
    "\n",
    "**Wave 1**: Non-critical systems (build confidence)\n",
    "**Wave 2**: Business applications (during low-usage windows)\n",
    "**Wave 3**: Mission-critical services (with full rollback plans)\n",
    "\n",
    "## Cloud Architecture\n",
    "\n",
    "**Multi-AZ Deployment**: High availability across 3 availability zones\n",
    "**Auto-Scaling**: Handles traffic spikes automatically, reduces costs during low usage\n",
    "**Disaster Recovery**: RPO=1 hour, RTO=4 hours\n",
    "\n",
    "## Cost Governance\n",
    "\n",
    "Budget alerts, rightsizing recommendations, and reserved instance strategies typically reduce cloud spend by 30-40% compared to on-demand pricing.\n",
    "\n",
    "## Security & Compliance\n",
    "\n",
    "All cloud resources configured following CIS benchmarks. Continuous compliance monitoring via AWS Security Hub / Azure Security Center.\"\"\",\n",
    "        \"success_rate\": 0.91,\n",
    "        \"year\": 2025\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Knowledge base created with {len(KNOWLEDGE_BASE)} high-quality examples\")\n",
    "print(\"\\nCategories:\")\n",
    "for doc in KNOWLEDGE_BASE:\n",
    "    print(f\"  - {doc['category']}: {doc['title']} (Success rate: {doc['success_rate']:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ee733",
   "metadata": {},
   "source": [
    "## Part 4: Creating Embeddings\n",
    "\n",
    "Now we convert each document into a vector (embedding). This lets us perform similarity search.\n",
    "\n",
    "### What Are Embeddings?\n",
    "\n",
    "Think of embeddings as \"GPS coordinates\" for meaning:\n",
    "- Similar concepts are close together in vector space\n",
    "- Unrelated concepts are far apart\n",
    "- The model learned these relationships from massive text datasets\n",
    "\n",
    "We'll use our local LLM's embedding endpoint (compatible with OpenAI API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc371a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding service working!\n",
      "   Text: 'AI-powered cybersecurity threat detection system'\n",
      "   Embedding dimensions: 768\n",
      "   First 5 values: [0.04822518303990364, 0.06791120767593384, -0.18309970200061798, 0.016416719183325768, 0.08895919471979141]\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class EmbeddingService:\n",
    "    \"\"\"Simple wrapper for creating text embeddings\"\"\"\n",
    "    \n",
    "    # Embedding model (different from chat model)\n",
    "    EMBEDDING_MODEL = \"text-embedding-nomic-embed-text-v1.5\"\n",
    "    EMBEDDING_DIMENSION = 768\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.base_url = config.LLM_BASE_URL\n",
    "        \n",
    "    async def create_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Create embedding for a single text\"\"\"\n",
    "        if not text or not text.strip():\n",
    "            raise ValueError(\"Text cannot be empty\")\n",
    "            \n",
    "        async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/embeddings\",\n",
    "                json={\n",
    "                    \"input\": text,\n",
    "                    \"model\": self.EMBEDDING_MODEL  # Use embedding model, not chat model\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            return data['data'][0]['embedding']\n",
    "    \n",
    "    async def create_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Create embeddings for multiple texts\"\"\"\n",
    "        # Process in batch for efficiency\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            emb = await self.create_embedding(text)\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "\n",
    "# Initialize\n",
    "embedding_service = EmbeddingService(config)\n",
    "\n",
    "# Test it\n",
    "test_text = \"AI-powered cybersecurity threat detection system\"\n",
    "test_embedding = await embedding_service.create_embedding(test_text)\n",
    "\n",
    "print(f\"‚úÖ Embedding service working!\")\n",
    "print(f\"   Text: '{test_text}'\")\n",
    "print(f\"   Embedding dimensions: {len(test_embedding)}\")\n",
    "print(f\"   First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de296732",
   "metadata": {},
   "source": [
    "### Understanding Cosine Similarity\n",
    "\n",
    "To find similar documents, we compare their embeddings using cosine similarity:\n",
    "\n",
    "```\n",
    "similarity = dot_product(vec1, vec2) / (||vec1|| * ||vec2||)\n",
    "```\n",
    "\n",
    "Result is between -1 and 1:\n",
    "- **1.0**: Identical meaning\n",
    "- **0.9-1.0**: Very similar\n",
    "- **0.7-0.9**: Somewhat related\n",
    "- **<0.5**: Unrelated\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321a3af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Similarity Test:\n",
      "\n",
      "  'AI cybersecurity threat detection'\n",
      "  vs\n",
      "  'ML-based security monitoring system'\n",
      "  ‚Üí Similarity: 0.670 (Very similar! ‚úÖ)\n",
      "\n",
      "  'AI cybersecurity threat detection'\n",
      "  vs\n",
      "  'Office furniture and interior design'\n",
      "  ‚Üí Similarity: 0.319 (Very different, as expected ‚úÖ)\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    vec1_np = np.array(vec1)\n",
    "    vec2_np = np.array(vec2)\n",
    "    \n",
    "    dot_product = np.dot(vec1_np, vec2_np)\n",
    "    norm1 = np.linalg.norm(vec1_np)\n",
    "    norm2 = np.linalg.norm(vec2_np)\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Test similarity between different concepts\n",
    "text1 = \"AI cybersecurity threat detection\"\n",
    "text2 = \"ML-based security monitoring system\"\n",
    "text3 = \"Office furniture and interior design\"\n",
    "\n",
    "emb1 = await embedding_service.create_embedding(text1)\n",
    "emb2 = await embedding_service.create_embedding(text2)\n",
    "emb3 = await embedding_service.create_embedding(text3)\n",
    "\n",
    "sim_1_2 = cosine_similarity(emb1, emb2)\n",
    "sim_1_3 = cosine_similarity(emb1, emb3)\n",
    "\n",
    "print(\"üìä Similarity Test:\")\n",
    "print(f\"\\n  '{text1}'\")\n",
    "print(f\"  vs\")\n",
    "print(f\"  '{text2}'\")\n",
    "print(f\"  ‚Üí Similarity: {sim_1_2:.3f} (Very similar! ‚úÖ)\\n\")\n",
    "\n",
    "print(f\"  '{text1}'\")\n",
    "print(f\"  vs\")\n",
    "print(f\"  '{text3}'\")\n",
    "print(f\"  ‚Üí Similarity: {sim_1_3:.3f} (Very different, as expected ‚úÖ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8eb0ba",
   "metadata": {},
   "source": [
    "## Part 5: Building the Vector Store\n",
    "\n",
    "Now let's store our knowledge base documents with their embeddings in ChromaDB.\n",
    "\n",
    "### Why Use a Vector Database?\n",
    "\n",
    "We could calculate similarity against every document manually, but:\n",
    "- Slow with 1000+ documents\n",
    "- Need to recompute every query\n",
    "\n",
    "Vector databases like Chroma:\n",
    "- Index embeddings for fast search (milliseconds)\n",
    "- Handle filtering by metadata\n",
    "- Persist data to disk\n",
    "\n",
    "Let's set it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14967ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB initialized\n",
      "   Collection: procurement_knowledge_base\n",
      "   Documents: 0\n"
     ]
    }
   ],
   "source": [
    "# Install ChromaDB if needed\n",
    "# !pip install chromadb\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma (in-memory for this notebook)\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    "))\n",
    "\n",
    "# Create or get collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"procurement_knowledge_base\",\n",
    "    metadata={\"description\": \"High-quality bid examples for RAG\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB initialized\")\n",
    "print(f\"   Collection: {collection.name}\")\n",
    "print(f\"   Documents: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abc7b0",
   "metadata": {},
   "source": [
    "### Loading Documents into ChromaDB\n",
    "\n",
    "For each document, we need:\n",
    "1. Document text (what to retrieve)\n",
    "2. Embedding (for similarity search)\n",
    "3. Metadata (for filtering)\n",
    "4. Unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b18154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating embeddings for knowledge base...\n",
      "   (This may take 30-60 seconds for 5 documents)\n",
      "\n",
      "   ‚úì 1/5: AI-Powered Threat Detection Implementation\n",
      "   ‚úì 2/5: Predictive Maintenance AI Platform\n",
      "   ‚úì 3/5: Custom CRM System Development\n",
      "   ‚úì 4/5: Business Intelligence Dashboard Suite\n",
      "   ‚úì 5/5: Cloud Migration and Infrastructure Modernization\n",
      "\n",
      "‚úÖ Knowledge base loaded: 5 documents\n"
     ]
    }
   ],
   "source": [
    "async def load_knowledge_base():\n",
    "    \"\"\"Load documents into ChromaDB\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Creating embeddings for knowledge base...\")\n",
    "    print(f\"   (This may take 30-60 seconds for {len(KNOWLEDGE_BASE)} documents)\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(KNOWLEDGE_BASE):\n",
    "        # Create embedding\n",
    "        embedding = await embedding_service.create_embedding(doc['content'])\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        collection.add(\n",
    "            documents=[doc['content']],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[{\n",
    "                \"category\": doc['category'],\n",
    "                \"title\": doc['title'],\n",
    "                \"success_rate\": doc['success_rate'],\n",
    "                \"year\": doc['year']\n",
    "            }],\n",
    "            ids=[doc['id']]\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úì {i+1}/{len(KNOWLEDGE_BASE)}: {doc['title']}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Knowledge base loaded: {collection.count()} documents\")\n",
    "\n",
    "# Load it\n",
    "await load_knowledge_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22ecad",
   "metadata": {},
   "source": [
    "## Part 6: Querying the Vector Store\n",
    "\n",
    "Now for the magic: semantic search!\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. User has a tender (e.g., \"AI cybersecurity project\")\n",
    "2. We embed the tender description\n",
    "3. ChromaDB finds documents with similar embeddings\n",
    "4. We get back the most relevant examples\n",
    "\n",
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cada32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'We need to implement AI-based threat detection for government security.'\n",
      "\n",
      "Top 2 matches:\n",
      "\n",
      "1. AI-Powered Threat Detection Implementation\n",
      "   Category: cybersecurity\n",
      "   Similarity: 0.631\n",
      "   Content preview: ## Executive Summary\n",
      "\n",
      "Our proposed solution delivers enterprise-grade threat detection leveraging machine learning algorithms specifically tuned for g...\n",
      "\n",
      "2. Predictive Maintenance AI Platform\n",
      "   Category: ai\n",
      "   Similarity: 0.235\n",
      "   Content preview: ## Solution Overview\n",
      "\n",
      "Our predictive maintenance platform reduces equipment downtime by 40% through advanced AI-driven failure prediction. The system ...\n"
     ]
    }
   ],
   "source": [
    "async def search_knowledge_base(query: str, n_results: int = 2):\n",
    "    \"\"\"Search for relevant documents\"\"\"\n",
    "    \n",
    "    # Create embedding for query\n",
    "    query_embedding = await embedding_service.create_embedding(query)\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test Query 1: Cybersecurity\n",
    "query1 = \"We need to implement AI-based threat detection for government security.\"\n",
    "print(f\"üîç Query: '{query1}'\\n\")\n",
    "\n",
    "results1 = await search_knowledge_base(query1, n_results=2)\n",
    "\n",
    "print(\"Top 2 matches:\")\n",
    "for i, (doc_id, metadata, distance) in enumerate(zip(\n",
    "    results1['ids'][0],\n",
    "    results1['metadatas'][0],\n",
    "    results1['distances'][0]\n",
    ")):\n",
    "    print(f\"\\n{i+1}. {metadata['title']}\")\n",
    "    print(f\"   Category: {metadata['category']}\")\n",
    "    print(f\"   Similarity: {1 - distance:.3f}\")\n",
    "    print(f\"   Content preview: {results1['documents'][0][i][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d607f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'Custom CRM application development with cloud hosting'\n",
      "\n",
      "Top 2 matches:\n",
      "\n",
      "1. Custom CRM System Development\n",
      "   Category: software\n",
      "   Similarity: 0.446\n",
      "\n",
      "2. Cloud Migration and Infrastructure Modernization\n",
      "   Category: cloud\n",
      "   Similarity: 0.157\n"
     ]
    }
   ],
   "source": [
    "# Test Query 2: Software Development\n",
    "query2 = \"Custom CRM application development with cloud hosting\"\n",
    "print(f\"üîç Query: '{query2}'\\n\")\n",
    "\n",
    "results2 = await search_knowledge_base(query2, n_results=2)\n",
    "\n",
    "print(\"Top 2 matches:\")\n",
    "for i, (metadata, distance) in enumerate(zip(\n",
    "    results2['metadatas'][0],\n",
    "    results2['distances'][0]\n",
    ")):\n",
    "    print(f\"\\n{i+1}. {metadata['title']}\")\n",
    "    print(f\"   Category: {metadata['category']}\")\n",
    "    print(f\"   Similarity: {1 - distance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6d624",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Notice how:\n",
    "- Cybersecurity query ‚Üí Cybersecurity document has highest similarity\n",
    "- CRM query ‚Üí Software/Cloud documents rank highest\n",
    "- The model understands semantic relationships (not just keyword matching)\n",
    "\n",
    "This is the power of embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9ba97",
   "metadata": {},
   "source": [
    "## Part 7: RAG-Enhanced Document Generation\n",
    "\n",
    "Now let's put it all together: use retrieved examples to improve document generation.\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```python\n",
    "def generate_with_rag(tender):\n",
    "    # 1. Retrieve relevant examples\n",
    "    examples = search_knowledge_base(tender.description)\n",
    "    \n",
    "    # 2. Augment prompt\n",
    "    prompt = f\"\"\"\n",
    "    Here are examples of excellent bids:\n",
    "    {examples}\n",
    "    \n",
    "    Now generate a bid for:\n",
    "    {tender}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Generate\n",
    "    return llm.generate(prompt)\n",
    "```\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36b3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG generation pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "from procurement_ai.services.llm import LLMService\n",
    "\n",
    "llm = LLMService(config)\n",
    "\n",
    "async def generate_without_rag(tender_description: str) -> str:\n",
    "    \"\"\"Baseline: Generate without RAG\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Generate a professional bid document for this tender:\n",
    "\n",
    "{tender_description}\n",
    "\n",
    "Include:\n",
    "- Executive summary\n",
    "- Technical approach\n",
    "- Timeline\n",
    "- Team qualifications\n",
    "\n",
    "Keep it concise (300-400 words).\"\"\"\n",
    "    \n",
    "    response = await llm.generate(\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "async def generate_with_rag(tender_description: str, n_examples: int = 2) -> str:\n",
    "    \"\"\"Enhanced: Generate with RAG\"\"\"\n",
    "    \n",
    "    # 1. Retrieve relevant examples\n",
    "    results = await search_knowledge_base(tender_description, n_results=n_examples)\n",
    "    \n",
    "    # 2. Format examples\n",
    "    examples_text = \"\"\n",
    "    for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "        examples_text += f\"\\n### Example {i+1}: {meta['title']}\\n{doc}\\n\"\n",
    "    \n",
    "    # 3. Augmented prompt\n",
    "    prompt = f\"\"\"You are writing a bid document. First, study these examples of excellent bids:\n",
    "\n",
    "---\n",
    "{examples_text}\n",
    "---\n",
    "\n",
    "Now, using the same level of professionalism and detail as the examples above, generate a bid for:\n",
    "\n",
    "{tender_description}\n",
    "\n",
    "Include:\n",
    "- Executive summary\n",
    "- Technical approach\n",
    "- Timeline\n",
    "- Team qualifications\n",
    "\n",
    "Match the quality and structure of the example documents. Keep it concise (300-400 words).\"\"\"\n",
    "    \n",
    "    response = await llm.generate(\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ RAG generation pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389fa2e",
   "metadata": {},
   "source": [
    "## Part 8: Comparing Results (Before vs. After RAG)\n",
    "\n",
    "Let's test both approaches and compare quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499b258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Test Tender:\n",
      "Healthcare AI Advisory System\n",
      "\n",
      "National Health Service requires an AI-powered system to provide real-time clinical decision support. \n",
      "The system should analyze patient data, suggest diagnoses, and recommend treatment plans while \n",
      "maintaining full compliance with medical privacy regulations.\n",
      "\n",
      "Budget: ‚Ç¨1.5M\n",
      "Timeline: 12 months\n",
      "Requirements: ISO 27001, HIPAA compliance\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test tender\n",
    "test_tender = \"\"\"Healthcare AI Advisory System\n",
    "\n",
    "National Health Service requires an AI-powered system to provide real-time clinical decision support. \n",
    "The system should analyze patient data, suggest diagnoses, and recommend treatment plans while \n",
    "maintaining full compliance with medical privacy regulations.\n",
    "\n",
    "Budget: ‚Ç¨1.5M\n",
    "Timeline: 12 months\n",
    "Requirements: ISO 27001, HIPAA compliance\"\"\"\n",
    "\n",
    "print(\"üè• Test Tender:\")\n",
    "print(test_tender)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170df110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Generating WITHOUT RAG...\n",
      "\n",
      "RESULT (No RAG):\n",
      "================================================================================\n",
      "**Bid for Healthcare AI Advisory System ‚Äì National Health Service**\n",
      "\n",
      "---\n",
      "\n",
      "### Executive Summary  \n",
      "We propose a fully‚Äëintegrated, AI‚Äëpowered Clinical Decision Support System (CDSS) that delivers real‚Äëtime diagnostic insights and treatment recommendations while safeguarding patient confidentiality. Leveraging state‚Äëof‚Äëthe‚Äëart natural‚Äëlanguage processing, federated learning and explainable AI, the solution will be deployed within 12 months at a cost of ‚Ç¨1.5‚ÄØM. Our architecture is ISO‚ÄØ27001‚Äëcertified and meets UK HIPAA equivalents, ensuring data integrity, auditability and compliance with the NHS Digital Data Security & Protection Toolkit.\n",
      "\n",
      "---\n",
      "\n",
      "### Technical Approach  \n",
      "1. **Data Layer** ‚Äì Secure, encrypted storage on NHS‚Äëapproved cloud infrastructure with role‚Äëbased access control. Federated learning pipelines aggregate anonymised signals from multiple sites without moving raw data.  \n",
      "2. **AI Engine** ‚Äì Transformer‚Äëbased models fine‚Äëtuned on NHS datasets, coupled with probabilistic disease‚Äëprediction modules. Confidence scores and evidence trails are generated for every recommendation to satisfy clinical audit requirements.  \n",
      "3. **Integration** ‚Äì RESTful APIs and HL7‚ÄØFHIR adapters embed the CDSS into existing Electronic Health Record (EHR) workflows. Real‚Äëtime alerts are pushed to clinicians via the NHS portal or mobile device.  \n",
      "4. **Governance & Compliance** ‚Äì End‚Äëto‚Äëend encryption, data minimisation, and continuous vulnerability scanning. Regular penetration tests and ISO‚ÄØ27001 audit cycles are built into the delivery plan.\n",
      "\n",
      "---\n",
      "\n",
      "### Timeline (12 months)  \n",
      "\n",
      "| Phase | Months | Deliverables |\n",
      "|-------|--------|--------------|\n",
      "| 1. Discovery & Architecture | 0‚Äë2 | Requirements workshop, data inventory, compliance audit |\n",
      "| 2. Prototype & Model Training | 3‚Äë5 | MVP CDSS, initial model performance metrics |\n",
      "| 3. Integration & Pilot | 6‚Äë8 | API connectors, pilot rollout in two NHS sites |\n",
      "| 4. Validation & Optimization | 9‚Äë10 | Clinical validation studies, model refinement |\n",
      "| 5. Deployment & Training | 11 | Full rollout, user training, knowledge transfer |\n",
      "| 6. Handover & Post‚ÄëGo‚ÄëLive | 12 | Final audit, sign‚Äëoff, support handover |\n",
      "\n",
      "---\n",
      "\n",
      "### Team Qualifications  \n",
      "- **Project Lead ‚Äì Dr. Elena Martinez** (\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate WITHOUT RAG\n",
    "print(\"üìÑ Generating WITHOUT RAG...\\n\")\n",
    "doc_without_rag = await generate_without_rag(test_tender)\n",
    "\n",
    "print(\"RESULT (No RAG):\")\n",
    "print(\"=\"*80)\n",
    "print(doc_without_rag)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b969c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Generating WITH RAG...\n",
      "\n",
      "RESULT (With RAG):\n",
      "================================================================================\n",
      "**Executive Summary**\n",
      "\n",
      "We propose a fully‚Äëintegrated, AI‚Äëpowered Clinical Decision Support System (CDSS) for the National Health Service that delivers real‚Äëtime diagnostic insights and personalized treatment recommendations while meeting stringent ISO‚ÄØ27001 and HIPAA standards. Leveraging federated learning, secure multi‚Äëparty computation, and explainable AI, the platform will ingest EMR data, laboratory results, imaging metadata, and clinician notes to produce evidence‚Äëbased suggestions that can be reviewed, overridden, or accepted by clinicians. Pilot studies in two NHS trusts have shown a 22‚ÄØ% increase in diagnostic accuracy and a 15‚ÄØ% reduction in treatment cycle times, with no compromise on patient privacy. The ‚Ç¨1.5‚ÄØM investment covers system design, implementation, staff training, and a 12‚Äëmonth post‚Äëdeployment support window.\n",
      "\n",
      "**Technical Approach**\n",
      "\n",
      "1. **Data Ingestion & Governance**  \n",
      "   ‚Ä¢ Secure API gateways (FHIR‚Äëbased) aggregate structured and unstructured data from EMR, PACS, and laboratory systems.  \n",
      "   ‚Ä¢ Data is anonymized on‚Äëpremises using tokenization and differential privacy before feeding the ML pipeline, preserving HIPAA ‚Äúminimum necessary‚Äù principles.\n",
      "\n",
      "2. **Model Architecture**  \n",
      "   ‚Ä¢ Multi‚Äëmodal transformer models fuse structured vitals, lab values, imaging embeddings, and NLP‚Äëextracted clinical notes.  \n",
      "   ‚Ä¢ Federated learning across NHS sites trains a global model without moving patient data, ensuring compliance and reducing bandwidth costs.  \n",
      "   ‚Ä¢ Explainability modules (SHAP, LIME) provide transparent rationale for each recommendation.\n",
      "\n",
      "3. **Inference & Alerting**  \n",
      "   ‚Ä¢ Real‚Äëtime inference engine (ONNX runtime) delivers diagnostics within 2‚ÄØseconds of data entry.  \n",
      "   ‚Ä¢ Clinical dashboards and EMR widgets present ranked differential diagnoses, risk scores, and evidence‚Äëbased treatment pathways.  \n",
      "   ‚Ä¢ Configurable alert thresholds trigger physician notifications via secure messaging or EMR alerts.\n",
      "\n",
      "4. **Security & Compliance**  \n",
      "   ‚Ä¢ End‚Äëto‚Äëend encryption (TLS‚ÄØ1.3, AES‚Äë256) and role‚Äëbased access control enforce ISO‚ÄØ27001 controls.  \n",
      "   ‚Ä¢ Regular penetration testing, vulnerability scanning, and automated compliance reporting satisfy HIPAA Security Rule requirements.\n",
      "\n",
      "**Timeline (12‚ÄØMonths)**\n",
      "\n",
      "| Phase | Duration | Milestones |\n",
      "|-------|----------|------------|\n",
      "| 1 ‚Äì Architecture\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate WITH RAG\n",
    "print(\"üìÑ Generating WITH RAG...\\n\")\n",
    "doc_with_rag = await generate_with_rag(test_tender)\n",
    "\n",
    "print(\"RESULT (With RAG):\")\n",
    "print(\"=\"*80)\n",
    "print(doc_with_rag)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1041c1",
   "metadata": {},
   "source": [
    "### Quality Comparison\n",
    "\n",
    "Look for these improvements in the RAG version:\n",
    "\n",
    "**Structure**:\n",
    "- ‚ùì Does it follow the example format?\n",
    "- ‚ùì Are sections better organized?\n",
    "\n",
    "**Technical Detail**:\n",
    "- ‚ùì More specific technical approaches?\n",
    "- ‚ùì Concrete numbers and metrics?\n",
    "\n",
    "**Professionalism**:\n",
    "- ‚ùì Sounds more like a real bid?\n",
    "- ‚ùì Addresses compliance explicitly?\n",
    "\n",
    "**Persuasiveness**:\n",
    "- ‚ùì Mentions relevant experience?\n",
    "- ‚ùì Provides reassurance on key concerns?\n",
    "\n",
    "In most cases, RAG versions show significant quality improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b48c0",
   "metadata": {},
   "source": [
    "## Part 9: Measuring RAG Impact\n",
    "\n",
    "We can't just eyeball quality. Let's measure improvement objectively.\n",
    "\n",
    "### Metrics to Track\n",
    "\n",
    "1. **Retrieval Quality**: Are we finding relevant examples?\n",
    "   - Measure: Average similarity score of retrieved docs\n",
    "   - Good: >0.75 similarity\n",
    "\n",
    "2. **Generation Quality**: Is the output better?\n",
    "   - Use LLM-as-judge to rate documents on 1-10 scale\n",
    "   - Compare: with-RAG vs. without-RAG\n",
    "\n",
    "3. **Relevance**: Does generated content incorporate retrieved examples?\n",
    "   - Check for similar terminology/structure\n",
    "\n",
    "Let's implement basic measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be5fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running RAG evaluation on 3 test cases...\n",
      "\n",
      "Test 1/3: AI-powered cybersecurity threat detection for government sys...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'search_knowledge_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     44\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  ‚ö†Ô∏è  Retrieval quality needs improvement\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m eval_results = \u001b[38;5;28;01mawait\u001b[39;00m evaluate_rag_quality()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mevaluate_rag_quality\u001b[39m\u001b[34m(n_tests)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tests\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_case[:\u001b[32m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Check retrieval quality\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m search_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43msearch_knowledge_base\u001b[49m(test_case, n_results=\u001b[32m2\u001b[39m)\n\u001b[32m     23\u001b[39m avg_similarity = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m - d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m search_results[\u001b[33m'\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]) / \u001b[38;5;28mlen\u001b[39m(search_results[\u001b[33m'\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m     25\u001b[39m results.append({\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcase\u001b[39m\u001b[33m'\u001b[39m: test_case,\n\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mretrieval_similarity\u001b[39m\u001b[33m'\u001b[39m: avg_similarity,\n\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mretrieved_categories\u001b[39m\u001b[33m'\u001b[39m: [m[\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m search_results[\u001b[33m'\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]]\n\u001b[32m     29\u001b[39m })\n",
      "\u001b[31mNameError\u001b[39m: name 'search_knowledge_base' is not defined"
     ]
    }
   ],
   "source": [
    "async def evaluate_rag_quality(n_tests: int = 3):\n",
    "    \"\"\"Evaluate RAG improvements\n",
    "    \n",
    "    Note: Uses predefined test cases internally.\n",
    "    Fixed: Removed unused 'tender_description' parameter that was causing TypeError.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üî¨ Running RAG evaluation on {n_tests} test cases...\\n\")\n",
    "    \n",
    "    test_cases = [\n",
    "        \"AI-powered cybersecurity threat detection for government systems\",\n",
    "        \"Custom CRM software development with cloud hosting\",\n",
    "        \"Predictive maintenance AI platform for industrial equipment\"\n",
    "    ][:n_tests]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"Test {i}/{n_tests}: {test_case[:60]}...\")\n",
    "        \n",
    "        # Check retrieval quality\n",
    "        search_results = await search_knowledge_base(test_case, n_results=2)\n",
    "        avg_similarity = sum(1 - d for d in search_results['distances'][0]) / len(search_results['distances'][0])\n",
    "        \n",
    "        results.append({\n",
    "            'case': test_case,\n",
    "            'retrieval_similarity': avg_similarity,\n",
    "            'retrieved_categories': [m['category'] for m in search_results['metadatas'][0]]\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚Üí Retrieval similarity: {avg_similarity:.3f}\")\n",
    "        print(f\"  ‚Üí Retrieved: {', '.join(results[-1]['retrieved_categories'])}\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    avg_ret = sum(r['retrieval_similarity'] for r in results) / len(results)\n",
    "    \n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(f\"  Average retrieval similarity: {avg_ret:.3f}\")\n",
    "    if avg_ret > 0.75:\n",
    "        print(\"  ‚úÖ Excellent retrieval quality!\")\n",
    "    elif avg_ret > 0.65:\n",
    "        print(\"  ‚úì Good retrieval quality\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Retrieval quality needs improvement\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad55627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "eval_results = await evaluate_rag_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2c8ab",
   "metadata": {},
   "source": [
    "## Part 10: Advanced RAG Techniques (Optional)\n",
    "\n",
    "Our basic RAG is working! Here are enhancements for production:\n",
    "\n",
    "### 1. Metadata Filtering\n",
    "\n",
    "Retrieve only from specific categories:\n",
    "\n",
    "```python\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3,\n",
    "    where={\"category\": \"cybersecurity\"}  # Filter!\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Hybrid Search\n",
    "\n",
    "Combine semantic search with keyword matching:\n",
    "- Semantic: Finds conceptually similar docs\n",
    "- Keyword: Ensures specific terms are present\n",
    "\n",
    "### 3. Re-ranking\n",
    "\n",
    "After retrieval, re-rank by:\n",
    "- Recency (prefer newer examples)\n",
    "- Success rate (prefer proven approaches)\n",
    "- Specificity (prefer detailed over generic)\n",
    "\n",
    "### 4. Dynamic k\n",
    "\n",
    "Retrieve variable numbers of examples:\n",
    "- Complex tender ‚Üí More examples (k=3-4)\n",
    "- Simple tender ‚Üí Fewer examples (k=1-2)\n",
    "\n",
    "### 5. Chunk Optimization\n",
    "\n",
    "Instead of whole documents:\n",
    "- Split into sections (executive summary, technical approach, etc.)\n",
    "- Retrieve most relevant sections from different documents\n",
    "- Mix and match best parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc98f06",
   "metadata": {},
   "source": [
    "## Summary: What We Learned\n",
    "\n",
    "### RAG Workflow\n",
    "\n",
    "1. **Build Knowledge Base**: Collect high-quality example documents\n",
    "2. **Create Embeddings**: Convert text to vectors using embedding models\n",
    "3. **Store in Vector DB**: Use ChromaDB for efficient similarity search\n",
    "4. **Retrieve**: Find relevant examples for each new request\n",
    "5. **Augment**: Add examples to LLM prompt\n",
    "6. **Generate**: LLM produces better output informed by examples\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "‚úÖ **Quality Improvement**: Documents match professional standards  \n",
    "‚úÖ **Domain Adaptation**: LLM learns your specific patterns  \n",
    "‚úÖ **Consistency**: Examples ensure similar structure/tone  \n",
    "‚úÖ **Reduced Hallucination**: Grounded in real examples  \n",
    "\n",
    "### When to Use RAG\n",
    "\n",
    "RAG is powerful when:\n",
    "- You have high-quality example documents\n",
    "- Output quality matters (not just correctness)\n",
    "- Domain-specific knowledge is needed\n",
    "- You want to incorporate institutional knowledge\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand RAG, we'll:\n",
    "1. Integrate it into the DocumentGenerator agent\n",
    "2. Build a knowledge base management system\n",
    "3. Add evaluation to measure RAG impact\n",
    "4. Deploy to production\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Try these to deepen your understanding:\n",
    "\n",
    "1. **Add more examples**: Create 2-3 more knowledge base entries in different categories\n",
    "\n",
    "2. **Test edge cases**: What happens when you query about a category not in the KB?\n",
    "\n",
    "3. **Experiment with k**: Try n_results=1, 2, 3, 4. How does it affect output?\n",
    "\n",
    "4. **Metadata filtering**: Modify search to only retrieve from one category\n",
    "\n",
    "5. **Quality comparison**: Generate 3 documents (with/without RAG) and compare side-by-side\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrated RAG fundamentals. In the next phase, we'll build production-ready RAG infrastructure for the procurement system.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
