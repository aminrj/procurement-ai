{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e22fef",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "We only need `httpx` for making HTTP requests to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645b2b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from anyio->httpx) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bb50b",
   "metadata": {},
   "source": [
    "## Step 2: Your First LLM Call\n",
    "\n",
    "LM Studio provides an OpenAI-compatible API. That means:\n",
    "- Endpoint: `http://localhost:1234/v1/chat/completions`\n",
    "- Format: Same as OpenAI's API\n",
    "- Local: Runs on your machine, no API keys needed!\n",
    "\n",
    "Let's make a simple call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413eae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! üëã Hope you're having a fantastic day!\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "\n",
    "# API configuration\n",
    "BASE_URL = \"http://localhost:1234/v1\"\n",
    "MODEL = \"local-model\"  # LM Studio uses this generic name\n",
    "\n",
    "# Make a simple request\n",
    "async def call_llm(prompt: str) -> str:\n",
    "    \"\"\"Call LM Studio with a simple prompt\"\"\"\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{BASE_URL}/chat/completions\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 100,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Test it!\n",
    "response = await call_llm(\"Say hello in a friendly way!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299af9b",
   "metadata": {},
   "source": [
    "## Step 3: Understand the Response\n",
    "\n",
    "The LLM returns a JSON response. Let's look at the full structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab4861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-ea2zckudyqj4otlkn0tlta\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1769779895,\n",
      "  \"model\": \"openai/gpt-oss-20b\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The answer is **4**.\",\n",
      "        \"reasoning\": \"Simple math: 4.\",\n",
      "        \"tool_calls\": []\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 74,\n",
      "    \"completion_tokens\": 23,\n",
      "    \"total_tokens\": 97\n",
      "  },\n",
      "  \"stats\": {},\n",
      "  \"system_fingerprint\": \"openai/gpt-oss-20b\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "async def call_llm_full(prompt: str) -> dict:\n",
    "    \"\"\"Call LLM and return full response\"\"\"\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{BASE_URL}/chat/completions\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0.7,\n",
    "            },\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "# See the full structure\n",
    "full_response = await call_llm_full(\"What is 2+2?\")\n",
    "print(json.dumps(full_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aab76b",
   "metadata": {},
   "source": [
    "## Step 4: Real Use Case - Classify a Tender\n",
    "\n",
    "Now let's use the LLM for something useful: determining if a procurement tender is relevant to our tech company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd25b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: AI Cybersecurity Tender\n",
      "==================================================\n",
      "RELEVANT ‚Äì The tender seeks an AI‚Äëdriven system for real‚Äëtime threat detection, directly aligning with the company‚Äôs expertise in AI, cybersecurity, and software development.\n",
      "\n",
      "\n",
      "Example 2: Office Furniture\n",
      "==================================================\n",
      "NOT RELEVANT ‚Äì the tender is for ergonomic office furniture, which does not involve AI, cybersecurity, or software development services that the company specializes in.\n"
     ]
    }
   ],
   "source": [
    "async def classify_tender(title: str, description: str) -> str:\n",
    "    \"\"\"Use LLM to classify if a tender is relevant\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a procurement analyst. Is this tender relevant for a tech company\n",
    "specializing in AI, cybersecurity, and software development?\n",
    "\n",
    "TENDER TITLE: {title}\n",
    "\n",
    "DESCRIPTION: {description}\n",
    "\n",
    "Answer with \"RELEVANT\" or \"NOT RELEVANT\" and explain why in one sentence.\n",
    "\"\"\"\n",
    "    \n",
    "    return await call_llm(prompt)\n",
    "\n",
    "# Test with real examples\n",
    "print(\"Example 1: AI Cybersecurity Tender\")\n",
    "print(\"=\" * 50)\n",
    "result1 = await classify_tender(\n",
    "    title=\"AI-Powered Threat Detection System\",\n",
    "    description=\"Government needs AI system to detect cybersecurity threats in real-time.\"\n",
    ")\n",
    "print(result1)\n",
    "\n",
    "print(\"\\n\\nExample 2: Office Furniture\")\n",
    "print(\"=\" * 50)\n",
    "result2 = await classify_tender(\n",
    "    title=\"Office Furniture Supply\",\n",
    "    description=\"Supply 500 ergonomic chairs and desks for government building.\"\n",
    ")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93071a",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You just:\n",
    "1. Made your first LLM API call\n",
    "2. Understood the response format\n",
    "3. Built a real tender classifier\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **LLMs are just HTTP APIs** - You send text, get text back\n",
    "- **Temperature controls randomness** - Lower = more consistent\n",
    "- **Prompts are instructions** - Clear prompts = better results\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll learn about **structured outputs** using Pydantic to get JSON instead of free text. This is crucial for building reliable AI systems!\n",
    "\n",
    "‚û°Ô∏è Continue to `02_structured_outputs.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
