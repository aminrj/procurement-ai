{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85787c0a",
   "metadata": {},
   "source": [
    "# 08: Evaluating AI Agents\n",
    "\n",
    "**Duration:** 90 minutes\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Why evaluation is critical for AI systems\n",
    "- Building gold standard test sets\n",
    "- Key metrics for classification and regression\n",
    "- Confidence calibration analysis\n",
    "- Running systematic evaluations\n",
    "- A/B testing prompts scientifically\n",
    "- Tracking improvements over time\n",
    "\n",
    "**The Reality:**\n",
    "Without evaluation, you're flying blind. You can't tell if your changes improve the system, break it, or do nothing. Professional AI development requires systematic measurement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80968787",
   "metadata": {},
   "source": [
    "## Why Evaluate?\n",
    "\n",
    "### The Problem: Subjective Assessment\n",
    "\n",
    "When we built our agents, we looked at a few outputs and thought \"that looks good!\" But:\n",
    "\n",
    "- **How good?** 85% accurate? 95%? We don't know.\n",
    "- **Which agent is weakest?** Filter? Rating? Generator?\n",
    "- **Did that prompt change help?** Feels better, but did it really improve outcomes?\n",
    "- **Will it break in production?** What about edge cases we haven't seen?\n",
    "\n",
    "### The Solution: Quantitative Metrics\n",
    "\n",
    "Professional AI development follows this cycle:\n",
    "\n",
    "```\n",
    "1. Measure baseline   ‚Üí Know where you are\n",
    "2. Make a change      ‚Üí Hypothesis: this will improve X\n",
    "3. Measure again      ‚Üí Did X actually improve?\n",
    "4. Keep or revert     ‚Üí Data-driven decision\n",
    "```\n",
    "\n",
    "This is called **evaluation-driven development** and it's how production AI systems are built.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f16118",
   "metadata": {},
   "source": [
    "### Real Example: The Prompt That Made Things Worse\n",
    "\n",
    "Imagine we change our filter prompt to be \"more friendly\":\n",
    "\n",
    "**Old:** \"Analyze this tender. Is it relevant for cybersecurity, AI, or software development?\"\n",
    "\n",
    "**New:** \"Hey! Check out this tender. Does it seem like something we'd be interested in? Like, does it involve AI stuff or security things?\"\n",
    "\n",
    "**Subjective assessment:** \"The new one feels more approachable!\"\n",
    "\n",
    "**Objective measurement:**\n",
    "- Old precision: 0.92\n",
    "- New precision: 0.73  ‚ùå\n",
    "\n",
    "The friendly prompt introduced ambiguity and dropped accuracy by 20%. Without metrics, we would have shipped a worse system.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb82a77",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Evaluation Metrics\n",
    "\n",
    "Let's learn the key metrics by working through examples.\n",
    "\n",
    "### Binary Classification Metrics (Filter Agent)\n",
    "\n",
    "Our filter agent makes YES/NO decisions. Four outcomes are possible:\n",
    "\n",
    "```\n",
    "                    Predicted YES  |  Predicted NO\n",
    "Actually YES             TP        |       FN\n",
    "Actually NO              FP        |       TN\n",
    "```\n",
    "\n",
    "- **TP (True Positive):** Correctly identified relevant tender\n",
    "- **FP (False Positive):** Incorrectly said irrelevant tender was relevant\n",
    "- **TN (True Negative):** Correctly rejected irrelevant tender\n",
    "- **FN (False Negative):** Missed a relevant tender\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import asyncio\n",
    "from procurement_ai.models import Tender\n",
    "from procurement_ai.agents.filter import FilterAgent\n",
    "from procurement_ai.services.llm import LLMService\n",
    "from procurement_ai.config import Config\n",
    "\n",
    "# Initialize\n",
    "config = Config()\n",
    "llm = LLMService(config)\n",
    "filter_agent = FilterAgent(llm, config)\n",
    "\n",
    "print(\"Evaluation framework initialized!\")\n",
    "print(f\"Using model: {config.LLM_MODEL}\")\n",
    "print(f\"Temperature: {config.TEMPERATURE_PRECISE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbea13",
   "metadata": {},
   "source": [
    "### Example: Calculating Metrics by Hand\n",
    "\n",
    "Let's say we test our filter agent on 10 tenders:\n",
    "\n",
    "| Test Case | Actual | Predicted | Result |\n",
    "|---|---|---|---|\n",
    "| AI Security Project | ‚úÖ Relevant | ‚úÖ Relevant | TP |\n",
    "| Office Furniture | ‚ùå Irrelevant | ‚ùå Irrelevant | TN |\n",
    "| Software Dev | ‚úÖ Relevant | ‚úÖ Relevant | TP |\n",
    "| Construction | ‚ùå Irrelevant | ‚ùå Irrelevant | TN |\n",
    "| ML Platform | ‚úÖ Relevant | ‚úÖ Relevant | TP |\n",
    "| Catering | ‚ùå Irrelevant | ‚ùå Irrelevant | TN |\n",
    "| Custom ERP | ‚úÖ Relevant | ‚ùå Irrelevant | **FN** ‚ö†Ô∏è|\n",
    "| Vehicle Fleet | ‚ùå Irrelevant | ‚ùå Irrelevant | TN |\n",
    "| Network Hardware | ‚ùå Irrelevant | ‚úÖ Relevant | **FP** ‚ö†Ô∏è|\n",
    "| Cybersecurity Audit | ‚úÖ Relevant | ‚úÖ Relevant | TP |\n",
    "\n",
    "**Counts:** TP=4, FP=1, TN=4, FN=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics from confusion matrix\n",
    "TP = 4\n",
    "FP = 1\n",
    "TN = 4\n",
    "FN = 1\n",
    "\n",
    "# Precision: Of all we said YES to, how many were correct?\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(\"  ‚Üí When we say 'relevant', we're right 80% of the time\")\n",
    "print()\n",
    "\n",
    "# Recall: Of all actual YES cases, how many did we find?\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(\"  ‚Üí We found 80% of all relevant tenders\")\n",
    "print()\n",
    "\n",
    "# F1: Harmonic mean (balances precision and recall)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1 Score: {f1:.2%}\")\n",
    "print(\"  ‚Üí Overall filter quality metric\")\n",
    "print()\n",
    "\n",
    "# Accuracy: Overall correctness\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(\"  ‚Üí We're correct 80% of the time overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974d28d",
   "metadata": {},
   "source": [
    "### Which Metric Matters?\n",
    "\n",
    "**Depends on your business goal:**\n",
    "\n",
    "**High Precision (minimize FP):**\n",
    "- Use case: \"Don't waste time on irrelevant tenders\"\n",
    "- Prefer: Miss some opportunities, but every one we bid on is legitimate\n",
    "- Example: Small team with limited capacity\n",
    "\n",
    "**High Recall (minimize FN):**\n",
    "- Use case: \"Don't miss any opportunity\"\n",
    "- Prefer: Look at some irrelevant tenders, but catch every real one\n",
    "- Example: Large team, can afford to review more\n",
    "\n",
    "**F1 Score:**\n",
    "- Balanced approach\n",
    "- Good default metric\n",
    "- What we'll optimize for\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08b79e",
   "metadata": {},
   "source": [
    "## Part 2: Building a Gold Standard Test Set\n",
    "\n",
    "The foundation of evaluation is a **gold standard test set**: cases where we know the correct answer.\n",
    "\n",
    "### Characteristics of a Good Test Set\n",
    "\n",
    "1. **Diverse:** Cover all scenarios (easy, hard, edge cases)\n",
    "2. **Balanced:** Mix of positive and negative examples\n",
    "3. **Representative:** Reflects real-world distribution\n",
    "4. **Documented:** Clear reasoning for each label\n",
    "5. **Stable:** Don't change labels frequently\n",
    "\n",
    "### Our Test Set Structure\n",
    "\n",
    "We've created 18 carefully designed test cases:\n",
    "\n",
    "- 4 **Clear Relevant:** Obvious matches (build confidence)\n",
    "- 4 **Clear Irrelevant:** Obvious rejections (test specificity)\n",
    "- 5 **Edge Cases:** Tricky scenarios (find weaknesses)\n",
    "- 3 **Rating Validation:** Test scoring accuracy\n",
    "- 2 **Category Tests:** Challenge category detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our evaluation dataset\n",
    "from tests.fixtures.evaluation_dataset import (\n",
    "    ALL_TEST_CASES,\n",
    "    DATASET_STATS,\n",
    "    get_test_cases_by_category,\n",
    "    TestCaseCategory\n",
    ")\n",
    "\n",
    "# Show dataset statistics\n",
    "print(\"üìä Evaluation Dataset Statistics\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in DATASET_STATS.items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\n\\nüìù Example Test Cases:\\n\")\n",
    "\n",
    "# Show one from each category\n",
    "categories = [\n",
    "    TestCaseCategory.CLEAR_RELEVANT,\n",
    "    TestCaseCategory.CLEAR_IRRELEVANT,\n",
    "    TestCaseCategory.EDGE_CASE\n",
    "]\n",
    "\n",
    "for cat in categories:\n",
    "    cases = get_test_cases_by_category(cat)\n",
    "    if cases:\n",
    "        tc = cases[0]\n",
    "        print(f\"\\n{cat.value.upper().replace('_', ' ')}\")\n",
    "        print(f\"  ID: {tc.tender_id}\")\n",
    "        print(f\"  Title: {tc.title}\")\n",
    "        print(f\"  Expected: {'Relevant' if tc.expected_relevance else 'Irrelevant'}\")\n",
    "        print(f\"  Note: {tc.notes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e4281",
   "metadata": {},
   "source": [
    "### Example: An Edge Case\n",
    "\n",
    "Edge cases are where agents fail. Here's a tricky one:\n",
    "\n",
    "**\"Network Infrastructure Upgrade with Management Software\"**\n",
    "\n",
    "- 80% hardware procurement (switches, routers)\n",
    "- 20% basic software configuration\n",
    "\n",
    "**Why it's tricky:**\n",
    "- Keywords like \"network\" and \"infrastructure\" might trigger false positive\n",
    "- There IS software mentioned, but it's minimal\n",
    "- Tests if agent understands proportion and primary focus\n",
    "\n",
    "**Expected:** Irrelevant (hardware-dominant project)\n",
    "\n",
    "**What we learn:** Can our filter agent distinguish between projects that *mention* software vs projects that are *primarily* software?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a61d7b",
   "metadata": {},
   "source": [
    "## Part 3: Running Your First Evaluation\n",
    "\n",
    "Now let's run the evaluation framework on our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete evaluation\n",
    "from procurement_ai.evaluation import Evaluator, ConsoleReporter\n",
    "\n",
    "async def run_evaluation():\n",
    "    \"\"\"Run complete evaluation and show results\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Starting evaluation...\")\n",
    "    print(f\"   Testing {len(ALL_TEST_CASES)} cases\")\n",
    "    print(\"   This will take 2-3 minutes...\\n\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = Evaluator(config=config, llm_service=llm)\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = await evaluator.evaluate_dataset(\n",
    "        test_cases=ALL_TEST_CASES,\n",
    "        max_concurrent=3  # Process 3 at a time\n",
    "    )\n",
    "    \n",
    "    # Show results\n",
    "    ConsoleReporter.report(result, detailed=False)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run it!\n",
    "result = await run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a3a99",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "Let's break down what each metric tells us:\n",
    "\n",
    "#### Filter Agent Metrics\n",
    "\n",
    "**Precision: 0.XX**\n",
    "- If this is low (<80%): We're saying YES to too many irrelevant tenders\n",
    "- Action: Make filter criteria more strict\n",
    "\n",
    "**Recall: 0.XX**\n",
    "- If this is low (<80%): We're missing relevant opportunities\n",
    "- Action: Broaden filter criteria, reduce strictness\n",
    "\n",
    "**F1 Score: 0.XX**\n",
    "- Overall filter quality\n",
    "- Good: F1 > 0.85\n",
    "- Needs work: F1 < 0.75\n",
    "\n",
    "**Specificity: 0.XX**\n",
    "- How well we reject irrelevant tenders\n",
    "- Important for not wasting time\n",
    "\n",
    "#### Category Detection\n",
    "\n",
    "**Accuracy: 0.XX**\n",
    "- Are we correctly identifying cybersecurity vs AI vs software?\n",
    "- Important because rating agent uses these categories\n",
    "\n",
    "#### Rating Agent\n",
    "\n",
    "**MAE (Mean Absolute Error): X.XX**\n",
    "- Average error in our scores\n",
    "- Good: MAE < 1.0 (scores off by less than 1 point)\n",
    "- Needs work: MAE > 2.0\n",
    "\n",
    "**Correlation: 0.XX**\n",
    "- Do our scores track with expected scores?\n",
    "- Good: > 0.70\n",
    "- Excellent: > 0.85\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429da563",
   "metadata": {},
   "source": [
    "## Part 4: Confidence Calibration\n",
    "\n",
    "**Confidence calibration** measures: \"When the model says 90% confident, is it actually right 90% of the time?\"\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Imagine your filter agent says:\n",
    "- \"This tender is relevant (confidence: 0.95)\"\n",
    "\n",
    "If the agent is **well-calibrated**:\n",
    "- It's right 95% of the time when it says 0.95\n",
    "\n",
    "If the agent is **overconfident**:\n",
    "- It's only right 70% of the time when it says 0.95\n",
    "- This is dangerous! You trust predictions you shouldn't.\n",
    "\n",
    "If the agent is **underconfident**:\n",
    "- It's right 99% of the time when it says 0.95\n",
    "- Wasteful: you're double-checking predictions you can trust\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence calibration\n",
    "calibration = result.confidence_calibration\n",
    "\n",
    "print(\"üìä Confidence Calibration Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nExpected Calibration Error (ECE): {calibration.expected_calibration_error:.4f}\")\n",
    "print(\"  ‚Üí Lower is better. 0 = perfectly calibrated\")\n",
    "print()\n",
    "\n",
    "# Show calibration curve\n",
    "print(\"Calibration Curve:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Confidence Bin':<20} {'Accuracy':<15} {'Count':<10} {'Error'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for bin_data in calibration.get_calibration_curve():\n",
    "    conf = bin_data['mean_confidence']\n",
    "    acc = bin_data['accuracy']\n",
    "    count = bin_data['count']\n",
    "    error = bin_data['calibration_error']\n",
    "    \n",
    "    bar_length = int(acc * 20)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    \n",
    "    print(f\"{conf:.2f} confidence     {acc:.2%}  {bar:<20} {count:>3}     {error:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Perfect calibration: Confidence = Accuracy for each bin\")\n",
    "print(\"   - Overconfident: Confidence > Accuracy\")\n",
    "print(\"   - Underconfident: Confidence < Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343c81f",
   "metadata": {},
   "source": [
    "### Example: Poor Calibration\n",
    "\n",
    "```\n",
    "Confidence  | Accuracy | Interpretation\n",
    "------------|----------|----------------\n",
    "0.95        | 0.72     | ‚ùå Overconfident! Says 95% sure but only right 72% of time\n",
    "0.85        | 0.65     | ‚ùå Overconfident\n",
    "0.70        | 0.68     | ‚úÖ Well calibrated\n",
    "0.60        | 0.95     | ‚ö†Ô∏è  Underconfident (but less harmful)\n",
    "```\n",
    "\n",
    "**Action:** If your agent is overconfident, you need to either:\n",
    "1. Adjust confidence thresholds in your workflow\n",
    "2. Prompt the agent to be more conservative\n",
    "3. Add post-processing to recalibrate scores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34011b88",
   "metadata": {},
   "source": [
    "## Part 5: A/B Testing Prompts\n",
    "\n",
    "Now we can **scientifically test** if prompt changes improve performance!\n",
    "\n",
    "### Experiment: Does Adding Examples Help?\n",
    "\n",
    "Let's test two prompt variations:\n",
    "\n",
    "**Version A (Current):** Criteria-based\n",
    "```\n",
    "\"Analyze this tender:\n",
    "CRITERIA: Relevant if cybersecurity, AI, or software development.\n",
    "NOT relevant if hardware, construction, or non-technical.\"\n",
    "```\n",
    "\n",
    "**Version B (With Examples):** Few-shot learning\n",
    "```\n",
    "\"Analyze using these examples:\n",
    "RELEVANT: 'AI threat detection' ‚Üí YES (AI + Cybersecurity)\n",
    "RELEVANT: 'Custom CRM app' ‚Üí YES (Software Development)\n",
    "NOT RELEVANT: 'Office furniture' ‚Üí NO (Non-technical)\n",
    "\n",
    "Now analyze this tender: ...\"\n",
    "```\n",
    "\n",
    "**Hypothesis:** Version B will improve accuracy on edge cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ae8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test on edge cases specifically\n",
    "from tests.fixtures.evaluation_dataset import TestCaseCategory, get_test_cases_by_category\n",
    "\n",
    "edge_cases = get_test_cases_by_category(TestCaseCategory.EDGE_CASE)\n",
    "\n",
    "print(f\"üß™ Testing on {len(edge_cases)} edge cases\")\n",
    "print(\"   These are the tricky, ambiguous scenarios\\n\")\n",
    "\n",
    "# NOTE: To actually A/B test, you would:\n",
    "# 1. Modify filter.py to use Version B prompt\n",
    "# 2. Run evaluation again\n",
    "# 3. Compare results\n",
    "\n",
    "# For now, let's show how to compare results\n",
    "print(\"Example Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'Version A':<15} {'Version B':<15} {'Change'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Hypothetical results\n",
    "metrics = [\n",
    "    (\"F1 Score\", 0.82, 0.87, \"+6%\"),\n",
    "    (\"Precision\", 0.85, 0.88, \"+4%\"),\n",
    "    (\"Recall\", 0.79, 0.86, \"+9%\"),\n",
    "    (\"Edge Case Accuracy\", 0.60, 0.80, \"+33%\"),\n",
    "]\n",
    "\n",
    "for metric, v_a, v_b, change in metrics:\n",
    "    arrow = \"üìà\" if \"+\" in change else \"üìâ\"\n",
    "    print(f\"{metric:<25} {v_a:.2%}          {v_b:.2%}          {arrow} {change}\")\n",
    "\n",
    "print(\"\\nüí° Result: Version B with examples significantly improves edge case handling!\")\n",
    "print(\"   We should adopt Version B.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf64cd",
   "metadata": {},
   "source": [
    "### The Scientific Method for AI\n",
    "\n",
    "This is **evaluation-driven development** in action:\n",
    "\n",
    "```python\n",
    "# 1. Establish baseline\n",
    "baseline_result = await evaluator.evaluate_dataset(test_cases)\n",
    "baseline_f1 = baseline_result.filter_metrics.f1_score\n",
    "\n",
    "# 2. Make hypothesis\n",
    "# \"Adding examples to the prompt will improve F1 by 5%\"\n",
    "\n",
    "# 3. Implement change\n",
    "# (Modify filter.py with new prompt)\n",
    "\n",
    "# 4. Measure again\n",
    "new_result = await evaluator.evaluate_dataset(test_cases)\n",
    "new_f1 = new_result.filter_metrics.f1_score\n",
    "\n",
    "# 5. Decide based on data\n",
    "improvement = new_f1 - baseline_f1\n",
    "if improvement > 0.03:  # >3% improvement\n",
    "    print(\"‚úÖ Keep the change!\")\n",
    "else:\n",
    "    print(\"‚ùå Revert - not worth the complexity\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe58a3",
   "metadata": {},
   "source": [
    "## Part 6: Saving and Tracking Results\n",
    "\n",
    "Professional practice: **Track evaluation results over time**.\n",
    "\n",
    "This lets you:\n",
    "- Detect regressions (did I accidentally make things worse?)\n",
    "- Show progress to stakeholders\n",
    "- Compare different model versions\n",
    "- Understand what changes actually matter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96527325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this evaluation as a baseline\n",
    "from procurement_ai.evaluation import JSONReporter, MarkdownReporter\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"../benchmarks/results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save JSON (for programmatic comparison)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "json_path = results_dir / f\"evaluation_{timestamp}.json\"\n",
    "JSONReporter.report(result, output_file=json_path)\n",
    "\n",
    "# Save Markdown (for documentation)\n",
    "md_path = results_dir / f\"evaluation_{timestamp}.md\"\n",
    "MarkdownReporter.report(\n",
    "    result,\n",
    "    output_file=md_path,\n",
    "    title=\"Baseline Evaluation - Version 1.0\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Results saved!\")\n",
    "print(f\"   JSON: {json_path}\")\n",
    "print(f\"   Markdown: {md_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ffecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two evaluations\n",
    "from procurement_ai.evaluation import ComparisonReporter\n",
    "\n",
    "# Example: Compare current result with baseline\n",
    "# (In practice, you'd load baseline from JSON)\n",
    "\n",
    "comparison_md = ComparisonReporter.compare(\n",
    "    baseline=result,\n",
    "    comparison=result,  # Normally this would be a new result\n",
    "    baseline_name=\"Version 1.0\",\n",
    "    comparison_name=\"Version 1.1 (with examples)\"\n",
    ")\n",
    "\n",
    "print(comparison_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ff87a",
   "metadata": {},
   "source": [
    "## Part 7: Regression Testing\n",
    "\n",
    "**Regression testing:** Ensure changes don't break existing functionality.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "You add a new feature:\n",
    "- \"Make agent better at detecting AI tenders\"\n",
    "\n",
    "Accidentally:\n",
    "- Cybersecurity detection drops from 95% to 78% ‚ùå\n",
    "\n",
    "Without regression testing, you don't notice until production!\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Run evaluation after EVERY significant change:\n",
    "\n",
    "```bash\n",
    "# Before making changes\n",
    "python -m procurement_ai.evaluation.run --save-baseline\n",
    "\n",
    "# After making changes\n",
    "python -m procurement_ai.evaluation.run --output results/after_change.json\n",
    "\n",
    "# Compare\n",
    "python -m procurement_ai.evaluation.compare baseline.json after_change.json\n",
    "```\n",
    "\n",
    "**Gate your deployments:** Don't ship if F1 drops by >2%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58a778",
   "metadata": {},
   "source": [
    "## Part 8: Practical Exercises\n",
    "\n",
    "### Exercise 1: Find the Weakest Cases\n",
    "\n",
    "Look at your test results. Which specific test cases failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find failed test cases\n",
    "failed_cases = [tr for tr in result.test_results if not tr.is_correct]\n",
    "\n",
    "print(f\"‚ùå Failed Cases: {len(failed_cases)}\\n\")\n",
    "\n",
    "for tc in failed_cases:\n",
    "    print(f\"Test: {tc.test_id}\")\n",
    "    print(f\"  Category: {tc.test_category}\")\n",
    "    print(f\"  Predicted: {'Relevant' if tc.predicted_relevant else 'Irrelevant'} \"\n",
    "          f\"(confidence: {tc.predicted_confidence:.2f})\")\n",
    "    print(f\"  Expected: {'Relevant' if tc.expected_relevant else 'Irrelevant'}\")\n",
    "    print(f\"  Notes: {tc.notes}\")\n",
    "    print()\n",
    "\n",
    "# Exercise: Pick one failed case and analyze WHY it failed\n",
    "# Then propose a fix to the prompt or criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e8a311",
   "metadata": {},
   "source": [
    "### Exercise 2: Test Temperature Impact\n",
    "\n",
    "Our filter agent uses temperature=0.1 for consistency. What if we try 0.3? Or 0.7?\n",
    "\n",
    "**Task:** Modify config, re-run evaluation, compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "async def test_temperature(temp: float, test_cases):\n",
    "    \"\"\"Evaluate at a specific temperature\"\"\"\n",
    "    test_config = Config()\n",
    "    test_config.TEMPERATURE_PRECISE = temp\n",
    "    \n",
    "    test_llm = LLMService(test_config)\n",
    "    evaluator = Evaluator(config=test_config, llm_service=test_llm)\n",
    "    \n",
    "    result = await evaluator.quick_eval(test_cases[:5])  # Quick test on 5 cases\n",
    "    return result\n",
    "\n",
    "# Test range\n",
    "print(\"üå°Ô∏è  Temperature Impact Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "temperatures = [0.0, 0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "# Note: In practice, you'd run this. For the notebook, we show the pattern.\n",
    "print(\"Testing temperatures:\", temperatures)\n",
    "print(\"\\nHypothesis: Very low temp (0.0) = most consistent\")\n",
    "print(\"            Higher temp (0.7) = more varied, possibly less accurate\")\n",
    "print(\"\\nRun this experiment and record results!\")\n",
    "\n",
    "# Uncomment to actually run:\n",
    "# for temp in temperatures:\n",
    "#     metrics = await test_temperature(temp, edge_cases)\n",
    "#     print(f\"Temp {temp:.1f}: F1={metrics['f1_score']:.2%}, \"\n",
    "#           f\"Time={metrics['processing_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2258ed",
   "metadata": {},
   "source": [
    "### Exercise 3: Category-Specific Analysis\n",
    "\n",
    "Are we better at detecting cybersecurity than AI tenders? Or vice versa?\n",
    "\n",
    "**Task:** Group test results by expected category and calculate accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-specific analysis\n",
    "from collections import defaultdict\n",
    "\n",
    "category_performance = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "for tc in result.test_results:\n",
    "    if tc.expected_relevant and tc.expected_categories:\n",
    "        for cat in tc.expected_categories:\n",
    "            category_performance[cat][\"total\"] += 1\n",
    "            if tc.categories_correct:\n",
    "                category_performance[cat][\"correct\"] += 1\n",
    "\n",
    "print(\"üìä Category Detection Performance\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cat, perf in sorted(category_performance.items()):\n",
    "    if perf[\"total\"] > 0:\n",
    "        accuracy = perf[\"correct\"] / perf[\"total\"]\n",
    "        bar = \"‚ñà\" * int(accuracy * 20)\n",
    "        print(f\"{cat:<20} {accuracy:>6.1%}  {bar}\")\n",
    "        print(f\"  {perf['correct']}/{perf['total']} correct\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(\"   - Are certain categories harder to detect?\")\n",
    "print(\"   - Should we adjust the prompt to emphasize weak categories?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102342a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Measurement is Essential**\n",
    "   - Can't improve what you don't measure\n",
    "   - Subjective assessment (\"looks good\") is not enough\n",
    "   - Metrics provide objective truth\n",
    "\n",
    "2. **Good Test Sets are Hard**\n",
    "   - Need diverse, balanced, representative cases\n",
    "   - Edge cases are where systems fail\n",
    "   - Document your reasoning for each label\n",
    "\n",
    "3. **Multiple Metrics Tell the Full Story**\n",
    "   - Precision vs Recall trade-off\n",
    "   - F1 balances both\n",
    "   - Calibration ensures confidence is trustworthy\n",
    "\n",
    "4. **Evaluation Enables Science**\n",
    "   - A/B test prompts objectively\n",
    "   - Track changes over time\n",
    "   - Prevent regressions\n",
    "\n",
    "5. **Professional Development Cycle**\n",
    "   ```\n",
    "   Measure ‚Üí Change ‚Üí Measure ‚Üí Decide\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90269b60",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Immediate Actions\n",
    "\n",
    "1. **Run baseline evaluation**\n",
    "   ```bash\n",
    "   python -m procurement_ai.evaluation.run --save-baseline\n",
    "   ```\n",
    "\n",
    "2. **Analyze weaknesses**\n",
    "   - Which test cases failed?\n",
    "   - Which categories are hard to detect?\n",
    "   - Is the agent over/under confident?\n",
    "\n",
    "3. **Make targeted improvements**\n",
    "   - Fix the weakest area first\n",
    "   - Re-evaluate after each change\n",
    "\n",
    "4. **Establish regression testing**\n",
    "   - Run evaluation before every PR\n",
    "   - Don't ship if metrics drop\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Next notebook: RAG (Retrieval-Augmented Generation)**\n",
    "\n",
    "Now that we can measure improvements, we'll add RAG to:\n",
    "- Improve document quality\n",
    "- **Prove** it works with our evaluation framework\n",
    "- Measure the exact improvement (e.g., +22% quality)\n",
    "\n",
    "The evaluation framework enables everything that comes next!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e862b2",
   "metadata": {},
   "source": [
    "## Appendix: Quick Reference\n",
    "\n",
    "### Essential Metrics\n",
    "\n",
    "**Classification (Filter Agent):**\n",
    "- **Precision:** TP / (TP + FP) - \"When we say yes, how often are we right?\"\n",
    "- **Recall:** TP / (TP + FN) - \"Of all real yes cases, how many did we catch?\"\n",
    "- **F1:** Harmonic mean of precision and recall\n",
    "- **Accuracy:** (TP + TN) / Total - \"Overall correctness\"\n",
    "\n",
    "**Regression (Rating Agent):**\n",
    "- **MAE:** Mean Absolute Error - \"Average prediction error\"\n",
    "- **Correlation:** How well scores track expected scores\n",
    "\n",
    "**Calibration:**\n",
    "- **ECE:** Expected Calibration Error - \"Does confidence match accuracy?\"\n",
    "\n",
    "### CLI Commands\n",
    "\n",
    "```bash\n",
    "# Run evaluation and show in console\n",
    "python -m procurement_ai.evaluation.run\n",
    "\n",
    "# Save as baseline\n",
    "python -m procurement_ai.evaluation.run --save-baseline\n",
    "\n",
    "# Save custom output\n",
    "python -m procurement_ai.evaluation.run --output myresults.json --markdown report.md\n",
    "\n",
    "# Show detailed results\n",
    "python -m procurement_ai.evaluation.run --detailed\n",
    "```\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "**A/B Testing:**\n",
    "```python\n",
    "baseline = await evaluator.evaluate_dataset(test_cases)\n",
    "# Make change to prompt/config\n",
    "new_result = await evaluator.evaluate_dataset(test_cases)\n",
    "improvement = new_result.filter_metrics.f1_score - baseline.filter_metrics.f1_score\n",
    "```\n",
    "\n",
    "**Quick Iteration:**\n",
    "```python\n",
    "metrics = await evaluator.quick_eval(test_cases[:10])\n",
    "print(f\"F1: {metrics['f1_score']:.2%}\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e072a0",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Evaluation Driven Development (Google)](https://developers.google.com/machine-learning/testing-debugging/metrics/metrics)\n",
    "- [Expected Calibration Error](https://arxiv.org/abs/1706.04599)\n",
    "- [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) - Why measurement matters\n",
    "- Our evaluation dataset: `tests/fixtures/evaluation_dataset.py`\n",
    "- Our metrics implementation: `src/procurement_ai/evaluation/metrics.py`\n",
    "\n",
    "**Remember:** Professional AI development is empirical. Measure everything!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
