{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef72f8d",
   "metadata": {},
   "source": [
    "# 02: Structured Outputs with Pydantic\n",
    "\n",
    "**Duration:** 30 minutes\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Why structured outputs matter\n",
    "- How to use Pydantic models to define schemas\n",
    "- Getting JSON instead of free text from LLMs\n",
    "- Handling validation and retries\n",
    "\n",
    "**Why This Matters:**\n",
    "Free text responses are great for humans, but terrible for software. Structured outputs let you build reliable systems that process LLM responses programmatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23337a5",
   "metadata": {},
   "source": [
    "## Step 1: Install Pydantic\n",
    "\n",
    "Pydantic is Python's most popular data validation library. It lets you define data structures with types and validation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0069012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: anyio in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd8697",
   "metadata": {},
   "source": [
    "## Step 2: Define Your First Pydantic Model\n",
    "\n",
    "Instead of getting \"RELEVANT\" as text, let's get a structured classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf1868f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example classification:\n",
      "{\n",
      "  \"is_relevant\": true,\n",
      "  \"confidence\": 0.95,\n",
      "  \"categories\": [\n",
      "    \"cybersecurity\",\n",
      "    \"ai\"\n",
      "  ],\n",
      "  \"reasoning\": \"This tender combines cybersecurity and AI which matches our expertise.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "\n",
    "class TenderCategory(str, Enum):\n",
    "    \"\"\"Possible tender categories\"\"\"\n",
    "    CYBERSECURITY = \"cybersecurity\"\n",
    "    AI = \"ai\"\n",
    "    SOFTWARE = \"software\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "class TenderClassification(BaseModel):\n",
    "    \"\"\"Structured output for tender classification\"\"\"\n",
    "    is_relevant: bool = Field(description=\"Is this tender relevant?\")\n",
    "    confidence: float = Field(description=\"Confidence score 0-1\", ge=0, le=1)\n",
    "    categories: List[TenderCategory] = Field(description=\"Detected categories\")\n",
    "    reasoning: str = Field(description=\"Why you made this decision\")\n",
    "\n",
    "# Test creating an instance\n",
    "example = TenderClassification(\n",
    "    is_relevant=True,\n",
    "    confidence=0.95,\n",
    "    categories=[TenderCategory.CYBERSECURITY, TenderCategory.AI],\n",
    "    reasoning=\"This tender combines cybersecurity and AI which matches our expertise.\"\n",
    ")\n",
    "\n",
    "print(\"Example classification:\")\n",
    "print(example.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbba762",
   "metadata": {},
   "source": [
    "## Step 3: Understanding JSON Schema\n",
    "\n",
    "Pydantic models can export JSON schemas. We'll use this to tell the LLM what format we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9afbb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Schema for TenderClassification:\n",
      "{\n",
      "  \"$defs\": {\n",
      "    \"TenderCategory\": {\n",
      "      \"description\": \"Possible tender categories\",\n",
      "      \"enum\": [\n",
      "        \"cybersecurity\",\n",
      "        \"ai\",\n",
      "        \"software\",\n",
      "        \"other\"\n",
      "      ],\n",
      "      \"title\": \"TenderCategory\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"description\": \"Structured output for tender classification\",\n",
      "  \"properties\": {\n",
      "    \"is_relevant\": {\n",
      "      \"description\": \"Is this tender relevant?\",\n",
      "      \"title\": \"Is Relevant\",\n",
      "      \"type\": \"boolean\"\n",
      "    },\n",
      "    \"confidence\": {\n",
      "      \"description\": \"Confidence score 0-1\",\n",
      "      \"maximum\": 1,\n",
      "      \"minimum\": 0,\n",
      "      \"title\": \"Confidence\",\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"categories\": {\n",
      "      \"description\": \"Detected categories\",\n",
      "      \"items\": {\n",
      "        \"$ref\": \"#/$defs/TenderCategory\"\n",
      "      },\n",
      "      \"title\": \"Categories\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"reasoning\": {\n",
      "      \"description\": \"Why you made this decision\",\n",
      "      \"title\": \"Reasoning\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"is_relevant\",\n",
      "    \"confidence\",\n",
      "    \"categories\",\n",
      "    \"reasoning\"\n",
      "  ],\n",
      "  \"title\": \"TenderClassification\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Get the JSON schema from our model\n",
    "schema = TenderClassification.model_json_schema()\n",
    "\n",
    "print(\"JSON Schema for TenderClassification:\")\n",
    "print(json.dumps(schema, indent=2))\n",
    "\n",
    "# This tells us:\n",
    "# - What fields are required\n",
    "# - What type each field should be\n",
    "# - Validation constraints (like confidence between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9f0c8",
   "metadata": {},
   "source": [
    "## Step 4: Building the Structured Prompt\n",
    "\n",
    "To get structured output, we need to tell the LLM:\n",
    "1. What format we want (JSON)\n",
    "2. What the schema looks like\n",
    "3. That it should ONLY return valid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc29684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured prompt:\n",
      "Is this tender about AI?\n",
      "\n",
      "CRITICAL: Respond with ONLY valid JSON matching this schema:\n",
      "{\n",
      "  \"$defs\": {\n",
      "    \"TenderCategory\": {\n",
      "      \"description\": \"Possible tender categories\",\n",
      "      \"enum\": [\n",
      "        \"cybersecurity\",\n",
      "        \"ai\",\n",
      "        \"software\",\n",
      "        \"other\"\n",
      "      ],\n",
      "      \"title\": \"TenderCategory\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"description\": \"Structured output for tender classification\",\n",
      "  \"properties\": {\n",
      "    \"is_relevant\": {\n",
      "      \"description\": \"Is this tender relevant?\",\n",
      "      \"title\": \"Is Relevant\",\n",
      "      \"type\": \"boolean\"\n",
      "    },\n",
      "    \"confidence\": {\n",
      "      \"description\": \"Confidence score 0-1\",\n",
      "      \"maximum\": 1,\n",
      "      \"minimum\": 0,\n",
      "      \"title\": \"Confidence\",\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"categories\": {\n",
      "      \"description\": \"Detected categories\",\n",
      "      \"items\": {\n",
      "        \"$ref\": \"#/$defs/TenderCategory\"\n",
      "      },\n",
      "      \"title\": \"Categories\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"reasoning\": {\n",
      "      \"description\": \"Why you made this decision\",\n",
      "      \"title\": \"Reasoning\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"is_relevant\",\n",
      "    \"confidence\",\n",
      "    \"categories\",\n",
      "    \"reasoning\"\n",
      "  ],\n",
      "  \"title\": \"TenderClassification\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "Do not include any markdown formatting, code blocks, or explanatory text.\n",
      "Return ONLY the raw JSON object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_structured_prompt(user_prompt: str, model_class: type[BaseModel]) -> str:\n",
    "    \"\"\"Add schema instructions to a prompt\"\"\"\n",
    "    \n",
    "    # Get an example of the output format\n",
    "    schema = model_class.model_json_schema()\n",
    "    \n",
    "    # Build complete prompt with instructions\n",
    "    full_prompt = f\"\"\"{user_prompt}\n",
    "\n",
    "CRITICAL: Respond with ONLY valid JSON matching this schema:\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Do not include any markdown formatting, code blocks, or explanatory text.\n",
    "Return ONLY the raw JSON object.\n",
    "\"\"\"\n",
    "    \n",
    "    return full_prompt\n",
    "\n",
    "# Test it\n",
    "test_prompt = \"Is this tender about AI?\"\n",
    "structured = build_structured_prompt(test_prompt, TenderClassification)\n",
    "print(\"Structured prompt:\")\n",
    "print(structured)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098c319",
   "metadata": {},
   "source": [
    "## Step 5: Making the API Call\n",
    "\n",
    "Now let's call the LLM and parse the response into our Pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731d8622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Structured LLM function ready!\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "from typing import TypeVar, Type\n",
    "\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "BASE_URL = \"http://localhost:1234/v1\"\n",
    "MODEL = \"local-model\"\n",
    "\n",
    "async def call_llm_structured(\n",
    "    prompt: str, \n",
    "    response_model: Type[T],\n",
    "    temperature: float = 0.1\n",
    ") -> T:\n",
    "    \"\"\"Call LLM and return structured output\"\"\"\n",
    "    \n",
    "    # Build structured prompt\n",
    "    full_prompt = build_structured_prompt(prompt, response_model)\n",
    "    \n",
    "    # Make API call\n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{BASE_URL}/chat/completions\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a precise data analyst. Always return valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": full_prompt}\n",
    "                ],\n",
    "                \"temperature\": temperature,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        result = response.json()\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Clean the response (remove markdown, extra text, etc.)\n",
    "        content = content.strip()\n",
    "        if content.startswith(\"```json\"):\n",
    "            content = content[7:]\n",
    "        if content.startswith(\"```\"):\n",
    "            content = content[3:]\n",
    "        if content.endswith(\"```\"):\n",
    "            content = content[:-3]\n",
    "        content = content.strip()\n",
    "        \n",
    "        # Parse JSON and validate with Pydantic\n",
    "        data = json.loads(content)\n",
    "        return response_model.model_validate(data)\n",
    "\n",
    "print(\"‚úì Structured LLM function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e9eed",
   "metadata": {},
   "source": [
    "## Step 6: Test with Real Data\n",
    "\n",
    "Let's classify some real tenders using structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20a4c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: AI Cybersecurity Tender\n",
      "============================================================\n",
      "Relevant: True\n",
      "Confidence: 0.95\n",
      "Categories: ['cybersecurity', 'ai']\n",
      "Reasoning: The tender requests a machine learning-based cybersecurity solution for real-time threat detection and automated incident response, directly aligning with the company's expertise in AI and cybersecurity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Clearly relevant tender\n",
    "print(\"Example 1: AI Cybersecurity Tender\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Analyze this tender:\n",
    "\n",
    "TITLE: AI-Powered Network Intrusion Detection System\n",
    "DESCRIPTION: Government agency seeks vendor to develop and deploy machine learning-based \n",
    "cybersecurity solution for real-time threat detection across enterprise network. \n",
    "Must include automated incident response and integration with existing SIEM.\n",
    "\n",
    "Determine if this is relevant for a tech company specializing in AI and cybersecurity.\n",
    "\"\"\"\n",
    "\n",
    "result1 = await call_llm_structured(prompt1, TenderClassification)\n",
    "print(f\"Relevant: {result1.is_relevant}\")\n",
    "print(f\"Confidence: {result1.confidence}\")\n",
    "print(f\"Categories: {[c.value for c in result1.categories]}\")\n",
    "print(f\"Reasoning: {result1.reasoning}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b08ed133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: Office Furniture\n",
      "============================================================\n",
      "Relevant: False\n",
      "Confidence: 0.95\n",
      "Categories: ['other']\n",
      "Reasoning: The tender requests ergonomic chairs, standing desks, and filing cabinets for a government office building. This procurement is purely for physical office furniture and does not involve any technology, software, AI solutions, or cybersecurity services. Therefore it is not relevant to a tech company specializing in AI and cybersecurity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Clearly NOT relevant\n",
    "print(\"Example 2: Office Furniture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "Analyze this tender:\n",
    "\n",
    "TITLE: Office Furniture Supply Contract\n",
    "DESCRIPTION: Supply 500 ergonomic chairs, standing desks, and filing cabinets \n",
    "for new government office building. Delivery required within 3 months.\n",
    "\n",
    "Determine if this is relevant for a tech company specializing in AI and cybersecurity.\n",
    "\"\"\"\n",
    "\n",
    "result2 = await call_llm_structured(prompt2, TenderClassification)\n",
    "print(f\"Relevant: {result2.is_relevant}\")\n",
    "print(f\"Confidence: {result2.confidence}\")\n",
    "print(f\"Categories: {[c.value for c in result2.categories]}\")\n",
    "print(f\"Reasoning: {result2.reasoning}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "404754b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3: Hardware with Software Component\n",
      "============================================================\n",
      "Relevant: True\n",
      "Confidence: 0.95\n",
      "Categories: ['cybersecurity']\n",
      "Reasoning: The tender seeks enterprise firewall appliances with installation and configuration, directly pertaining to cybersecurity infrastructure. An AI-focused company that also specializes in cybersecurity would find this procurement relevant as it aligns with their core domain of securing networks and could integrate AI-driven threat detection or management into the firewall solutions.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Edge case - Hardware with some software\n",
    "print(\"Example 3: Hardware with Software Component\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt3 = \"\"\"\n",
    "Analyze this tender:\n",
    "\n",
    "TITLE: Firewall Hardware Procurement\n",
    "DESCRIPTION: Purchase 50 enterprise firewall appliances. Vendor must provide \n",
    "installation and basic configuration. Devices come with built-in software.\n",
    "\n",
    "Determine if this is relevant for a tech company specializing in AI and cybersecurity.\n",
    "\"\"\"\n",
    "\n",
    "result3 = await call_llm_structured(prompt3, TenderClassification)\n",
    "print(f\"Relevant: {result3.is_relevant}\")\n",
    "print(f\"Confidence: {result3.confidence}\")\n",
    "print(f\"Categories: {[c.value for c in result3.categories]}\")\n",
    "print(f\"Reasoning: {result3.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8c7ee",
   "metadata": {},
   "source": [
    "## Step 7: Handling Validation Errors\n",
    "\n",
    "Sometimes LLMs return invalid data. Pydantic catches these errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a96158a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing validation:\n",
      "============================================================\n",
      "‚úì Caught validation error:\n",
      "1 validation error for TenderClassification\n",
      "confidence\n",
      "  Input should be less than or equal to 1 [type=less_than_equal, input_value=1.5, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/less_than_equal\n",
      "\n",
      "\n",
      "‚úì Caught enum error:\n",
      "1 validation error for TenderClassification\n",
      "categories.0\n",
      "  Input should be 'cybersecurity', 'ai', 'software' or 'other' [type=enum, input_value='blockchain', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/enum\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "# Try to create invalid data\n",
    "print(\"Testing validation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # This should fail - confidence > 1.0\n",
    "    bad_data = TenderClassification(\n",
    "        is_relevant=True,\n",
    "        confidence=1.5,  # Invalid! Must be 0-1\n",
    "        categories=[TenderCategory.AI],\n",
    "        reasoning=\"Test\"\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"‚úì Caught validation error:\")\n",
    "    print(e)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "try:\n",
    "    # This should fail - wrong enum value\n",
    "    bad_data2 = TenderClassification(\n",
    "        is_relevant=True,\n",
    "        confidence=0.8,\n",
    "        categories=[\"blockchain\"],  # Invalid category!\n",
    "        reasoning=\"Test\"\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"‚úì Caught enum error:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8897cf2",
   "metadata": {},
   "source": [
    "## Step 8: Adding Retry Logic\n",
    "\n",
    "LLMs are unreliable. Sometimes they return malformed JSON. Let's add retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd5ebbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retry logic ready!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def call_llm_structured_with_retry(\n",
    "    prompt: str,\n",
    "    response_model: Type[T],\n",
    "    max_retries: int = 3,\n",
    "    temperature: float = 0.1\n",
    ") -> T:\n",
    "    \"\"\"Call LLM with retry logic for robustness\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = await call_llm_structured(prompt, response_model, temperature)\n",
    "            if attempt > 0:\n",
    "                print(f\"  ‚úì Success on attempt {attempt + 1}\")\n",
    "            return result\n",
    "            \n",
    "        except (json.JSONDecodeError, ValidationError, KeyError) as e:\n",
    "            print(f\"  Attempt {attempt + 1}/{max_retries} failed: {str(e)[:100]}\")\n",
    "            \n",
    "            if attempt == max_retries - 1:\n",
    "                raise Exception(f\"Failed after {max_retries} attempts: {e}\")\n",
    "            \n",
    "            # Wait before retry\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "print(\"‚úì Retry logic ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24996b",
   "metadata": {},
   "source": [
    "## Step 9: Comparing Text vs Structured Outputs\n",
    "\n",
    "Let's see the difference side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8de83973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT OUTPUT:\n",
      "============================================================\n",
      "**Yes**\n",
      "\n",
      "The tender is specifically for developing a custom web application (a tax portal) with secure authentication, which falls squarely within the core services of a tech company that builds software solutions. It involves web development, security implementation, and integration with tax‚Äërelated data‚Äîareas that a tech firm would typically handle.\n",
      "\n",
      "Type: <class 'str'>\n",
      "Hard to parse programmatically\n",
      "\n",
      "\n",
      "STRUCTURED OUTPUT:\n",
      "============================================================\n",
      "{\n",
      "  \"is_relevant\": true,\n",
      "  \"confidence\": 0.95,\n",
      "  \"categories\": [\n",
      "    \"software\"\n",
      "  ],\n",
      "  \"reasoning\": \"The tender requests the development of a web application for tax filing, which directly involves custom software creation and aligns with the services offered by a tech company.\"\n",
      "}\n",
      "\n",
      "Type: <class '__main__.TenderClassification'>\n",
      "Easy to use: structured_result.is_relevant = True\n",
      "Type-safe: structured_result.confidence = 0.95\n"
     ]
    }
   ],
   "source": [
    "async def call_llm_text(prompt: str) -> str:\n",
    "    \"\"\"Simple text response (from notebook 01)\"\"\"\n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{BASE_URL}/chat/completions\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0.1,\n",
    "            },\n",
    "        )\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "test_prompt = \"\"\"\n",
    "Is this tender relevant for a tech company?\n",
    "\n",
    "TITLE: Custom Software Development for Tax Portal\n",
    "DESCRIPTION: Build web application for tax filing with secure authentication.\n",
    "\n",
    "Answer with yes/no and explain briefly.\n",
    "\"\"\"\n",
    "\n",
    "print(\"TEXT OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "text_result = await call_llm_text(test_prompt)\n",
    "print(text_result)\n",
    "print(f\"\\nType: {type(text_result)}\")\n",
    "print(\"Hard to parse programmatically\\n\\n\")\n",
    "\n",
    "print(\"STRUCTURED OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "structured_result = await call_llm_structured_with_retry(test_prompt, TenderClassification)\n",
    "print(structured_result.model_dump_json(indent=2))\n",
    "print(f\"\\nType: {type(structured_result)}\")\n",
    "print(f\"Easy to use: structured_result.is_relevant = {structured_result.is_relevant}\")\n",
    "print(f\"Type-safe: structured_result.confidence = {structured_result.confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa429a",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You now understand structured outputs!\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Pydantic models define schemas** - Clear contract for LLM outputs\n",
    "2. **JSON schemas guide LLMs** - Tell them exactly what format you need\n",
    "3. **Validation catches errors** - Pydantic ensures data quality\n",
    "4. **Retries handle failures** - LLMs are unreliable, plan for it\n",
    "5. **Structured > Text** - Much easier to build software with\n",
    "\n",
    "## Pattern You Learned\n",
    "\n",
    "```python\n",
    "# 1. Define schema\n",
    "class MyOutput(BaseModel):\n",
    "    field1: str\n",
    "    field2: float\n",
    "\n",
    "# 2. Add schema to prompt\n",
    "prompt = f\"{user_question}\\n\\nReturn JSON matching: {schema}\"\n",
    "\n",
    "# 3. Parse and validate\n",
    "result = MyOutput.model_validate(json.loads(llm_response))\n",
    "\n",
    "# 4. Use type-safe data\n",
    "print(result.field1)  # IDE knows this is a string!\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you can get structured outputs, let's build our first agent: the **Filter Agent** that classifies tenders!\n",
    "\n",
    "‚û°Ô∏è Continue to `03_filter_agent.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
