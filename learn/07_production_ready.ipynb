{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3435cfba",
   "metadata": {},
   "source": [
    "# 07: Production-Ready Deployment\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Converting notebooks to production code\n",
    "- Configuration management\n",
    "- Logging and monitoring\n",
    "- Error handling and retries\n",
    "- Testing strategies\n",
    "- Deployment patterns\n",
    "\n",
    "**The Reality:**\n",
    "Notebook code works for exploration. Production code needs reliability, observability, and maintainability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c9d09",
   "metadata": {},
   "source": [
    "## Development vs Production\n",
    "\n",
    "| Aspect | Development (Notebook) | Production (Code) |\n",
    "|--------|----------------------|------------------|\n",
    "| Error handling | Fail fast, show errors | Retry, log, recover |\n",
    "| Configuration | Hardcoded values | Environment variables, config files |\n",
    "| Logging | Print statements | Structured logging with levels |\n",
    "| Testing | Manual, ad-hoc | Automated, comprehensive |\n",
    "| Secrets | In code (OK for local) | External stores, never in code |\n",
    "| Dependencies | Latest versions | Pinned, reproducible |\n",
    "\n",
    "Let's upgrade our system for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3028da6",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9fbdcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (2.12.5)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpx) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/ARAJI/git/ai_projects/procurement-ai/venv/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b2e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports ready\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Type, TypeVar, Optional\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ“ Imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9d6e1",
   "metadata": {},
   "source": [
    "## Step 2: Production Configuration\n",
    "\n",
    "Never hardcode config. Use environment variables and a Config class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8ffec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration valid\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Production configuration management\n",
    "    \n",
    "    Best practices:\n",
    "    - All settings in one place\n",
    "    - Environment variables for deployment flexibility\n",
    "    - Sensible defaults for development\n",
    "    - Type hints for validation\n",
    "    \"\"\"\n",
    "    \n",
    "    # LLM Configuration\n",
    "    LLM_BASE_URL: str = os.getenv(\"LLM_BASE_URL\", \"http://localhost:1234/v1\")\n",
    "    LLM_MODEL: str = os.getenv(\"LLM_MODEL\", \"local-model\")\n",
    "    LLM_TIMEOUT: float = float(os.getenv(\"LLM_TIMEOUT\", \"120.0\"))\n",
    "    \n",
    "    # Temperature settings\n",
    "    TEMPERATURE_PRECISE: float = 0.1  # Filter, rating\n",
    "    TEMPERATURE_CREATIVE: float = 0.7  # Document generation\n",
    "    \n",
    "    # Thresholds\n",
    "    MIN_CONFIDENCE: float = float(os.getenv(\"MIN_CONFIDENCE\", \"0.6\"))\n",
    "    MIN_SCORE: float = float(os.getenv(\"MIN_SCORE\", \"7.0\"))\n",
    "    \n",
    "    # Retry configuration\n",
    "    MAX_RETRIES: int = int(os.getenv(\"MAX_RETRIES\", \"3\"))\n",
    "    RETRY_DELAY: float = float(os.getenv(\"RETRY_DELAY\", \"2.0\"))\n",
    "    \n",
    "    # Logging\n",
    "    LOG_LEVEL: str = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls) -> bool:\n",
    "        \"\"\"Validate configuration\"\"\"\n",
    "        try:\n",
    "            assert 0 <= cls.MIN_CONFIDENCE <= 1, \"MIN_CONFIDENCE must be 0-1\"\n",
    "            assert 0 <= cls.MIN_SCORE <= 10, \"MIN_SCORE must be 0-10\"\n",
    "            assert cls.MAX_RETRIES > 0, \"MAX_RETRIES must be positive\"\n",
    "            return True\n",
    "        except AssertionError as e:\n",
    "            print(f\"âŒ Configuration error: {e}\")\n",
    "            return False\n",
    "\n",
    "# Validate on import\n",
    "if Config.validate():\n",
    "    print(\"âœ“ Configuration valid\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7751ba",
   "metadata": {},
   "source": [
    "## Step 3: Structured Logging\n",
    "\n",
    "Replace print statements with proper logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7beb018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 09:10:07 - procurement_ai - INFO - Logging configured\n",
      "2026-02-01 09:10:07 - procurement_ai - WARNING - This is a warning\n"
     ]
    }
   ],
   "source": [
    "def setup_logging() -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure structured logging\n",
    "    \n",
    "    Levels:\n",
    "    - DEBUG: Detailed diagnostic info\n",
    "    - INFO: General progress updates\n",
    "    - WARNING: Recoverable issues\n",
    "    - ERROR: Serious problems\n",
    "    - CRITICAL: System failures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger(\"procurement_ai\")\n",
    "    logger.setLevel(getattr(logging, Config.LOG_LEVEL))\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Console handler with formatting\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Format: timestamp - level - message\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "logger.info(\"Logging configured\")\n",
    "logger.debug(\"This is a debug message\")\n",
    "logger.warning(\"This is a warning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3f7e5",
   "metadata": {},
   "source": [
    "## Step 4: Production LLM Service with Retries\n",
    "\n",
    "Add robust error handling, retries, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8ecee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Production LLM service ready\n"
     ]
    }
   ],
   "source": [
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"\n",
    "    Production-ready LLM service\n",
    "    \n",
    "    Features:\n",
    "    - Automatic retries with exponential backoff\n",
    "    - Structured logging\n",
    "    - Error categorization (retryable vs fatal)\n",
    "    - Request/response validation\n",
    "    - Timeout handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config = None, logger: logging.Logger = None):\n",
    "        self.config = config or Config()\n",
    "        self.logger = logger or setup_logging()\n",
    "    \n",
    "    async def generate_structured(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: Type[T],\n",
    "        system_prompt: str,\n",
    "        temperature: float,\n",
    "        operation: str = \"llm_call\"\n",
    "    ) -> T:\n",
    "        \"\"\"Generate structured output with retries\"\"\"\n",
    "        \n",
    "        for attempt in range(self.config.MAX_RETRIES):\n",
    "            try:\n",
    "                self.logger.debug(\n",
    "                    f\"Attempt {attempt + 1}/{self.config.MAX_RETRIES} for {operation}\"\n",
    "                )\n",
    "                \n",
    "                result = await self._call_api(\n",
    "                    prompt,\n",
    "                    response_model,\n",
    "                    system_prompt,\n",
    "                    temperature\n",
    "                )\n",
    "                \n",
    "                if attempt > 0:\n",
    "                    self.logger.info(f\"Success on attempt {attempt + 1} for {operation}\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except (json.JSONDecodeError, ValidationError, KeyError) as e:\n",
    "                self.logger.warning(\n",
    "                    f\"Attempt {attempt + 1} failed for {operation}: {type(e).__name__}: {str(e)[:100]}\"\n",
    "                )\n",
    "                \n",
    "                if attempt == self.config.MAX_RETRIES - 1:\n",
    "                    self.logger.error(f\"All retries exhausted for {operation}\")\n",
    "                    raise Exception(f\"Failed after {self.config.MAX_RETRIES} attempts: {e}\")\n",
    "                \n",
    "                # Exponential backoff\n",
    "                delay = self.config.RETRY_DELAY * (2 ** attempt)\n",
    "                self.logger.debug(f\"Waiting {delay}s before retry\")\n",
    "                await asyncio.sleep(delay)\n",
    "            \n",
    "            except httpx.TimeoutException as e:\n",
    "                self.logger.error(f\"Timeout for {operation}: {e}\")\n",
    "                raise\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error for {operation}: {e}\")\n",
    "                raise\n",
    "    \n",
    "    async def _call_api(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: Type[T],\n",
    "        system_prompt: str,\n",
    "        temperature: float\n",
    "    ) -> T:\n",
    "        \"\"\"Make API call and parse response\"\"\"\n",
    "        \n",
    "        # Build prompt with schema\n",
    "        schema = response_model.model_json_schema()\n",
    "        full_prompt = f\"\"\"{prompt}\\n\\nCRITICAL: Respond with ONLY valid JSON matching this schema:\\n{json.dumps(schema, indent=2)}\\n\\nReturn ONLY the raw JSON object.\"\"\"\n",
    "        \n",
    "        async with httpx.AsyncClient(timeout=self.config.LLM_TIMEOUT) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.config.LLM_BASE_URL}/chat/completions\",\n",
    "                json={\n",
    "                    \"model\": self.config.LLM_MODEL,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": full_prompt}\n",
    "                    ],\n",
    "                    \"temperature\": temperature,\n",
    "                },\n",
    "            )\n",
    "            \n",
    "            result = response.json()\n",
    "            content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            \n",
    "            # Clean\n",
    "            for marker in [\"```json\", \"```\"]:\n",
    "                if content.startswith(marker):\n",
    "                    content = content[len(marker):]\n",
    "                if content.endswith(marker):\n",
    "                    content = content[:-len(marker)]\n",
    "            content = content.strip()\n",
    "            \n",
    "            data = json.loads(content)\n",
    "            return response_model.model_validate(data)\n",
    "\n",
    "print(\"âœ“ Production LLM service ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc967a6d",
   "metadata": {},
   "source": [
    "## Step 5: Production Agents with Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a536773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Production agents ready\n"
     ]
    }
   ],
   "source": [
    "# Import models from previous notebooks (simplified here)\n",
    "class TenderCategory(str, Enum):\n",
    "    CYBERSECURITY = \"cybersecurity\"\n",
    "    AI = \"ai\"\n",
    "    SOFTWARE = \"software\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "class Tender(BaseModel):\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    organization: str\n",
    "    deadline: str\n",
    "    estimated_value: str | None = None\n",
    "\n",
    "class FilterResult(BaseModel):\n",
    "    is_relevant: bool\n",
    "    confidence: float\n",
    "    categories: List[TenderCategory]\n",
    "    reasoning: str\n",
    "\n",
    "class FilterAgent:\n",
    "    \"\"\"Production filter agent\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: LLMService, logger: logging.Logger):\n",
    "        self.llm = llm\n",
    "        self.logger = logger\n",
    "    \n",
    "    async def filter(self, tender: Tender) -> FilterResult:\n",
    "        self.logger.info(f\"Filtering tender: {tender.id}\")\n",
    "        \n",
    "        prompt = f\"\"\"Analyze this tender:\\n\\nTITLE: {tender.title}\\nDESCRIPTION: {tender.description}\\n\\nCRITERIA: Relevant if involves cybersecurity, AI/ML, or software development.\"\"\"\n",
    "        \n",
    "        system = \"You are an expert procurement analyst. Be precise and conservative.\"\n",
    "        \n",
    "        result = await self.llm.generate_structured(\n",
    "            prompt,\n",
    "            FilterResult,\n",
    "            system,\n",
    "            Config.TEMPERATURE_PRECISE,\n",
    "            operation=\"filter_agent\"\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Filter result for {tender.id}: relevant={result.is_relevant}, \"\n",
    "            f\"confidence={result.confidence:.2f}\"\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ“ Production agents ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d3d92",
   "metadata": {},
   "source": [
    "## Step 6: Production Orchestrator with Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d40fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Production orchestrator ready\n"
     ]
    }
   ],
   "source": [
    "class ProcessedTender(BaseModel):\n",
    "    tender: Tender\n",
    "    filter_result: Optional[FilterResult] = None\n",
    "    status: str = \"pending\"\n",
    "    processing_time: float = 0.0\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "class ProductionOrchestrator:\n",
    "    \"\"\"\n",
    "    Production orchestrator with:\n",
    "    - Comprehensive logging\n",
    "    - Error tracking\n",
    "    - Performance metrics\n",
    "    - Health checks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Config = None,\n",
    "        logger: logging.Logger = None\n",
    "    ):\n",
    "        self.config = config or Config()\n",
    "        self.logger = logger or setup_logging()\n",
    "        \n",
    "        # Initialize services\n",
    "        self.llm = LLMService(self.config, self.logger)\n",
    "        self.filter_agent = FilterAgent(self.llm, self.logger)\n",
    "        \n",
    "        # Metrics\n",
    "        self.metrics = {\n",
    "            \"total_processed\": 0,\n",
    "            \"filtered_out\": 0,\n",
    "            \"complete\": 0,\n",
    "            \"errors\": 0,\n",
    "        }\n",
    "    \n",
    "    async def health_check(self) -> bool:\n",
    "        \"\"\"Verify system is operational\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Running health check\")\n",
    "            \n",
    "            # Test LLM connectivity\n",
    "            async with httpx.AsyncClient(timeout=5.0) as client:\n",
    "                response = await client.get(f\"{self.config.LLM_BASE_URL.replace('/v1', '')}/health\")\n",
    "                \n",
    "            self.logger.info(\"Health check passed\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Health check failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def process_tender(self, tender: Tender) -> ProcessedTender:\n",
    "        \"\"\"Process tender with full monitoring\"\"\"\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        result = ProcessedTender(tender=tender)\n",
    "        \n",
    "        self.logger.info(f\"Starting processing for tender {tender.id}\")\n",
    "        \n",
    "        try:\n",
    "            # Stage 1: Filter\n",
    "            result.filter_result = await self.filter_agent.filter(tender)\n",
    "            \n",
    "            if not result.filter_result.is_relevant:\n",
    "                result.status = \"filtered_out\"\n",
    "                self.metrics[\"filtered_out\"] += 1\n",
    "                self.logger.info(f\"Tender {tender.id} filtered out\")\n",
    "            else:\n",
    "                result.status = \"complete\"\n",
    "                self.metrics[\"complete\"] += 1\n",
    "                self.logger.info(f\"Tender {tender.id} processing complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result.status = \"error\"\n",
    "            result.error_message = str(e)\n",
    "            self.metrics[\"errors\"] += 1\n",
    "            self.logger.error(f\"Error processing tender {tender.id}: {e}\", exc_info=True)\n",
    "        \n",
    "        finally:\n",
    "            result.processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            self.metrics[\"total_processed\"] += 1\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"Completed tender {tender.id}: status={result.status}, \"\n",
    "                f\"time={result.processing_time:.2f}s\"\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_metrics(self) -> dict:\n",
    "        \"\"\"Get current metrics\"\"\"\n",
    "        return self.metrics.copy()\n",
    "\n",
    "print(\"âœ“ Production orchestrator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94465a",
   "metadata": {},
   "source": [
    "## Step 7: Test Production System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce89356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 09:10:07 - procurement_ai - INFO - Running health check\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HEALTH CHECK\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 09:10:07 - procurement_ai - INFO - Health check passed\n",
      "2026-02-01 09:10:07 - procurement_ai - INFO - Starting processing for tender PROD-001\n",
      "2026-02-01 09:10:07 - procurement_ai - INFO - Filtering tender: PROD-001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System health: âœ“ Healthy\n",
      "\n",
      "======================================================================\n",
      "PROCESSING TEST TENDER\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 09:10:16 - procurement_ai - INFO - Filter result for PROD-001: relevant=True, confidence=0.95\n",
      "2026-02-01 09:10:16 - procurement_ai - INFO - Tender PROD-001 processing complete\n",
      "2026-02-01 09:10:16 - procurement_ai - INFO - Completed tender PROD-001: status=complete, time=8.76s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "Status: complete\n",
      "Processing time: 8.76s\n",
      "Relevant: True\n",
      "Confidence: 0.95\n",
      "\n",
      "======================================================================\n",
      "METRICS\n",
      "======================================================================\n",
      "total_processed: 1\n",
      "filtered_out: 0\n",
      "complete: 1\n",
      "errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize production system\n",
    "orchestrator = ProductionOrchestrator()\n",
    "\n",
    "# Health check\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HEALTH CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Note: This may fail if LLM Studio doesn't have a health endpoint\n",
    "# That's OK - it's just a demonstration\n",
    "try:\n",
    "    healthy = await orchestrator.health_check()\n",
    "    print(f\"System health: {'âœ“ Healthy' if healthy else 'âœ— Unhealthy'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Health check not available: {e}\")\n",
    "    print(\"(This is expected - continuing with processing)\")\n",
    "\n",
    "# Process test tender\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSING TEST TENDER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_tender = Tender(\n",
    "    id=\"PROD-001\",\n",
    "    title=\"AI Security Platform\",\n",
    "    description=\"Develop AI-powered cybersecurity platform with ML threat detection\",\n",
    "    organization=\"Government Agency\",\n",
    "    deadline=\"2025-01-31\",\n",
    "    estimated_value=\"$1M\"\n",
    ")\n",
    "\n",
    "result = await orchestrator.process_tender(test_tender)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Processing time: {result.processing_time:.2f}s\")\n",
    "if result.filter_result:\n",
    "    print(f\"Relevant: {result.filter_result.is_relevant}\")\n",
    "    print(f\"Confidence: {result.filter_result.confidence:.2f}\")\n",
    "\n",
    "# Show metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METRICS\")\n",
    "print(\"=\"*70)\n",
    "metrics = orchestrator.get_metrics()\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b169f1",
   "metadata": {},
   "source": [
    "## Step 8: Environment Configuration\n",
    "\n",
    "Create a `.env` file for environment-specific settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d010748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example .env file:\n",
      "\n",
      "# LLM Configuration\n",
      "LLM_BASE_URL=http://localhost:1234/v1\n",
      "LLM_MODEL=local-model\n",
      "LLM_TIMEOUT=120.0\n",
      "\n",
      "# Thresholds\n",
      "MIN_CONFIDENCE=0.6\n",
      "MIN_SCORE=7.0\n",
      "\n",
      "# Retry Configuration\n",
      "MAX_RETRIES=3\n",
      "RETRY_DELAY=2.0\n",
      "\n",
      "# Logging\n",
      "LOG_LEVEL=INFO\n",
      "\n",
      "# Production overrides (comment out for development)\n",
      "# LLM_BASE_URL=https://api.groq.com/v1\n",
      "# LLM_MODEL=llama-3.1-8b\n",
      "# LOG_LEVEL=WARNING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example .env file content\n",
    "env_example = \"\"\"\n",
    "# LLM Configuration\n",
    "LLM_BASE_URL=http://localhost:1234/v1\n",
    "LLM_MODEL=local-model\n",
    "LLM_TIMEOUT=120.0\n",
    "\n",
    "# Thresholds\n",
    "MIN_CONFIDENCE=0.6\n",
    "MIN_SCORE=7.0\n",
    "\n",
    "# Retry Configuration\n",
    "MAX_RETRIES=3\n",
    "RETRY_DELAY=2.0\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL=INFO\n",
    "\n",
    "# Production overrides (comment out for development)\n",
    "# LLM_BASE_URL=https://api.groq.com/v1\n",
    "# LLM_MODEL=llama-3.1-8b\n",
    "# LOG_LEVEL=WARNING\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example .env file:\")\n",
    "print(env_example)\n",
    "\n",
    "# Write to file (optional)\n",
    "# with open('.env.example', 'w') as f:\n",
    "#     f.write(env_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015af519",
   "metadata": {},
   "source": [
    "## Step 9: Testing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec08405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 09:10:16 - procurement_ai - ERROR - Unexpected error for llm_call: [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING TESTS\n",
      "======================================================================\n",
      "Testing configuration validation...\n",
      "  âœ“ Valid configuration passes\n",
      "âŒ Configuration error: MIN_CONFIDENCE must be 0-1\n",
      "  âœ“ Invalid MIN_CONFIDENCE caught\n",
      "\n",
      "âœ“ Configuration tests passed\n",
      "\n",
      "Testing error handling...\n",
      "  âœ“ Error caught correctly: ConnectError\n",
      "\n",
      "âœ“ Error handling tests passed\n",
      "\n",
      "Testing Pydantic models...\n",
      "  âœ“ Valid tender created\n",
      "  âœ— Should have raised ValidationError\n",
      "\n",
      "âœ“ Model tests passed\n",
      "\n",
      "======================================================================\n",
      "ALL TESTS PASSED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def test_config_validation():\n",
    "    \"\"\"Test configuration validation\"\"\"\n",
    "    print(\"Testing configuration validation...\")\n",
    "    \n",
    "    # Valid config\n",
    "    assert Config.validate() == True\n",
    "    print(\"  âœ“ Valid configuration passes\")\n",
    "    \n",
    "    # Test individual validations\n",
    "    original_confidence = Config.MIN_CONFIDENCE\n",
    "    \n",
    "    Config.MIN_CONFIDENCE = 1.5  # Invalid\n",
    "    assert Config.validate() == False\n",
    "    print(\"  âœ“ Invalid MIN_CONFIDENCE caught\")\n",
    "    \n",
    "    Config.MIN_CONFIDENCE = original_confidence  # Restore\n",
    "    \n",
    "    print(\"\\nâœ“ Configuration tests passed\")\n",
    "\n",
    "async def test_error_handling():\n",
    "    \"\"\"Test error handling and retries\"\"\"\n",
    "    print(\"\\nTesting error handling...\")\n",
    "    \n",
    "    # Test with invalid LLM URL (will fail and retry)\n",
    "    bad_config = Config()\n",
    "    bad_config.LLM_BASE_URL = \"http://invalid:9999/v1\"\n",
    "    bad_config.MAX_RETRIES = 1  # Fail fast\n",
    "    \n",
    "    llm = LLMService(bad_config, setup_logging())\n",
    "    \n",
    "    try:\n",
    "        result = await llm.generate_structured(\n",
    "            \"test\",\n",
    "            FilterResult,\n",
    "            \"test\",\n",
    "            0.1\n",
    "        )\n",
    "        print(\"  âœ— Should have raised exception\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ“ Error caught correctly: {type(e).__name__}\")\n",
    "    \n",
    "    print(\"\\nâœ“ Error handling tests passed\")\n",
    "\n",
    "def test_models():\n",
    "    \"\"\"Test Pydantic models\"\"\"\n",
    "    print(\"\\nTesting Pydantic models...\")\n",
    "    \n",
    "    # Valid tender\n",
    "    tender = Tender(\n",
    "        id=\"TEST\",\n",
    "        title=\"Test\",\n",
    "        description=\"Test\",\n",
    "        organization=\"Test\",\n",
    "        deadline=\"2024-12-31\"\n",
    "    )\n",
    "    print(\"  âœ“ Valid tender created\")\n",
    "    \n",
    "    # Invalid filter result (confidence > 1)\n",
    "    try:\n",
    "        FilterResult(\n",
    "            is_relevant=True,\n",
    "            confidence=1.5,  # Invalid!\n",
    "            categories=[TenderCategory.AI],\n",
    "            reasoning=\"Test\"\n",
    "        )\n",
    "        print(\"  âœ— Should have raised ValidationError\")\n",
    "    except ValidationError:\n",
    "        print(\"  âœ“ Invalid FilterResult caught\")\n",
    "    \n",
    "    print(\"\\nâœ“ Model tests passed\")\n",
    "\n",
    "# Run tests\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_config_validation()\n",
    "await test_error_handling()\n",
    "test_models()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL TESTS PASSED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d080b",
   "metadata": {},
   "source": [
    "## Step 10: Deployment Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f4c56c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Production Deployment Checklist\n",
      "\n",
      "## Pre-Deployment\n",
      "- [ ] All configuration in environment variables\n",
      "- [ ] No secrets in code\n",
      "- [ ] Dependencies pinned in requirements.txt\n",
      "- [ ] Tests passing\n",
      "- [ ] Error handling tested\n",
      "- [ ] Logging configured correctly\n",
      "\n",
      "## Infrastructure\n",
      "- [ ] LLM endpoint accessible\n",
      "- [ ] Health check endpoint configured\n",
      "- [ ] Monitoring/alerting set up\n",
      "- [ ] Log aggregation configured\n",
      "- [ ] Backup strategy in place\n",
      "\n",
      "## Security\n",
      "- [ ] API keys stored securely (e.g., AWS Secrets Manager)\n",
      "- [ ] Network security configured\n",
      "- [ ] Rate limiting implemented\n",
      "- [ ] Input validation enabled\n",
      "- [ ] Audit logging enabled\n",
      "\n",
      "## Performance\n",
      "- [ ] Timeout values tuned\n",
      "- [ ] Retry strategy tested\n",
      "- [ ] Resource limits set\n",
      "- [ ] Scalability plan in place\n",
      "- [ ] Performance benchmarks established\n",
      "\n",
      "## Operations\n",
      "- [ ] Deployment procedure documented\n",
      "- [ ] Rollback plan prepared\n",
      "- [ ] Monitoring dashboard created\n",
      "- [ ] On-call rotation defined\n",
      "- [ ] Incident response plan documented\n",
      "\n",
      "## Post-Deployment\n",
      "- [ ] Smoke tests passed\n",
      "- [ ] Monitoring alerts verified\n",
      "- [ ] Performance metrics baseline\n",
      "- [ ] Documentation updated\n",
      "- [ ] Team trained\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deployment_checklist = \"\"\"\n",
    "# Production Deployment Checklist\n",
    "\n",
    "## Pre-Deployment\n",
    "- [ ] All configuration in environment variables\n",
    "- [ ] No secrets in code\n",
    "- [ ] Dependencies pinned in requirements.txt\n",
    "- [ ] Tests passing\n",
    "- [ ] Error handling tested\n",
    "- [ ] Logging configured correctly\n",
    "\n",
    "## Infrastructure\n",
    "- [ ] LLM endpoint accessible\n",
    "- [ ] Health check endpoint configured\n",
    "- [ ] Monitoring/alerting set up\n",
    "- [ ] Log aggregation configured\n",
    "- [ ] Backup strategy in place\n",
    "\n",
    "## Security\n",
    "- [ ] API keys stored securely (e.g., AWS Secrets Manager)\n",
    "- [ ] Network security configured\n",
    "- [ ] Rate limiting implemented\n",
    "- [ ] Input validation enabled\n",
    "- [ ] Audit logging enabled\n",
    "\n",
    "## Performance\n",
    "- [ ] Timeout values tuned\n",
    "- [ ] Retry strategy tested\n",
    "- [ ] Resource limits set\n",
    "- [ ] Scalability plan in place\n",
    "- [ ] Performance benchmarks established\n",
    "\n",
    "## Operations\n",
    "- [ ] Deployment procedure documented\n",
    "- [ ] Rollback plan prepared\n",
    "- [ ] Monitoring dashboard created\n",
    "- [ ] On-call rotation defined\n",
    "- [ ] Incident response plan documented\n",
    "\n",
    "## Post-Deployment\n",
    "- [ ] Smoke tests passed\n",
    "- [ ] Monitoring alerts verified\n",
    "- [ ] Performance metrics baseline\n",
    "- [ ] Documentation updated\n",
    "- [ ] Team trained\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10beac8f",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You now have a production-ready AI system!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "1. **Configuration management** - Environment variables, validation\n",
    "2. **Structured logging** - Proper logging levels and formats\n",
    "3. **Error handling** - Retries, exponential backoff, categorization\n",
    "4. **Monitoring** - Metrics, health checks, telemetry\n",
    "5. **Testing** - Unit tests, integration tests, validation\n",
    "6. **Deployment** - Checklists, procedures, best practices\n",
    "\n",
    "## Development â†’ Production Journey\n",
    "\n",
    "```\n",
    "Notebook (Exploration)     â†’  Production Code\n",
    "â”œâ”€â”€ Print statements       â†’  Structured logging\n",
    "â”œâ”€â”€ Hardcoded values       â†’  Environment config\n",
    "â”œâ”€â”€ Fail fast             â†’  Retry with backoff\n",
    "â”œâ”€â”€ Manual testing        â†’  Automated tests\n",
    "â”œâ”€â”€ Ad-hoc runs           â†’  Monitoring & metrics\n",
    "â””â”€â”€ \"Works on my machine\" â†’  Reproducible deploys\n",
    "```\n",
    "\n",
    "## Key Production Patterns\n",
    "\n",
    "### 1. Retry with Exponential Backoff\n",
    "```python\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        return do_something()\n",
    "    except RetryableError:\n",
    "        wait = base_delay * (2 ** attempt)\n",
    "        await asyncio.sleep(wait)\n",
    "```\n",
    "\n",
    "### 2. Circuit Breaker (Next Step)\n",
    "```python\n",
    "if error_rate > threshold:\n",
    "    open_circuit()  # Stop making requests\n",
    "    return fallback_response()\n",
    "```\n",
    "\n",
    "### 3. Graceful Degradation\n",
    "```python\n",
    "try:\n",
    "    return expensive_ai_call()\n",
    "except ServiceUnavailable:\n",
    "    return cached_result()  # Better than nothing\n",
    "```\n",
    "\n",
    "## Next Steps for Production\n",
    "\n",
    "1. **Containerization** - Dockerize the application\n",
    "2. **Orchestration** - Kubernetes or cloud services\n",
    "3. **Observability** - Prometheus, Grafana, DataDog\n",
    "4. **CI/CD** - Automated testing and deployment\n",
    "5. **Scaling** - Horizontal scaling, load balancing\n",
    "6. **Caching** - Redis for frequently accessed data\n",
    "7. **Queue** - RabbitMQ/SQS for async processing\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **12-Factor App:** https://12factor.net\n",
    "- **Production ML:** https://madewithml.com\n",
    "- **LLM Ops:** Best practices evolving rapidly\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Course Complete!\n",
    "\n",
    "You've built a complete procurement intelligence system from scratch:\n",
    "\n",
    "1. âœ“ Made your first LLM call\n",
    "2. âœ“ Learned structured outputs with Pydantic\n",
    "3. âœ“ Built a filter agent (classification)\n",
    "4. âœ“ Built a rating agent (multi-dimensional scoring)\n",
    "5. âœ“ Built a document generator (creative generation)\n",
    "6. âœ“ Orchestrated agents into a pipeline\n",
    "7. âœ“ Made it production-ready\n",
    "\n",
    "**You now understand:**\n",
    "- LLMs as software components\n",
    "- Prompt engineering fundamentals\n",
    "- Multi-agent architectures\n",
    "- Production best practices\n",
    "\n",
    "**Keep building! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
